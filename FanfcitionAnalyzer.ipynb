{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112971dd-17b3-4942-a584-66e69c9f8234",
   "metadata": {},
   "source": [
    "# Computergestützte Analyse von Fanfiction auf Archive of Our Own\n",
    "\n",
    "## Eine korpusbasierte Frequenz-, Topic- und Sentimentanalyse themenspezifischer Diskurse\n",
    "\n",
    "Dieses Notebook wertet als CSV bereitgestellte Fanfiction-Daten der Plattform AO3 aus und ermöglicht die Analyse themenspezifischer Diskurse. Dazu wird untersucht, wie viele Texte Themenbezug aufweisen, wie breit und tief dieses Thema in diesen Werken behandelt wird, und welche Bedeutung diesem Thema im Gesamt-Topic-Spektrum zukommt.\n",
    "\n",
    "Über die Bereitstellung eines Untersuchungsgegenstandes und einer Vergleichsliste kann dieses Notebook grundsätzlich für unterschiedliche Themenbereich genutzt werden. Es fokussiert dabei auf inhaltliche Zusammenhänge und berücksichtigt keine zeitlichen Dimensionen. Um zeitliche Entwicklungen nachvollziehen zu können, muss die Analyse daher jeweils getrennt für die einzelnen Zeiträume durchgeführt werden.\n",
    "\n",
    "**Zentrale Annahmen:**  \n",
    "- Fanfiction ist kein neutraler Raum, sondern reagiert auf externe Diskurse\n",
    "- AO3 Tags sind soziale Metadaten und spiegeln bewusste Selbstverortung der Autor*innen wieder\n",
    "- Textinhalte können implizite Repräsentationen enthalten, die nicht explizit getaggt sind\n",
    "\n",
    "**Vorgehen:**  \n",
    "Dieses Notbeook gliedert sich in folgende Abschnitte\n",
    "1. Allgemeines Setup und Einstellungen\n",
    "2. Einlesen und Aufbereiten des Untersuchungsgegenstands\n",
    "3. Einlesen von Vergleichsliste, Charakter-Liste und Stopwords\n",
    "4. Berechnung eines themenspezifischen Likelihood-Scores  \n",
    "5. Allgemeine statistische Auswertung  \n",
    "6. Topic Modeling zur Einbettung des Themas im Gesamtkontext  \n",
    "\n",
    "Sämtliche durchlaufene Schritte werden in einer Ausgabedatei mit Zeitstempel dokumentiert.  \n",
    "\n",
    "Zur Durführung einer Untersuchung müssen zwingend die Abschnitte 1-4 ausgeführt werden. Die Analyse-bezogenen Abschnitte 5 und 6 können je nach Bedarf ausgeführt werden. Hinweise zu Einstellungsmöglichkeiten werden in den jeweiligen Abschnitten gegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39441a-b57c-4519-bc2e-3c740a58ec90",
   "metadata": {},
   "source": [
    "### 1. Allgemeines Setup und Einstellungen\n",
    "Im nachfolgenden werden die erforderlichen Bibliotheken und Pakete installiert und geladen. Zudem werden die Dateipfade für Eingabe- und Ausgabedateien definiert und eine zentrale Auswertungs-Datei angelegt. Die Installation dient nur der lokalen Ausführung; Versionsstände sind nicht fixiert.  \n",
    "\n",
    "Dieses Notebook arbeitet grundsätzlich mit folgender Ordnerstruktur\n",
    "**Ausführungort**: hier wird die Python Umgebung initialisiert.  \n",
    "--> Notebook FanfictionScraper & FanfictionAnalyzer + Systemdateien  \n",
    "**HilfsDokumente**: hier liegen Skripte und Dokumente, die im Rahmen der Notebooks aufgerufen werden.  \n",
    "--> die Scraper-Skripte sowie die Vergleichslisten  \n",
    "\n",
    "Die Datenquellen (von FanfictionScraper generierte Ausgabedokumente mit Metadaten und Textkörpern) können in ***Zelle 3** angepasst werden:\n",
    "`TEXT_CSV = \"Ausgabedokumente/AO3_HP_FanFic_10_2025_text.csv\"`\n",
    "`META_CSV = \"Ausgabedokumente/AO3_HP_FanFic_10_2025_meta.csv\"`\n",
    "\n",
    "Der Zielordner in dem Ausgabedokumente (Analysedokumente, Grafiken, Übersichten), die im Rahmen dieses Notebooks erstellt werden, gespeichert werden, können ebenso wie der Dateiname (Prefix) in ***Zelle 3** angepasst werden:\n",
    "**Ausgabedokumente**: wird automatische erzeugt! Hier werden die Ergebnisse des FanfictionScraper gespeichert.  \n",
    "`OUTPUT_PREFIX = \"Analysis_HP_FanFic_10_2025\"`\n",
    "`OUTPUT_FOLDER = \"Analysedokumente\"`\n",
    "\n",
    "\n",
    "#### Aufbau\n",
    "**Zelle 1:** Installtion/Upgrade der erforderlichen Bibliotheken & Pakete *(optional)*  \n",
    "**Zelle 2:** Importieren der erforderlichen Bibliotheken & Pakete  \n",
    "**Zelle 3:** Definieren von Input und Output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a209d582-22dc-4ab0-9a80-53121bda88a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m5 packages\u001b[0m \u001b[2min 325ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 0.89ms\u001b[0m\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 76ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.60ms\u001b[0m\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m6 packages\u001b[0m \u001b[2min 223ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m15 packages\u001b[0m \u001b[2min 448ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m15 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m12 packages\u001b[0m \u001b[2min 165ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m12 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sofern erforderlich: Installation/Upgrade der nachfolgenden Bibliotheken\n",
    "!uv pip install --upgrade scikit-learn\n",
    "!uv pip install --upgrade lxml\n",
    "!uv pip install --upgrade nltk\n",
    "!uv pip install --upgrade seaborn\n",
    "!uv pip install --upgrade wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "34491619-2829-4371-9960-f53e18c387de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "from collections import Counter, defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# --- Third-party Libraries: Data & Math ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- NLP & Text Processing ---\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# --- Machine Learning / Topic Modeling ---\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer,\n",
    "    ENGLISH_STOP_WORDS\n",
    ")\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "58396b06-7435-45ea-8be9-39d33cf8ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "# - Pfad derjenigen Datei, die die Texte der Fanfiction enthält:\n",
    "TEXT_CSV = \"Ausgabedokumente/AO3_HP_FanFic_10_2015_text.csv\"\n",
    "# - Pfad derjenigen Datei, die die Metadaten wie Title, Rating, Category, Tags enthält:\n",
    "META_CSV = \"Ausgabedokumente/AO3_HP_FanFic_10_2015_meta.csv\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_PREFIX = \"Analysis_HP_FanFic_10_2015\"\n",
    "OUTPUT_FOLDER = \"Analysedokumente/2015\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# - Anlegen des Analyse-Dokuments\n",
    "ANALYSIS_OUTPUT_PATH = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_analyse.txt\"\n",
    ")\n",
    "\n",
    "if os.path.exists(ANALYSIS_OUTPUT_PATH):\n",
    "    raise RuntimeError(\n",
    "        f\"Analyse-Datei existiert bereits:\\n{ANALYSIS_OUTPUT_PATH}\\n\"\n",
    "        \"Bitte umbenennen oder löschen.\"\n",
    "    )\n",
    "\n",
    "def now_ts():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "with open(ANALYSIS_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(f\"{OUTPUT_PREFIX} – Gesamtanalyse\\n\")\n",
    "    f.write(f\"Erstellt am: {now_ts()}\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "def write_analysis_block(title, lines, also_print=True):\n",
    "    ts = now_ts()\n",
    "    if isinstance(lines, str):\n",
    "        lines = [lines]\n",
    "    block = []\n",
    "    block.append(f\"[{ts}] {title}\")\n",
    "    block.append(\"-\" * 60)\n",
    "    block.extend(lines)\n",
    "    block.append(\"\")\n",
    "    text = \"\\n\".join(block)\n",
    "    if also_print:\n",
    "        print(text)\n",
    "    with open(ANALYSIS_OUTPUT_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fdff5f-de9f-43f4-804e-b506a7a2c3e2",
   "metadata": {},
   "source": [
    "### 2. Einlesen der Daten, Textnormalisierung und Tag-Bereinigung\n",
    "Im folgenden werden die zu untersuchenden Daten sowie die themenspezifische Vergleichsliste geladen und normalisiert.  \n",
    "Im Notebook kann die Struktur des Datensatzes kontrolliert werden.  \n",
    "Zentrale Ergebnisse werden in das Analyse-Dokument geschrieben.\n",
    "\n",
    "#### Aufbau\n",
    "**Zelle 1:** Einlesen und Aufbereiten des Untersuchungsgegenstandes  \n",
    "**Zelle 2:** Prüfen der geladenen Datenstruktur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b32549b8-d0cd-40fe-bca0-f79d3d35221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Uni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# CSV einlesen\n",
    "df_text = pd.read_csv(TEXT_CSV, encoding='utf-8-sig', quotechar='\"')\n",
    "df_meta = pd.read_csv(META_CSV, encoding='utf-8-sig', quotechar='\"')\n",
    "\n",
    "# Zusammenführung über WorkID\n",
    "df = df_meta.merge(df_text, on=\"work_id\", how=\"left\")\n",
    "df = df.fillna(\"NaN\")\n",
    "\n",
    "# Text normalisieren und lemmatisieren\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_and_lemmatize(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Unicode-Normalisierung\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    # Kleinbuchstaben\n",
    "    text = text.lower()\n",
    "    # Sonderzeichen entfernen\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    # Mehrere Leerzeichen reduzieren\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Lemmatisierung\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"text_norm_lem\"] = df[\"text\"].apply(normalize_and_lemmatize)\n",
    "\n",
    "\n",
    "def prepare_text_for_topics(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Zahlen entfernen\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    # sehr kurze Wörter entfernen (<=2 Zeichen)\n",
    "    text = re.sub(r\"\\b\\w{1,2}\\b\", \" \", text)\n",
    "    # Mehrfach-Leerzeichen\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"text_topic\"] = df[\"text_norm_lem\"].apply(prepare_text_for_topics)\n",
    "\n",
    "\n",
    "# Tag-Aufbereitung\n",
    "TAG_COLS = [\"category\", \"relationship\", \"character\", \"freeform\"]\n",
    "\n",
    "def split_tags(cell):\n",
    "    if not cell:\n",
    "        return []\n",
    "    return [t.strip().lower() for t in str(cell).split(\",\") if t.strip()]\n",
    "\n",
    "for col in TAG_COLS:\n",
    "    df[col] = df[col].apply(split_tags)\n",
    "\n",
    "def normalize_tag(tag):\n",
    "    tag = tag.lower().strip()\n",
    "    tag = tag.replace(\" \", \"_\")\n",
    "    tag = tag.replace(\"-\", \"_\")\n",
    "    return tag\n",
    "\n",
    "for col in [\"freeform\", \"character\"]:\n",
    "    df[col] = df[col].apply(lambda tags: [normalize_tag(t) for t in tags if t not in [None, np.nan, \"nan\", \"NaN\"]])\n",
    "\n",
    "df[\"rating\"] = df[\"rating\"].str.strip().str.lower()\n",
    "df[\"num_characters\"] = df[\"character\"].apply(len)\n",
    "df[\"num_freeform_tags\"] = df[\"freeform\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5ccd36f5-6e93-45e2-a136-fdf71d2ff6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 21:37:30] Datenprüfung – Umfang, Struktur, Darstellung\n",
      "------------------------------------------------------------\n",
      "\n",
      "Der Datensatz umfasst 362 Werke und ist strukturiert nach:\n",
      "work_id, title, rating, category, relationship, character, freeform, text, text_norm_lem, text_topic, num_characters, num_freeform_tags\n",
      "\n",
      "------------------------------  nur im Notebook  ------------------------------\n",
      "Datenzugriff prüfen\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_id</th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>category</th>\n",
       "      <th>relationship</th>\n",
       "      <th>character</th>\n",
       "      <th>freeform</th>\n",
       "      <th>text</th>\n",
       "      <th>text_norm_lem</th>\n",
       "      <th>text_topic</th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_freeform_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5007118</td>\n",
       "      <td>The Truth about Harry Potter</td>\n",
       "      <td>general audiences</td>\n",
       "      <td>[m/m]</td>\n",
       "      <td>[draco malfoy/harry potter, various mentioned ...</td>\n",
       "      <td>[draco_malfoy, harry_potter, author___character]</td>\n",
       "      <td>[self_insert, crack!fic, seriously_pointless, ...</td>\n",
       "      <td>\\nFade in.\\n\\n\\nAn average-looking girl of an ...</td>\n",
       "      <td>fade in an average looking girl of an average ...</td>\n",
       "      <td>fade average looking girl average age with ave...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   work_id                         title             rating category  \\\n",
       "0  5007118  The Truth about Harry Potter  general audiences    [m/m]   \n",
       "\n",
       "                                        relationship  \\\n",
       "0  [draco malfoy/harry potter, various mentioned ...   \n",
       "\n",
       "                                          character  \\\n",
       "0  [draco_malfoy, harry_potter, author___character]   \n",
       "\n",
       "                                            freeform  \\\n",
       "0  [self_insert, crack!fic, seriously_pointless, ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\nFade in.\\n\\n\\nAn average-looking girl of an ...   \n",
       "\n",
       "                                       text_norm_lem  \\\n",
       "0  fade in an average looking girl of an average ...   \n",
       "\n",
       "                                          text_topic  num_characters  \\\n",
       "0  fade average looking girl average age with ave...               3   \n",
       "\n",
       "   num_freeform_tags  \n",
       "0                 10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fade in an average looking girl of an average age with average hair walked into \n",
      "------------------------------  nur im Notebook  ------------------------------\n"
     ]
    }
   ],
   "source": [
    "analysis_lines = []\n",
    "\n",
    "# Umfang und Struktur des Datensatzes\n",
    "analysis_lines.append(f\"\\nDer Datensatz umfasst {len(df)} Werke und ist strukturiert nach:\")\n",
    "analysis_lines.append(\", \".join(df.columns.tolist()) )\n",
    "\n",
    "# Zentral ausgeben + protokollieren\n",
    "write_analysis_block(\n",
    "    title=\"Datenprüfung – Umfang, Struktur, Darstellung\",\n",
    "    lines=analysis_lines\n",
    ")\n",
    "\n",
    "# Zugriff prüfen:\n",
    "print(\"-\"*30, \" nur im Notebook \", \"-\"*30)\n",
    "print(\"Datenzugriff prüfen\")\n",
    "display(df.head(1))\n",
    "print(df[\"text_norm_lem\"].iloc[0][:80])\n",
    "print(\"-\"*30, \" nur im Notebook \", \"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5aa2e-35e2-4888-ac86-3805654c17c6",
   "metadata": {},
   "source": [
    "### 3. Einlesen von Vergleichsliste, Characterliste und Stopwords\n",
    "Im nachfolgenden werden die für die Analyse benötigten Vergleichslisten, darunter die ***Liste zum Themenbezug*** (compare_list für die Berechnung des Likeliehood-Scores), die ***Charakterliste*** (character_list für die Analyse zum Themenbezug der Figuren) und die ***Liste der stopwords*** (Topic Modeling) eingelesen.  \n",
    "\n",
    "Die Dateien liegen grundsätzlich im Ordner ***HilfsDokumente***  \n",
    "Dateiursprünge der entsprechenden Listen können in ***Zelle 1** angepasst werden.  \n",
    "`comparison_file = \"HilfsDokumente/compare_list.txt\"`  \n",
    "`character_file = \"HilfsDokumente/character_list.txt\"`  \n",
    "`stopword_file = \"HilfsDokumente/stopword_list.txt\"`  \n",
    "\n",
    "\n",
    "#### Aufbau\n",
    "**Zelle 1:** Laden der Vergleichslisten aus dem Ordner \"HilfsDokumente\"   \n",
    "**Zelle 2:** Test der Wortliste *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ff1100e3-2c26-4868-bbf2-e8adbdbf35d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 21:37:30] Listen Einlesen – Übersicht\n",
      "------------------------------------------------------------\n",
      "Vergleichsliste erfolgreich eingelesen.\n",
      "Dateipfad: HilfsDokumente/compare_list.txt\n",
      "Umfang der Liste (Vergleichsliste): 134\n",
      "\n",
      "Auszug aus Vergleichsliste (erste 10 Terme):\n",
      "trans exclusionary radical feminist, gender recognition certificate, gender confirmation surgeries, person with a trans history, assigned female at birth, transgender individuals, transgender individual, assigned male at birth, gender non conforming, gender non confirming\n",
      "\n",
      "Character-Liste erfolgreich eingelesen.\n",
      "Dateipfad: HilfsDokumente/character_list.txt\n",
      "Umfang der Liste (Character-Liste): 229\n",
      "\n",
      "Auszug aus Character-Liste (erste 10 Terme):\n",
      "gregorowitsch, xenophilius, shacklebolt, grindelwald, crookshanks, longbottom, ollivander, scrimgeour, dumbledore, mcgonagall\n",
      "\n",
      "Stopword-Liste erfolgreich eingelesen.\n",
      "Dateipfad: HilfsDokumente/stopword_list.txt\n",
      "Umfang der Liste (Stopword-Liste): 6998\n",
      "\n",
      "Auszug aus Stopword-Liste (erste 10 Terme):\n",
      "uncomfortable, professional, conversation, nevertheless, relationship, practically, disappeared, nonetheless, christopher, arsecheecks\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pfade zu den Dateien\n",
    "comparison_file = \"HilfsDokumente/compare_list.txt\"\n",
    "character_file = \"HilfsDokumente/character_list.txt\"\n",
    "stopword_file = \"HilfsDokumente/stopword_list.txt\"\n",
    "\n",
    "analysis_lines = []\n",
    "\n",
    "# Funkion zum Einlesen und Prüfen einer Liste\n",
    "def load_list(file_path, list_name):\n",
    "    lines = []\n",
    "    title = \"\"\n",
    "    try:\n",
    "        terms = set()\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip().lower()\n",
    "                if line:\n",
    "                    # Zeile an Kommas aufteilen und einzelne Wörter strippen\n",
    "                    for word in line.split(\",\"):\n",
    "                        word_clean = word.strip()\n",
    "                        if word_clean:\n",
    "                            terms.add(word_clean)\n",
    "\n",
    "        if not terms:\n",
    "            raise ValueError(f\"{list_name} wurde gelesen, enthält aber keine gültigen Terme.\")\n",
    "\n",
    "        terms_sorted = sorted(terms, key=len, reverse=True)\n",
    "\n",
    "        lines.append(f\"{list_name} erfolgreich eingelesen.\")\n",
    "        lines.append(f\"Dateipfad: {file_path}\")\n",
    "        lines.append(f\"Umfang der Liste ({list_name}): {len(terms_sorted)}\")\n",
    "\n",
    "        preview_n = 10\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"Auszug aus {list_name} (erste {preview_n} Terme):\")\n",
    "        lines.append(\", \".join(terms_sorted[:preview_n]))\n",
    "        lines.append(\"\")\n",
    "\n",
    "        title = f\"{list_name} – erfolgreich geladen\"\n",
    "        return terms_sorted, lines, title\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        lines.append(f\"FEHLER: {list_name} konnte nicht gefunden werden.\")\n",
    "        lines.append(f\"Erwarteter Pfad: {file_path}\")\n",
    "        lines.append(\"\")\n",
    "        title = f\"{list_name} – FEHLER (Datei nicht gefunden)\"\n",
    "        return [], lines, title\n",
    "\n",
    "    except Exception as e:\n",
    "        lines.append(f\"FEHLER beim Einlesen von {list_name}:\")\n",
    "        lines.append(str(e))\n",
    "        lines.append(\"\")\n",
    "        title = f\"{list_name} – FEHLER (Einlesen fehlgeschlagen)\"\n",
    "        return [], lines, title\n",
    "\n",
    "\n",
    "# Vergleichsliste einlesen\n",
    "compare_list, compare_lines, compare_title = load_list(comparison_file, \"Vergleichsliste\")\n",
    "analysis_lines.extend(compare_lines)\n",
    "\n",
    "# Character-Liste einlesen\n",
    "character_list, character_lines, character_title = load_list(character_file, \"Character-Liste\")\n",
    "analysis_lines.extend(character_lines)\n",
    "\n",
    "# Stopword-Liste einlesen\n",
    "stopword_list, stopword_lines, stopword_title = load_list(stopword_file, \"Stopword-Liste\")\n",
    "analysis_lines.extend(stopword_lines)\n",
    "\n",
    "# Zentral ausgeben + protokollieren\n",
    "write_analysis_block(\n",
    "    title=\"Listen Einlesen – Übersicht\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1a16c8aa-963d-49a1-9bf6-0a74ebed2e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Begriffe mit Häufigkeit:\n",
      "assigned at birth: 2\n",
      "gender identity: 2\n",
      "transgender: 5\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Test der Vergleichsliste\n",
    "# - TestText\n",
    "text1 = \"\"\"\n",
    "Transgender is an adjective to describe people whose gender identity differs\n",
    "from the sex they were assigned at birth. \n",
    "People who are transgender may also use other terms, in addition to transgender, \n",
    "to describe their gender more specifically. \n",
    "Use the term(s) the person uses to describe their gender. \n",
    "It is important to note that being transgender is not dependent upon \n",
    "physical appearance or medical procedures. \n",
    "A person can call themself transgender the moment they realize \n",
    "that their gender identity is different than the sex they were assigned at birth.\n",
    "\"\"\"\n",
    "\n",
    "text1_lower = text1.lower()\n",
    "matches = []\n",
    "matches_counter = Counter()\n",
    "\n",
    "# - Greedy-Matching\n",
    "for term in compare_list:\n",
    "    term_escaped = re.escape(term)\n",
    "    # Alle Vorkommen zählen\n",
    "    found = re.findall(rf'\\b{term_escaped}\\b', text1_lower)\n",
    "    if found:\n",
    "        matches_counter[term] += len(found)\n",
    "        # Term aus Text entfernen, um Doppelzählungen zu vermeiden\n",
    "        text1_lower = re.sub(rf'\\b{term_escaped}\\b', ' ', text1_lower)\n",
    "\n",
    "# - Ausgabe\n",
    "print(\"Gefundene Begriffe mit Häufigkeit:\")\n",
    "for term, count in matches_counter.items():\n",
    "    print(f\"{term}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50795de5-54a3-4348-8f12-c3485accf042",
   "metadata": {},
   "source": [
    "### 4. Berechnung eines Likelihood-Score\n",
    "Um die Intensität der thematischen Auseinandersetzung differenziert abzubilden, wird ein kontinuierlicher Likelihood-Score eingeführt. Dieser berücksichtigt die Anzahl und Häufigkeit der in der Vergleichsliste enthaltenen Begriffe und modelliert den thematischen Bezug als graduelles Merkmal. \n",
    "\n",
    "#### Technische Beschreibung des Likelihood-Scores\n",
    "Der Likelihood-Score ist ein gewichteter Relevanzwert, der angibt, wie stark ein Fanfiction-Werk auf eine vordefinierte Vergleichsliste von Begriffen (z. B. trans-spezifische Terme) reagiert. Der Score kombiniert Treffer in Metadaten-Tags und im Textkörper, um eine differenzierte Einschätzung zu ermöglichen.  \n",
    "\n",
    "**Identifikation der Treffer**\n",
    "Metadaten (freeform + character) und lemmatisierte Textkörper werden mit der Vergleichsliste abgeglichen. Die Vergleichsliste ist dazu absteigend nach Ausdruckslänge sortiert, sodass zusammenhängende Ausdrücke korrekt identifiziert werden können (Greedy-Matching). Nach einem Treffer wird der entsprechende Textabschnitt maskiert, um Mehrfachzählungen derselben Textstelle und damit eine künstliche Erhöhung des Scores zu vermeiden. Die Trefferzahl wird pro Term erfasst, um Aussagen darüber treffen zu können, welche Begriffe wie häufig verwendet werden.  \n",
    "\n",
    "**Berechnung der Term-Scores**\n",
    "Bei der Berechnung des Scores soll berücksichtigt werden, dass viele unterschiedliche relevante Begriffen eine breitere, eine wiederholte Verwendung relevanter Begriffe eine tiefere Auseinandersetzung mit einer Thematik indizieren. Daher wird für jeden Bereich (freeform, character, text) ein Score gebildet, der zwei Komponenten kombiniert: Unique Hits gibt an, wie viele unterschiedliche Terme aus der Vergleichsliste im Text vorkommen, Frequency Sum enthält die standardmäßig logarithmisch skaliert Trefferanzahl pro Term.  \n",
    "\n",
    "**Gewichtung der Bereiche**\n",
    "Über die Gewichtung der Bereiche kann gesteuert werden, welcher Teil-Score (Tags vs. Text) stärker einfließen sollen. Da von den Autoren vergebene Tags regelmäßig prägnanter und oft genauer in der thematischen Einordnung sind, empfiehlt sich eine entsprächend stärkere Gewichtung. \n",
    "Die Gewichtung der Bereiche kann in ***Zelle 1*** eingestellt werden:\n",
    "`WEIGHTS = {\"tags\": 0.6, \"text\": 0.4}`  \n",
    "`USE_LOG_SCALING = True`  \n",
    "\n",
    "**Likeliehood-Positives**  \n",
    "Nach Berechnung des Likeliehood-Scores werden alle einschlägigen IDs mit dem berechneten Likeliehood-Score sowie den Matches in Tags und Text in eine Ausgabedatei (likeliehood_positives.csv) geschrieben und zusammen mit der Grafik likeliehood_distribution.png im Analyse-Ordner ausgegeben. Diese Übersicht sollte genutzt werden um\n",
    "- etwaige False Positives festzustellen. Dazu sollten inbesondere diejenigen Ids manuell geprüft werden, für die nur Begriffe gemached wurden, die etwas weiter vom thematischen Kern entfernt sind bzw. auch in anderen Themenzusammenhängen vorkommen können vom Kern-Begriff weiter entfernt sind. Begriffe, die vorwiegend in ***anderen Themenzusammenhängen*** vorkommen, sollten von der Untersuchung ausgenommen werden, indem sie von der compare_list.txt gestrichen werden.\n",
    "- den Schwellenwert für die weitere Untersuchung festzulegen (siehe nächsten Abschnitt)\n",
    "\n",
    "#### Aufbau\n",
    "**Zelle 1:** Gewichtung *(einstellbar)*  \n",
    "**Zelle 2:** Berechnung des Likeliehood-Scores  \n",
    "**Zelle 3:** Ausgabe zentraler Kennzahlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "156854a9-1a06-4825-8d59-31c54878475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gewichtung EINSTELLBAR!\n",
    "WEIGHTS = {\"tags\": 0.6, \"text\": 0.4} \n",
    "USE_LOG_SCALING = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "76c4f873-0abf-4a58-b38d-4bb17a1b1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches identifizieren und zählen (Greedy-Matching)\n",
    "def count_term_hits(text, terms):\n",
    "    counter = Counter()\n",
    "\n",
    "    if not text:\n",
    "        return counter\n",
    "\n",
    "    # Alles klein, Unterstriche und Bindestriche normalisieren\n",
    "    text_lower = text.lower()\n",
    "    text_normalized = text_lower.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "\n",
    "    # Maske zur Vermeidung doppelter Treffer (Greedy)\n",
    "    mask = [\" \"] * len(text_normalized)\n",
    "\n",
    "    for term in terms:\n",
    "        term_lower = term.lower()\n",
    "        term_normalized = term_lower.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "        pattern = re.escape(term_normalized)\n",
    "\n",
    "        # Regex für ganze Wörter / Phrasen\n",
    "        regex = re.compile(rf'\\b{pattern}\\b')\n",
    "\n",
    "        for m in regex.finditer(text_normalized):\n",
    "            start, end = m.span()\n",
    "\n",
    "            # Prüfen, ob Bereich bereits maskiert wurde\n",
    "            if all(c == \" \" for c in mask[start:end]):\n",
    "                counter[term] += 1\n",
    "                # Bereich maskieren\n",
    "                mask[start:end] = [\"*\"] * (end - start)\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "# Score berechnen\n",
    "def score(row):\n",
    "    # - Freeform Tags\n",
    "    freeform_text = \" \".join(row.get(\"freeform\", []))\n",
    "    freeform_hits = count_term_hits(freeform_text, compare_list)\n",
    "    freeform_unique = len(freeform_hits)\n",
    "    freeform_freq_sum = (\n",
    "        sum(math.log(1 + v) for v in freeform_hits.values()) if USE_LOG_SCALING else sum(freeform_hits.values())\n",
    "    )\n",
    "    freeform_score = freeform_unique + freeform_freq_sum\n",
    "\n",
    "    # - Character Tags\n",
    "    char_text = \" \".join(row.get(\"character\", []))\n",
    "    char_hits = count_term_hits(char_text, compare_list)\n",
    "    char_unique = len(char_hits)\n",
    "    char_freq_sum = (\n",
    "        sum(math.log(1 + v) for v in char_hits.values()) if USE_LOG_SCALING else sum(char_hits.values())\n",
    "    )\n",
    "    char_score = char_unique + char_freq_sum\n",
    "\n",
    "    # - Text\n",
    "    text_text = row.get(\"text_norm_lem\", \"\")\n",
    "    text_hits = count_term_hits(text_text, compare_list)\n",
    "    text_unique = len(text_hits)\n",
    "    text_freq_sum = (\n",
    "        sum(math.log(1 + v) for v in text_hits.values()) if USE_LOG_SCALING else sum(text_hits.values())\n",
    "    )\n",
    "    text_score = text_unique + text_freq_sum\n",
    "\n",
    "    # -- Gesamt-Score\n",
    "    total_score = round(\n",
    "        (WEIGHTS[\"tags\"] * (freeform_score + char_score) + WEIGHTS[\"text\"] * text_score),\n",
    "        2\n",
    "    )\n",
    "\n",
    "    return pd.Series({\n",
    "        \"likelihood\": total_score,\n",
    "        \"freeform_hits_counter\": freeform_hits,\n",
    "        \"character_hits_counter\": char_hits,\n",
    "        \"text_hits_counter\": text_hits,\n",
    "        \"freeform_score\": freeform_score,\n",
    "        \"character_score\": char_score,\n",
    "        \"text_score\": text_score\n",
    "    })\n",
    "\n",
    "\n",
    "# Score-Funktion auf DataFrame anwenden\n",
    "df[[\n",
    "    \"likelihood\",\n",
    "    \"freeform_hits_counter\",\n",
    "    \"character_hits_counter\",\n",
    "    \"text_hits_counter\",\n",
    "    \"freeform_score\",\n",
    "    \"character_score\",\n",
    "    \"text_score\"\n",
    "]] = df.apply(score, axis=1)\n",
    "\n",
    "\n",
    "# Trefferzahlen berechnen\n",
    "df[\"freeform_hits_count\"] = df[\"freeform_hits_counter\"].apply(lambda c: sum(c.values()))\n",
    "df[\"character_hits_count\"] = df[\"character_hits_counter\"].apply(lambda c: sum(c.values()))\n",
    "df[\"text_hits_count\"] = df[\"text_hits_counter\"].apply(lambda c: sum(c.values()))\n",
    "df[\"tag_hits_count\"] = df[\"freeform_hits_count\"] + df[\"character_hits_count\"]\n",
    "df[\"total_hits_count\"] = df[\"freeform_hits_count\"] + df[\"character_hits_count\"] + df[\"text_hits_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c18fea2a-7b4a-4918-b894-5ae977c9de19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 21:37:48] Likelihood-Score - Gewichtung und Berechnung\n",
      "------------------------------------------------------------\n",
      "Score- und Treffer-Berechnung abgeschlossen für\n",
      "Gewichtung: tags=0.6, text=0.4\n",
      "Log-Skalierung: True\n",
      "\n",
      "Anzahl der Werke mit mindestens einem Treffer: 3\n",
      "Summe aller Treffer: 7\n",
      "Durchschnittliche Treffer pro Werk: 2.33\n",
      "\n",
      "Likelihood-Score Verteilung:\n",
      "Höchster Score: 2.24\n",
      "Niedrigster Score: 0.0\n",
      "Durchschnitt (Mean): 0.01\n",
      "Median: 0.0\n",
      "Standardabweichung: 0.13\n",
      "\n",
      "Histogramm des Likelihood-Scores gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_likelihood_distribution.png\n",
      "\n",
      "Pfad zur CSV-Datei für die manuelle Prüfung: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_likelihood_positives.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grundverteilung beschreiben\n",
    "likelihood_desc = df[\"likelihood\"].describe()\n",
    "min_score = likelihood_desc[\"min\"]\n",
    "max_score = likelihood_desc[\"max\"]\n",
    "mean_score = round(likelihood_desc[\"mean\"], 2)\n",
    "median_score = likelihood_desc[\"50%\"]\n",
    "std_score = round(likelihood_desc[\"std\"], 2)\n",
    "\n",
    "# Grafische Darstellung der Grundverteilung\n",
    "hist_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_likelihood_distribution.png\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(df[\"likelihood\"], bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Likelihood-Score\")\n",
    "plt.ylabel(\"Anzahl Texte\")\n",
    "plt.title(f\"Verteilung des Likelihood-Scores für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(hist_file)\n",
    "plt.close()  \n",
    "\n",
    "# Ausgabe in CSV-Datei zur manuellen Prüfung\n",
    "df_export = df.copy()\n",
    "\n",
    "df_export[\"freeform_hits_terms\"] = df_export[\"freeform_hits_counter\"].apply(\n",
    "    lambda c: \", \".join(c.keys()) if c else \"\"\n",
    ")\n",
    "df_export[\"character_hits_terms\"] = df_export[\"character_hits_counter\"].apply(\n",
    "    lambda c: \", \".join(c.keys()) if c else \"\"\n",
    ")\n",
    "df_export[\"text_hits_terms\"] = df_export[\"text_hits_counter\"].apply(\n",
    "    lambda c: \", \".join(c.keys()) if c else \"\"\n",
    ")\n",
    "\n",
    "# Nur Zeilen mit mindestens einem Treffer\n",
    "df_export = df_export[df_export[\"total_hits_count\"] > 0]\n",
    "\n",
    "output_columns = [\n",
    "    \"work_id\",\n",
    "    \"likelihood\",\n",
    "    \"freeform_hits_count\",\n",
    "    \"freeform_hits_terms\",\n",
    "    \"character_hits_count\",\n",
    "    \"character_hits_terms\",\n",
    "    \"text_hits_count\",\n",
    "    \"text_hits_terms\",\n",
    "    \"total_hits_count\"\n",
    "]\n",
    "\n",
    "output_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_likelihood_positives.csv\"\n",
    ")\n",
    "\n",
    "df_export[output_columns].to_csv(\n",
    "    output_file,\n",
    "    index=False,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "num_ids = len(df_export)\n",
    "\n",
    "# Zusammenfassung & Ausgabe im Analyse-Dokument\n",
    "analysis_lines = []\n",
    "analysis_lines.append(\"Score- und Treffer-Berechnung abgeschlossen für\")\n",
    "analysis_lines.append(f\"Gewichtung: tags={WEIGHTS['tags']}, text={WEIGHTS['text']}\")\n",
    "analysis_lines.append(f\"Log-Skalierung: {USE_LOG_SCALING}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "analysis_lines.append(f\"Anzahl der Werke mit mindestens einem Treffer: {num_ids}\")\n",
    "\n",
    "total_hits_sum = df_export[\"total_hits_count\"].sum()\n",
    "avg_hits_per_work = round(df_export[\"total_hits_count\"].mean(), 2) if num_ids > 0 else 0\n",
    "analysis_lines.append(f\"Summe aller Treffer: {total_hits_sum}\")\n",
    "analysis_lines.append(f\"Durchschnittliche Treffer pro Werk: {avg_hits_per_work}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "analysis_lines.append(\"Likelihood-Score Verteilung:\")\n",
    "analysis_lines.append(f\"Höchster Score: {max_score}\")\n",
    "analysis_lines.append(f\"Niedrigster Score: {min_score}\")\n",
    "analysis_lines.append(f\"Durchschnitt (Mean): {mean_score}\")\n",
    "analysis_lines.append(f\"Median: {median_score}\")\n",
    "analysis_lines.append(f\"Standardabweichung: {std_score}\")\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(f\"Histogramm des Likelihood-Scores gespeichert unter: {hist_file}\")\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(f\"Pfad zur CSV-Datei für die manuelle Prüfung: {output_file}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Zentral ausgeben & protokollieren\n",
    "write_analysis_block(\n",
    "    title=\"Likelihood-Score - Gewichtung und Berechnung\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d06c6-a2ae-4727-96fb-4541e15fd0cc",
   "metadata": {},
   "source": [
    "### 5. Allgemeine Statistische Auswertung\n",
    "Für die weitere Auswertung wird in ***Zelle 1*** zunächst auf Basis der Grundverteilung des Likeliehood-Scores ein Schwellenwert gesetzt, der mit zusätzlichen Vorgaben hinsichtlich der Matches in den Tags und Textkörpern ergänzt werden kann.  \n",
    "`LIKELIHOOD_THRESHOLD = 0.5`  \n",
    "`MIN_TAG_HITS = 0`  \n",
    "`MIN_TEXT_HITS = 0`  \n",
    "\n",
    "Die vor dem Hintergrund dieses Schwellenwertes als relevant klassifizierten Werke werden in diesem Abschnitt näher untersucht hinsichtlich\n",
    "- **Breite & Tiefe**: Wie viele Begriffe der Vergleichsliste werden verwendet (Breite) und wie häufig kommen einzelne Begriffe vor (Tiefe).\n",
    "- **Rating**: Von Autor:innen vergeben (Explicit, General Audiences, Mature, Not Rated, Teen and Up). Ziel war zu prüfen, ob transbezogene Inhalte mit sexualisierten Inhalten oder Altersbeschränkungen assoziiert sind, bzw. ob transbezogene Inhalte auch in Werken ohne Altersbeschränkung vorkommen.  \n",
    "- **Charakter**: Häufigkeit der Charaktere in relevanten Werken und Gesamt-Korpus, um die Relevanzrelation pro Figur zu bestimmen.  \n",
    "Die Auswertung liefert so quantitative Hinweise darauf, welche Begriffe, Tags, Charaktere und Alterskategorien im Zusammenhang mit einschlägigen Werken besonders häufig auftreten.\n",
    "\n",
    "#### Aufbau\n",
    "**Zelle 1:** Definition eines Schwellenwertes (Likeliehood-Score)  \n",
    "**Zelle 2:** Grundlegende Kennzahlen  \n",
    "**Zelle 3:** Breite und Tiefe der Matches  \n",
    "**Zelle 4:** Matches vs. Rating  \n",
    "**Zelle 5:** Matches vs. Charactere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9604e8fc-cf17-423a-b377-76ea4eb5049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition eines Schwellenwerts / Einstellbar!\n",
    "LIKELIHOOD_THRESHOLD = 0.5\n",
    "MIN_TAG_HITS = 0\n",
    "MIN_TEXT_HITS = 0\n",
    "\n",
    "df[\"is_relevant\"] = (\n",
    "    (df[\"likelihood\"] >= LIKELIHOOD_THRESHOLD) &\n",
    "    (df[\"tag_hits_count\"] >= MIN_TAG_HITS) &\n",
    "    (df[\"text_hits_count\"] >= MIN_TEXT_HITS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ba86e0c0-6ab8-4584-88a6-faa8b6d9577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 21:37:48] Grundlegende Kennzahlen zu den einschlägigen Works\n",
      "------------------------------------------------------------\n",
      "Definierter Likelihood-Schwellenwert: 0.5\n",
      "Mindestanzahl Matches in Tags: 0\n",
      "Mindestanzahl Matches in Text: 0\n",
      "\n",
      "Anzahl aller Werke: 362\n",
      "Anzahl einschlägiger Werke: 3\n",
      "Anteil einschlägiger Werke: 0.83 %\n",
      "\n",
      "Freeform-Tags:\n",
      "- Werke mit mindestens einem Freeform-Tag: 3 von 3 (100.0 %)\n",
      "- Gesamtanzahl Freeform-Tags in relevanten Werken: 15\n",
      "- Anteil dieser Freeform-Tags am gesamten Korpus: 0.9 %\n",
      "\n",
      "Character-Tags:\n",
      "- Werke mit mindestens einem Character-Tag: 3 von 3 (100.0 %)\n",
      "- Gesamtanzahl Character-Tags in relevanten Werken: 8\n",
      "- Anteil dieser Character-Tags am gesamten Korpus: 0.7 %\n",
      "\n",
      "Textumfang der einschlägigen Werke:\n",
      "- Gesamtlänge der Texte einschlägiger Werke: 3,293 Tokens\n",
      "- Anteil am gesamten Textkorpus: 0.4 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grundlegende Kennzahlen\n",
    "analysis_lines = []\n",
    "\n",
    "analysis_lines.append(f\"Definierter Likelihood-Schwellenwert: {LIKELIHOOD_THRESHOLD}\")\n",
    "analysis_lines.append(f\"Mindestanzahl Matches in Tags: {MIN_TAG_HITS}\")\n",
    "analysis_lines.append(f\"Mindestanzahl Matches in Text: {MIN_TEXT_HITS}\")\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(f\"Anzahl aller Werke: {len(df)}\")\n",
    "analysis_lines.append(f\"Anzahl einschlägiger Werke: {df['is_relevant'].sum()}\")\n",
    "analysis_lines.append(f\"Anteil einschlägiger Werke: {round(df['is_relevant'].mean() * 100, 2)} %\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "df_relevant = df[df[\"is_relevant\"]].copy()\n",
    "\n",
    "# Freeform-Tags\n",
    "# Anzahl Werke mit mindestens einem Freeform-Tag\n",
    "relevant_freeform_works = (df_relevant[\"freeform\"].apply(len) > 0).sum()\n",
    "all_freeform_works = (df[\"freeform\"].apply(len) > 0).sum()\n",
    "\n",
    "# Gesamtanzahl Freeform-Tags\n",
    "total_freeform_relevant = df_relevant[\"freeform\"].apply(len).sum()\n",
    "total_freeform_all = df[\"freeform\"].apply(len).sum()\n",
    "\n",
    "analysis_lines.append(\"Freeform-Tags:\")\n",
    "analysis_lines.append(\n",
    "    f\"- Werke mit mindestens einem Freeform-Tag: \"\n",
    "    f\"{relevant_freeform_works} von {len(df_relevant)} \"\n",
    "    f\"({round(100*relevant_freeform_works/len(df_relevant),1)} %)\"\n",
    ")\n",
    "analysis_lines.append(\n",
    "    f\"- Gesamtanzahl Freeform-Tags in relevanten Werken: {total_freeform_relevant}\"\n",
    ")\n",
    "analysis_lines.append(\n",
    "    f\"- Anteil dieser Freeform-Tags am gesamten Korpus: \"\n",
    "    f\"{round(100*total_freeform_relevant/total_freeform_all,1)} %\"\n",
    ")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Character-Tags\n",
    "relevant_character_works = (df_relevant[\"character\"].apply(len) > 0).sum()\n",
    "all_character_works = (df[\"character\"].apply(len) > 0).sum()\n",
    "\n",
    "total_character_relevant = df_relevant[\"character\"].apply(len).sum()\n",
    "total_character_all = df[\"character\"].apply(len).sum()\n",
    "\n",
    "analysis_lines.append(\"Character-Tags:\")\n",
    "analysis_lines.append(\n",
    "    f\"- Werke mit mindestens einem Character-Tag: \"\n",
    "    f\"{relevant_character_works} von {len(df_relevant)} \"\n",
    "    f\"({round(100*relevant_character_works/len(df_relevant),1)} %)\"\n",
    ")\n",
    "analysis_lines.append(\n",
    "    f\"- Gesamtanzahl Character-Tags in relevanten Werken: {total_character_relevant}\"\n",
    ")\n",
    "analysis_lines.append(\n",
    "    f\"- Anteil dieser Character-Tags am gesamten Korpus: \"\n",
    "    f\"{round(100*total_character_relevant/total_character_all,1)} %\"\n",
    ")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Text-Statistiken\n",
    "df[\"text_length\"] = df[\"text_norm_lem\"].apply(lambda x: len(x.split()))\n",
    "df_relevant[\"text_length\"] = df_relevant[\"text_norm_lem\"].apply(\n",
    "    lambda x: len(x.split())\n",
    ")\n",
    "\n",
    "total_text_length_relevant = df_relevant[\"text_length\"].sum()\n",
    "total_text_length_all = df[\"text_length\"].sum()\n",
    "\n",
    "analysis_lines.append(\"Textumfang der einschlägigen Werke:\")\n",
    "analysis_lines.append(\n",
    "    f\"- Gesamtlänge der Texte einschlägiger Werke: \"\n",
    "    f\"{total_text_length_relevant:,} Tokens\"\n",
    ")\n",
    "analysis_lines.append(\n",
    "    f\"- Anteil am gesamten Textkorpus: \"\n",
    "    f\"{round(100 * total_text_length_relevant / total_text_length_all, 1)} %\"\n",
    ")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# In Analyse-Dokument schreiben\n",
    "write_analysis_block(\n",
    "    title=f\"Grundlegende Kennzahlen zu den einschlägigen Works\",\n",
    "    lines=analysis_lines\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1aa06d3a-3af3-4b56-a77d-002f46d750f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 21:37:49] Breite & Tiefe der Matches – Statistik (compare_list)\n",
      "------------------------------------------------------------\n",
      "Für Tags (Freeform + Character, compare_list):\n",
      "Gesamtanzahl Treffer: 5 in 2 Werken\n",
      "Anzahl verschiedener Begriffe: 2\n",
      "Top 10 Begriffe: [('trans', 4), ('genderfluid', 1)]\n",
      "\n",
      "Für Textkörper (compare_list):\n",
      "Gesamtanzahl Treffer: 2 in 2 Werken\n",
      "Anzahl verschiedener Begriffe: 2\n",
      "Top 10 Begriffe: [('sex change', 1), ('trans', 1)]\n",
      "\n",
      "Co-Occurrence Top Begriffe (compare_list):\n",
      "             trans  sex change  genderfluid\n",
      "trans            5           0            0\n",
      "sex change       0           1            0\n",
      "genderfluid      0           0            1\n",
      "\n",
      "Grafik zur Co-Occurrence gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_cooccurrence_compare.png\n",
      "Grafik Wordcloud gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_wordcloud_compare.png\n",
      "Grafik zu der Art der Matches pro Werk gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_hits_tags_vs_text_compare.png\n",
      "Grafik zur Verteilung der Matches nach Werken gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_hits_per_work_compare.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Breite & Tiefe der Matches – Statistik\n",
    "analysis_lines = []\n",
    "\n",
    "# Einschlägige Werke filtern\n",
    "df_relevant = df[df[\"is_relevant\"]].copy()\n",
    "\n",
    "# Hilfsfunktionen\n",
    "def aggregate_counters(series):\n",
    "    \"\"\"Alle Counter in einer Serie zusammenführen\"\"\"\n",
    "    total = Counter()\n",
    "    for c in series:\n",
    "        total.update(c)\n",
    "    return total\n",
    "\n",
    "def works_with_hits(series_of_counters):\n",
    "    \"\"\"Zählt, in wie vielen Dokumenten mindestens ein Treffer vorhanden ist\"\"\"\n",
    "    return sum(1 for c in series_of_counters if sum(c.values()) > 0)\n",
    "\n",
    "# Counter pro Werk auf Basis compare_list\n",
    "def get_compare_hits(row):\n",
    "    \"\"\"\n",
    "    Zählt Treffer in freeform, character und text nur für compare_list.\n",
    "    Verwendet dieselbe Greedy-Matching-Logik wie der Likelihood-Score\n",
    "    (count_term_hits), um konsistente Ergebnisse sicherzustellen.\n",
    "    \"\"\"\n",
    "    combined_counter = Counter()\n",
    "\n",
    "    # Freeform\n",
    "    freeform_tags = row.get(\"freeform\", [])\n",
    "    freeform_text = \" \".join(freeform_tags)\n",
    "    freeform_hits = count_term_hits(freeform_text, compare_list)\n",
    "    combined_counter.update(freeform_hits)\n",
    "\n",
    "    # Character\n",
    "    char_tags = row.get(\"character\", [])\n",
    "    char_text = \" \".join(char_tags)\n",
    "    char_hits = count_term_hits(char_text, compare_list)\n",
    "    combined_counter.update(char_hits)\n",
    "\n",
    "    # Text\n",
    "    text_content = row.get(\"text_norm_lem\", \"\")\n",
    "    text_hits = count_term_hits(text_content, compare_list)\n",
    "    combined_counter.update(text_hits)\n",
    "\n",
    "    return freeform_hits, char_hits, text_hits, combined_counter\n",
    "\n",
    "# Counters erzeugen\n",
    "df_relevant[[\n",
    "    \"freeform_hits_counter\",\n",
    "    \"character_hits_counter\",\n",
    "    \"text_hits_counter\",\n",
    "    \"combined_counter\"\n",
    "]] = df_relevant.apply(\n",
    "    lambda row: pd.Series(get_compare_hits(row)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Aggregierte Counter\n",
    "counter_tags = aggregate_counters(\n",
    "    df_relevant[\"freeform_hits_counter\"] +\n",
    "    df_relevant[\"character_hits_counter\"]\n",
    ")\n",
    "counter_text = aggregate_counters(df_relevant[\"text_hits_counter\"])\n",
    "counter_combined = aggregate_counters(df_relevant[\"combined_counter\"])\n",
    "\n",
    "# Statistikfunktion\n",
    "def term_stats(counter, series_of_counters, name):\n",
    "    total_terms = sum(counter.values())\n",
    "    unique_terms = len(counter)\n",
    "    works_count = works_with_hits(series_of_counters)\n",
    "    top_terms = counter.most_common(10)\n",
    "\n",
    "    analysis_lines.append(f\"Für {name}:\")\n",
    "    analysis_lines.append(f\"Gesamtanzahl Treffer: {total_terms} in {works_count} Werken\")\n",
    "    analysis_lines.append(f\"Anzahl verschiedener Begriffe: {unique_terms}\")\n",
    "    analysis_lines.append(f\"Top 10 Begriffe: {top_terms}\")\n",
    "    analysis_lines.append(\"\")\n",
    "\n",
    "# --- Auswertungen ---\n",
    "term_stats(\n",
    "    counter_tags,\n",
    "    df_relevant[\"freeform_hits_counter\"] +\n",
    "    df_relevant[\"character_hits_counter\"],\n",
    "    \"Tags (Freeform + Character, compare_list)\"\n",
    ")\n",
    "term_stats(\n",
    "    counter_text,\n",
    "    df_relevant[\"text_hits_counter\"],\n",
    "    \"Textkörper (compare_list)\"\n",
    ")\n",
    "\n",
    "# Co-Occurrence Matrix (Top 20 kombinierte Begriffe)\n",
    "top_n = 20\n",
    "top_terms_list = [t for t, _ in counter_combined.most_common(top_n)]\n",
    "\n",
    "co_matrix = pd.DataFrame(\n",
    "    0,\n",
    "    index=top_terms_list,\n",
    "    columns=top_terms_list\n",
    ")\n",
    "\n",
    "for counter in df_relevant[\"combined_counter\"]:\n",
    "    terms_in_doc = {t: counter[t] for t in counter if t in top_terms_list}\n",
    "    for t1, count1 in terms_in_doc.items():\n",
    "        co_matrix.loc[t1, t1] += count1\n",
    "        for t2, count2 in terms_in_doc.items():\n",
    "            if t1 != t2:\n",
    "                co_matrix.loc[t1, t2] += min(count1, count2)\n",
    "\n",
    "analysis_lines.append(\"Co-Occurrence Top Begriffe (compare_list):\")\n",
    "analysis_lines.append(co_matrix.to_string())\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Heatmap\n",
    "co_matrix_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_cooccurrence_compare.png\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(co_matrix, cmap=\"viridis\", annot=True, fmt=\"d\")\n",
    "plt.title(f\"Co-Occurrence Top 20 Begriffe für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(co_matrix_file)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(f\"Grafik zur Co-Occurrence gespeichert unter: {co_matrix_file}\")\n",
    "\n",
    "# Wordcloud\n",
    "wordcloud_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_wordcloud_compare.png\"\n",
    ")\n",
    "\n",
    "wc = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color=\"white\"\n",
    ").generate_from_frequencies(counter_combined)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Wordcloud für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(wordcloud_file)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(f\"Grafik Wordcloud gespeichert unter: {wordcloud_file}\")\n",
    "\n",
    "# Histogramm: Tags vs. Text pro Werk\n",
    "hist_file_tags_text = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_hits_tags_vs_text_compare.png\"\n",
    ")\n",
    "\n",
    "# Berechnung der Treffer pro Werk\n",
    "tags_counts = (\n",
    "    df_relevant[\"freeform_hits_counter\"].apply(lambda c: sum(c.values())) +\n",
    "    df_relevant[\"character_hits_counter\"].apply(lambda c: sum(c.values()))\n",
    ")\n",
    "text_counts = df_relevant[\"text_hits_counter\"].apply(lambda c: sum(c.values()))\n",
    "work_labels = df_relevant[\"work_id\"].astype(str)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(work_labels, tags_counts, label=\"Tags (Freeform + Character)\")\n",
    "plt.bar(work_labels, text_counts, bottom=tags_counts, label=\"Text\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Werk-ID\")\n",
    "plt.ylabel(\"Anzahl Treffer\")\n",
    "plt.title(f\"Art der Matches pro Werk für {OUTPUT_PREFIX}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(hist_file_tags_text)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(\n",
    "    f\"Grafik zu der Art der Matches pro Werk gespeichert unter: {hist_file_tags_text}\"\n",
    ")\n",
    "\n",
    "# Histogramm: Gesamt-Treffer pro Werk \n",
    "hist_file_total = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_hits_per_work_compare.png\"\n",
    ")\n",
    "\n",
    "total_hits_per_work = df_relevant[\"combined_counter\"].apply(lambda c: sum(c.values()))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(total_hits_per_work, bins=30, edgecolor=\"black\")\n",
    "plt.xlabel(\"Gesamtanzahl Treffer pro Werk\")\n",
    "plt.ylabel(\"Anzahl Werke\")\n",
    "plt.title(\"Verteilung der Treffer pro Werk (compare_list)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(hist_file_total)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(\n",
    "    f\"Grafik zur Verteilung der Matches nach Werken gespeichert unter: {hist_file_total}\"\n",
    ")\n",
    "\n",
    "# Analyseblock schreiben\n",
    "write_analysis_block(\n",
    "    title=\"Breite & Tiefe der Matches – Statistik (compare_list)\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9590fdc6-27e5-4e75-9fff-575ad16800bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_1312\\2268886027.py:37: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=rating_counts.index, y=rating_counts.values, palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 21:37:50] Matches nach Rating\n",
      "------------------------------------------------------------\n",
      "Matches nach Rating\n",
      "- Analyse, in welchem Rating die Werke zum Themenschwerpunkt enthalten sind\n",
      "\n",
      "Anzahl einschlägiger Werke pro Rating:\n",
      "general audiences: 3 Werke (100.0%)\n",
      "\n",
      "Relativer Anteil im Vergleich zur Gesamtstichprobe (normiert):\n",
      "explicit: Gesamt 12.98% | Einschlägig 0.0%\n",
      "general audiences: Gesamt 44.48% | Einschlägig 100.0%\n",
      "mature: Gesamt 8.29% | Einschlägig 0.0%\n",
      "not rated: Gesamt 6.08% | Einschlägig 0.0%\n",
      "teen and up audiences: Gesamt 28.18% | Einschlägig 0.0%\n",
      "\n",
      "Einschlägige Werke pro Rating gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_rating_counts.png\n",
      "Rating-Vergleich Gesamt vs. Einschlägig gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_rating_comparison.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matches vs Rating\n",
    "analysis_lines = []\n",
    "analysis_lines.append(\"Matches nach Rating\")\n",
    "analysis_lines.append(\"- Analyse, in welchem Rating die Werke zum Themenschwerpunkt enthalten sind\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# - Anzahl und Anteile einschlägiger Werke\n",
    "rating_counts = df_relevant[\"rating\"].value_counts()\n",
    "rating_percent = (rating_counts / len(df_relevant) * 100).round(2)\n",
    "\n",
    "analysis_lines.append(\"Anzahl einschlägiger Werke pro Rating:\")\n",
    "for r, count in rating_counts.items():\n",
    "    analysis_lines.append(f\"{r}: {count} Werke ({rating_percent[r]}%)\")\n",
    "\n",
    "# - Vergleich: Gesamt vs. einschlägig\n",
    "rating_compare = pd.DataFrame({\n",
    "    \"gesamt\": df[\"rating\"].value_counts(normalize=True),\n",
    "    \"einschlägig\": df_relevant[\"rating\"].value_counts(normalize=True)\n",
    "}).fillna(0)\n",
    "\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(\"Relativer Anteil im Vergleich zur Gesamtstichprobe (normiert):\")\n",
    "for r in rating_compare.index:\n",
    "    gesamt_pct = round(rating_compare.loc[r, \"gesamt\"]*100, 2)\n",
    "    relevant_pct = round(rating_compare.loc[r, \"einschlägig\"]*100, 2)\n",
    "    analysis_lines.append(f\"{r}: Gesamt {gesamt_pct}% | Einschlägig {relevant_pct}%\")\n",
    "\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Grafiken erstellen\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# - Absolute Anzahl einschlägiger Werke pro Rating\n",
    "fig1_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_rating_counts.png\")\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=rating_counts.index, y=rating_counts.values, palette=\"viridis\")\n",
    "plt.ylabel(\"Anzahl Werke\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.title(f\"Einschlägige Werke pro Rating für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig1_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Einschlägige Werke pro Rating gespeichert unter: {fig1_file}\")\n",
    "\n",
    "# - Relativer Vergleich: Gesamt vs. einschlägig\n",
    "fig2_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_rating_comparison.png\")\n",
    "rating_compare_plot = rating_compare.sort_index()  # optional nach Rating sortieren\n",
    "rating_compare_plot.plot(kind=\"bar\", figsize=(8,5))\n",
    "plt.ylabel(\"Anteil (normiert)\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.title(f\"Rating-Vergleich Gesamt vs. Einschlägig für {OUTPUT_PREFIX}\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig2_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Rating-Vergleich Gesamt vs. Einschlägig gespeichert unter: {fig2_file}\")\n",
    "\n",
    "# Ausgabeblock schreiben\n",
    "write_analysis_block(\n",
    "    title=\"Matches nach Rating\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "153377f3-04b4-4606-99a4-225587272d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_1312\\3656765683.py:97: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"count\", y=\"character\", data=top_characters, palette=\"viridis\")\n",
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_1312\\3656765683.py:108: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"fraction_relevant\", y=\"character\", data=comparison_df.head(15), palette=\"coolwarm\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 21:37:51] Matches vs Character – kombiniert compare_list + character_list\n",
      "------------------------------------------------------------\n",
      "Figuren im Themenzusammenhang- Analyse der Figuren in einschlägigen Werken\n",
      "\n",
      "\n",
      "Figuren mit explizitem Bezug\n",
      "Top 15 Figuren mit explizitem Bezug (inkl. Work IDs):\n",
      "1. genderfluid!teddy (1 Werke)\n",
      "Work IDs: 5103935\n",
      "2. trans_sirius_black (1 Werke)\n",
      "Work IDs: 4926748\n",
      "\n",
      "Figuren mit implizitem Bezug (Vorkommen einschlägigen Texten)\n",
      "Top 15 Figuren in einschlägigen Werken:\n",
      "1. draco_malfoy: 2 Werke\n",
      "2. harry_potter: 2 Werke\n",
      "3. author___character: 1 Werke\n",
      "4. teddy_lupin: 1 Werke\n",
      "5. sirius_black: 1 Werke\n",
      "6. minerva_mcgonagall: 1 Werke\n",
      "\n",
      "Relativer Anteil einschlägige vs. alle Werke (mind. 5 Werke insgesamt):\n",
      "2. teddy_lupin: 1 / 7 Werke (12.5 % in relevanten Werken)\n",
      "15. minerva_mcgonagall: 1 / 13 Werke (7.1 % in relevanten Werken)\n",
      "12. draco_malfoy: 2 / 86 Werke (2.3 % in relevanten Werken)\n",
      "29. harry_potter: 2 / 127 Werke (1.6 % in relevanten Werken)\n",
      "33. sirius_black: 1 / 68 Werke (1.4000000000000001 % in relevanten Werken)\n",
      "1. bellatrix_black_lestrange: 0 / 5 Werke (0.0 % in relevanten Werken)\n",
      "3. albus_severus_potter: 0 / 8 Werke (0.0 % in relevanten Werken)\n",
      "4. lucius_malfoy: 0 / 14 Werke (0.0 % in relevanten Werken)\n",
      "5. rose_weasley: 0 / 6 Werke (0.0 % in relevanten Werken)\n",
      "9. gregory_goyle: 0 / 5 Werke (0.0 % in relevanten Werken)\n",
      "8. horace_slughorn: 0 / 5 Werke (0.0 % in relevanten Werken)\n",
      "7. luna_lovegood: 0 / 22 Werke (0.0 % in relevanten Werken)\n",
      "6. peter_pettigrew: 0 / 14 Werke (0.0 % in relevanten Werken)\n",
      "13. remus_lupin: 0 / 72 Werke (0.0 % in relevanten Werken)\n",
      "14. pansy_parkinson: 0 / 12 Werke (0.0 % in relevanten Werken)\n",
      "\n",
      "Grafik der Top Figuren gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_topcharacters.png\n",
      "Grafik zum relativen Vorkommen (relevant vs. nicht relevant) gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_relativecharacterrelevance.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matches vs Character \n",
    "\n",
    "analysis_lines = []\n",
    "analysis_lines.append(\"Figuren im Themenzusammenhang- Analyse der Figuren in einschlägigen Werken\")\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# df_relevant und df_nonrelevant\n",
    "df_relevant = df[df[\"is_relevant\"]].copy()\n",
    "df_nonrelevant = df[~df[\"is_relevant\"]].copy()\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Figuren mit explizitem Bezug\n",
    "analysis_lines.append(\"Figuren mit explizitem Bezug\")\n",
    "\n",
    "combined_tags_counter = Counter()\n",
    "combined_tags_work_ids = defaultdict(set) \n",
    "\n",
    "for idx, row in df_relevant.iterrows():\n",
    "    work_id = row[\"work_id\"]\n",
    "    all_tags = row.get(\"freeform\", []) + row.get(\"character\", [])\n",
    "\n",
    "    for tag in all_tags:\n",
    "        # Normalisierung\n",
    "        tag_norm = tag.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "        \n",
    "        # Prüfen: mindestens ein Begriff aus compare_list UND ein Begriff aus character_list enthalten\n",
    "        if any(term in tag_norm for term in compare_list) and any(char in tag_norm for char in character_list):\n",
    "            combined_tags_counter[tag] += 1\n",
    "            combined_tags_work_ids[tag].add(work_id)\n",
    "\n",
    "# Top 15 kombinierte Tags\n",
    "top_combined_tags = pd.DataFrame(\n",
    "    [\n",
    "        (tag, combined_tags_counter[tag], sorted(combined_tags_work_ids[tag]))\n",
    "        for tag, _ in combined_tags_counter.most_common(15)\n",
    "    ],\n",
    "    columns=[\"tag\", \"count\", \"work_ids\"]\n",
    ")\n",
    "\n",
    "# Ausgabe\n",
    "analysis_lines.append(\"Top 15 Figuren mit explizitem Bezug (inkl. Work IDs):\")\n",
    "for idx, row in top_combined_tags.iterrows():\n",
    "    work_ids_str = \", \".join(str(wid) for wid in row[\"work_ids\"])\n",
    "    analysis_lines.append(f\"{idx+1}. {row['tag']} ({row['count']} Werke)\")\n",
    "    analysis_lines.append(f\"Work IDs: {work_ids_str}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Implizites Vorkommen\n",
    "analysis_lines.append(\"Figuren mit implizitem Bezug (Vorkommen einschlägigen Texten)\")\n",
    "char_counter = Counter()\n",
    "for chars in df_relevant[\"character\"]:\n",
    "    char_counter.update(chars)\n",
    "\n",
    "top_characters = pd.DataFrame(\n",
    "    char_counter.most_common(15),\n",
    "    columns=[\"character\", \"count\"]\n",
    ")\n",
    "\n",
    "analysis_lines.append(\"Top 15 Figuren in einschlägigen Werken:\")\n",
    "for idx, row in top_characters.iterrows():\n",
    "    analysis_lines.append(f\"{idx+1}. {row['character']}: {row['count']} Werke\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Relatives Vorkommen einschlägig vs. alle Werke\n",
    "char_relevant = Counter()\n",
    "char_nonrelevant = Counter()\n",
    "\n",
    "for chars in df_relevant[\"character\"]:\n",
    "    char_relevant.update(chars)\n",
    "for chars in df_nonrelevant[\"character\"]:\n",
    "    char_nonrelevant.update(chars)\n",
    "\n",
    "comparison = []\n",
    "for char in set(list(char_relevant.keys()) + list(char_nonrelevant.keys())):\n",
    "    count_rel = char_relevant.get(char, 0)\n",
    "    count_nonrel = char_nonrelevant.get(char, 0)\n",
    "    if count_rel + count_nonrel >= 5:\n",
    "        rel_fraction = count_rel / (count_rel + count_nonrel)\n",
    "        comparison.append((char, count_rel, count_nonrel, round(rel_fraction, 3)))\n",
    "\n",
    "comparison_df = pd.DataFrame(\n",
    "    comparison, columns=[\"character\", \"relevant_count\", \"nonrelevant_count\", \"fraction_relevant\"]\n",
    ").sort_values(\"fraction_relevant\", ascending=False)\n",
    "\n",
    "analysis_lines.append(\"Relativer Anteil einschlägige vs. alle Werke (mind. 5 Werke insgesamt):\")\n",
    "for idx, row in comparison_df.head(15).iterrows():\n",
    "    analysis_lines.append(\n",
    "        f\"{idx+1}. {row['character']}: {row['relevant_count']} / {row['nonrelevant_count']} Werke \"\n",
    "        f\"({row['fraction_relevant']*100} % in relevanten Werken)\"\n",
    "    )\n",
    "\n",
    "# Grafiken\n",
    "fig_chars_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_topcharacters.png\")\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"count\", y=\"character\", data=top_characters, palette=\"viridis\")\n",
    "plt.xlabel(\"Anzahl einschlägiger Werke\")\n",
    "plt.ylabel(\"Figur\")\n",
    "plt.title(f\"Top 15 Figuren in einschlägigen Werken für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_chars_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"\\nGrafik der Top Figuren gespeichert unter: {fig_chars_file}\")\n",
    "\n",
    "fig_chars_rel_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_relativecharacterrelevance.png\")\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"fraction_relevant\", y=\"character\", data=comparison_df.head(15), palette=\"coolwarm\")\n",
    "plt.xlabel(\"Anteil in relevanten Werken\")\n",
    "plt.ylabel(\"Figur\")\n",
    "plt.title(f\"Figuren-Anteil in einschlägigen vs. allen Texten für {OUTPUT_PREFIX}\")\n",
    "plt.xlim(0,1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_chars_rel_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Grafik zum relativen Vorkommen (relevant vs. nicht relevant) gespeichert unter: {fig_chars_rel_file}\")\n",
    "\n",
    "# Ausgabeblock schreiben\n",
    "write_analysis_block(\n",
    "    title=\"Matches vs Character – kombiniert compare_list + character_list\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f00e6-beab-407c-abde-e19b646faeaa",
   "metadata": {},
   "source": [
    "### 6. Topic Modeling zur Einbettung des Themas im Gesamtkontext\n",
    "\n",
    "Im Rahmen dieses Projekts werden die Topic-Modeling-Analysen getrennt für **narrative Volltexte** und **paratextuelle Tags** durchgeführt, um die Haupt-Topics zu identifizieren. Zudem wird unterschieden zwischen: **allen Werken** und **einschlägigen Werken** (positiver Trans-Bezug), um zu prüfen, ob Trans-Themen tatsächlich dominieren und welche Topics damit verknüpft sind. Die vollständigen Topic-Zuordnungen werden im Ordner *Ausgabedokumente* gespeichert.\n",
    "\n",
    "**Vorverarbeitung** \n",
    "- Zusammenstellung des Textkorpus, ggf. Filter nach einschlägigen Texten  \n",
    "- Tokenisierung & Vectorisierung mit `CountVectorizer`\n",
    "  - `ngram_range = (1,2)`  \n",
    "  - `max_df = 0.65` (Wörter, die in mehr als 65% der Dokumente vorkommen, werden ignoriert - anpassbar!)  \n",
    "  - `min_df = 5` (Wörter, die in weniger als 5 Dokumenten vorkommen, werden ignoriert - anpassbar!)  \n",
    "- Frequenzfilterung des Vokabulars (seltene & häufige Begriffe)  \n",
    "- Stopwordfilterung über `stopword_list` und ggf. `character_list` (ACHTUNG: bei iterativer Anpassung der stopword_list muss diese neu eingebunden werden!)  \n",
    "- Ausschluss sehr seltener (`min_df=2`) und extrem häufiger Wörter (`max_df=0.5`)  \n",
    "- Erstellung der Bag-of-Words-Matrix  \n",
    "\n",
    "**Topic Modeling mit atent Dirichlet Allocation (LDA)**  \n",
    "- Trainieren eines LDA-Modells mit  \n",
    "  - `n_topics_all = 8` Anzahl an Topics, die gebildet werden (anpassbar! weniger Topics --> breitere, übergreifende Themen)  \n",
    "  - `top_n = 10` Anzahl an Wörter, die ein Topic enthält  (anpassbar! weniger Wörter --> klarere Themen)\n",
    "  - `learning_method = 'batch'`  (anpassbar!)  \n",
    "  - `random_state = 42` (Reproduzierbarkeit)  \n",
    "  - `max_iter = 15` (anpassbar! Erhöhung kann zu saubereren Themen führen, braucht aber mehr Rechenzeit)  \n",
    "- Extraktion der Top-Wörter pro Topic  \n",
    "- Berechnung der Topic-Verteilung pro Werk (`doc_topic_all`)  \n",
    "- Dominanzanalyse mit zwei Schwellenwerten:  \n",
    "  - **≥ 0.5** → stark dominantes Topic    \n",
    "  - **≥ 0.2** → präsentes Topic  \n",
    "\n",
    "\n",
    "#### Aufbau\n",
    "**Zelle 1:** Topic Modeling für alle Werke (Text)  \n",
    "**Zelle 2:** Topic Modeling für einschlägige Werke (Text)  \n",
    "**Zelle 3:** Topic Modeling für alle Werke (Tags)  \n",
    "**Zelle 4:** Topic Modeling für einschlägige Werke (Tags)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2e9b39ae-8308-483b-ac8a-d6f7573f91d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_1312\\3916013526.py:91: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=topic_means.values, y=topic_means.index, palette=\"viridis\")\n",
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_1312\\3916013526.py:105: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 21:37:57] Topic Modeling - Zusammenfassung über alle Werke (Texte)\n",
      "------------------------------------------------------------\n",
      "Charakter-Stopwords kombiniert: True\n",
      "Stopword-Liste für Topic Modeling verwendet: 7197 Begriffe\n",
      "Vokabulargröße nach Stopword-Filterung: 4517\n",
      "Topic Modeling – Top Wörter pro Topic:\n",
      "\n",
      "Topic 1: dare, muggle, truth, pint, magical, pure, blond, present, win, strong\n",
      "\n",
      "Topic 2: erection, piano, player, orgasm, haired, werewolf, trailed, member, sweaty, quidditch\n",
      "\n",
      "Topic 3: fear, library, died, gift, quidditch, fight, dead, muggle, figure, hoping\n",
      "\n",
      "Topic 4: floo, problem, muggle, firmly, healer, hoping, trouble, angry, mirror, promise\n",
      "\n",
      "Topic 5: lake, headmaster, path, detention, branch, forbidden, sleeping, afraid, hufflepuff, cloak\n",
      "\n",
      "Topic 6: department, muggle, creep, mystery, department mystery, giant, dated, study, magical, habit\n",
      "\n",
      "Topic 7: orgasm, firmly, fantasy, opening, erection, unable, breaking, intense, reaction, relaxed\n",
      "\n",
      "Topic 8: afraid, muggle, fear, dead, fight, fix, hoped, power, comfort, cloak\n",
      "\n",
      "Anzahl der Werke, in denen die Topics stark dominierend sind (>=0.5):\n",
      "Topic 1: 32 Werke (8.8%)\n",
      "Topic 2: 24 Werke (6.6%)\n",
      "Topic 3: 44 Werke (12.2%)\n",
      "Topic 4: 92 Werke (25.4%)\n",
      "Topic 5: 35 Werke (9.7%)\n",
      "Topic 6: 24 Werke (6.6%)\n",
      "Topic 7: 25 Werke (6.9%)\n",
      "Topic 8: 44 Werke (12.2%)\n",
      "\n",
      "Anzahl der Werke, in denen die Topics präsent sind (>=0.2):\n",
      "Topic 1: 48 Werke (13.3%)\n",
      "Topic 2: 44 Werke (12.2%)\n",
      "Topic 3: 76 Werke (21.0%)\n",
      "Topic 4: 136 Werke (37.6%)\n",
      "Topic 5: 45 Werke (12.4%)\n",
      "Topic 6: 30 Werke (8.3%)\n",
      "Topic 7: 55 Werke (15.2%)\n",
      "Topic 8: 58 Werke (16.0%)\n",
      "\n",
      "Vollständige Topic-Verteilung für alle Werke gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_topicmodeling_text_all.csv\n",
      "\n",
      "Grafik zum durchschnittlichen Topic-Anteil gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_topic_text_all_means.png\n",
      "Grafik zu den dominanten Topics pro Werk gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_topic_text_all_dominant.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Topic Modeling für alle Werke (Text)\n",
    "analysis_lines = []\n",
    "\n",
    "# Texte aller Werke\n",
    "texts_all = df[\"text_topic\"].tolist()\n",
    "\n",
    "# Stopwords\n",
    "all_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "all_stopwords.update(stopword_list)\n",
    "USE_CHARACTER_STOPWORDS = True  \n",
    "analysis_lines.append(f\"Charakter-Stopwords kombiniert: {USE_CHARACTER_STOPWORDS}\")\n",
    "\n",
    "if USE_CHARACTER_STOPWORDS:\n",
    "    all_stopwords.update(character_list)\n",
    "\n",
    "analysis_lines.append(f\"Stopword-Liste für Topic Modeling verwendet: {len(all_stopwords)} Begriffe\")\n",
    "\n",
    "# Bag-of-Words\n",
    "vectorizer_all = CountVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    max_df=0.60,\n",
    "    min_df=5,\n",
    "    stop_words=list(all_stopwords)\n",
    ")\n",
    "X_all = vectorizer_all.fit_transform(texts_all)\n",
    "analysis_lines.append(f\"Vokabulargröße nach Stopword-Filterung: {len(vectorizer_all.get_feature_names_out())}\")\n",
    "\n",
    "# LDA-Modell\n",
    "n_topics_all = 8 # Anzahl an Topics, die gebildet werden\n",
    "top_n = 10 # Anzahl an Wörter, die ein Topic enthält\n",
    "\n",
    "lda_all = LatentDirichletAllocation(\n",
    "    n_components=n_topics_all,\n",
    "    max_iter=15,\n",
    "    learning_method='batch',\n",
    "    random_state=42\n",
    ")\n",
    "lda_all.fit(X_all)\n",
    "\n",
    "# Topics extrahieren\n",
    "def get_topics(model, vectorizer, top_n):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topic_words = {}\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        top_words = [words[i] for i in topic.argsort()[-top_n:][::-1]]\n",
    "        topic_words[f\"Topic {idx+1}\"] = top_words\n",
    "    return topic_words\n",
    "\n",
    "topic_words = get_topics(lda_all, vectorizer_all, top_n)\n",
    "analysis_lines.append(\"Topic Modeling – Top Wörter pro Topic:\")\n",
    "analysis_lines.append(\"\")\n",
    "for t, words in topic_words.items():\n",
    "    analysis_lines.append(f\"{t}: {', '.join(words)}\")\n",
    "    analysis_lines.append(\"\")\n",
    "\n",
    "# Topic-Verteilung pro Work\n",
    "doc_topic_all = lda_all.transform(X_all)\n",
    "\n",
    "# Dominanzanalyse\n",
    "dominant_threshold_5 = 0.5\n",
    "topic_counts_5 = (doc_topic_all > dominant_threshold_5).sum(axis=0)\n",
    "analysis_lines.append(\"Anzahl der Werke, in denen die Topics stark dominierend sind (>=0.5):\")\n",
    "for i, count in enumerate(topic_counts_5):\n",
    "    analysis_lines.append(f\"Topic {i+1}: {count} Werke ({round(100*count/len(df),1)}%)\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "dominant_threshold_3 = 0.2\n",
    "topic_counts_3 = (doc_topic_all > dominant_threshold_3).sum(axis=0)\n",
    "analysis_lines.append(f\"Anzahl der Werke, in denen die Topics präsent sind (>={dominant_threshold_3}):\")\n",
    "for i, count in enumerate(topic_counts_3):\n",
    "    analysis_lines.append(f\"Topic {i+1}: {count} Werke ({round(100*count/len(df),1)}%)\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# CSV-Ausgabe der vollständigen Topic-Verteilung\n",
    "output_path = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_topicmodeling_text_all.csv\"\n",
    ")\n",
    "df_doc_topic_all = pd.DataFrame(\n",
    "    doc_topic_all,\n",
    "    columns=[f\"Topic_{i+1}\" for i in range(n_topics_all)]\n",
    ")\n",
    "df_doc_topic_all[\"work_id\"] = df[\"work_id\"].values\n",
    "df_doc_topic_all.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "analysis_lines.append(f\"Vollständige Topic-Verteilung für alle Werke gespeichert unter: {output_path}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Visualisierung: Durchschnittlicher Topic-Anteil\n",
    "topic_means = df_doc_topic_all.drop(columns=\"work_id\").mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=topic_means.values, y=topic_means.index, palette=\"viridis\")\n",
    "plt.xlabel(\"Durchschnittlicher Topic-Anteil\")\n",
    "plt.ylabel(\"Topic\")\n",
    "plt.title(f\"Relevanz der Topics im Korpus für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "topic_means_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_topic_text_all_means.png\")\n",
    "plt.savefig(topic_means_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Grafik zum durchschnittlichen Topic-Anteil gespeichert unter: {topic_means_file}\")\n",
    "\n",
    "# Visualisierung: Dominantes Topic pro Werk\n",
    "dominant_topic = doc_topic_all.argmax(axis=1)\n",
    "dominant_counts = pd.Series(dominant_topic).value_counts().sort_index()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(\n",
    "    x=[f\"Topic {i+1}\" for i in dominant_counts.index],\n",
    "    y=dominant_counts.values,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.ylabel(\"Anzahl Werke\")\n",
    "plt.xlabel(\"Dominantes Topic\")\n",
    "plt.title(f\"Dominantes Topic pro Werk für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "dominant_topic_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_topic_text_all_dominant.png\")\n",
    "plt.savefig(dominant_topic_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Grafik zu den dominanten Topics pro Werk gespeichert unter: {dominant_topic_file}\")\n",
    "\n",
    "# In Analyse-Dokument schreiben\n",
    "write_analysis_block(\n",
    "    title=\"Topic Modeling - Zusammenfassung über alle Werke (Texte)\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bba8001f-7582-471c-bb46-6126515f4abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_1312\\2301763810.py:94: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=topic_means.values, y=topic_means.index, palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 22:43:08] Topic Modeling – Zusammenfassung einschlägige Werke (Text)\n",
      "------------------------------------------------------------\n",
      "Charakter-Stopwords kombiniert: True\n",
      "Stopword-Liste für Topic Modeling verwendet: 7197 Begriffe\n",
      "Vokabulargröße nach Stopword-Filterung: 617\n",
      "Topic Modeling – Top Wörter pro Topic (einschlägige Werke):\n",
      "\n",
      "Topic 1: rowling, pairing, average, hostess, creature, frustration, fight, fic, fanfic, epilogue\n",
      "\n",
      "Topic 2: attempt repair, assure hoped, assure, belonged, belonged climbed, understood certainty, attempt, troublemaker handful, understood, deteriorating\n",
      "\n",
      "Topic 3: attempt repair, assure hoped, assure, belonged, belonged climbed, understood certainty, attempt, troublemaker handful, understood, deteriorating\n",
      "\n",
      "Topic 4: dormitory dormitory, attempt, assure hoped, attempt repair, belonged, belonged climbed, understood certainty, understood, troublemaker handful, stern\n",
      "\n",
      "Topic 5: attempt repair, assure hoped, assure, belonged, belonged climbed, understood certainty, attempt, troublemaker handful, understood, deteriorating\n",
      "\n",
      "Topic 6: mistake, dorm, scared going, hug, afraid dormitory, afraid, atmosphere, atmosphere choosing, unnoticed scared, belongs\n",
      "\n",
      "Anzahl der Werke, in denen die Topics stark dominant sind (>=0.5):\n",
      "Topic 1: 1 Werke (33.3%)\n",
      "Topic 2: 0 Werke (0.0%)\n",
      "Topic 3: 0 Werke (0.0%)\n",
      "Topic 4: 1 Werke (33.3%)\n",
      "Topic 5: 0 Werke (0.0%)\n",
      "Topic 6: 1 Werke (33.3%)\n",
      "\n",
      "Anzahl der Werke, in denen die Topics präsent sind (>=0.2):\n",
      "Topic 1: 1 Werke (33.3%)\n",
      "Topic 2: 0 Werke (0.0%)\n",
      "Topic 3: 0 Werke (0.0%)\n",
      "Topic 4: 1 Werke (33.3%)\n",
      "Topic 5: 0 Werke (0.0%)\n",
      "Topic 6: 1 Werke (33.3%)\n",
      "\n",
      "Vollständige Topic-Verteilung für einschlägige Werke gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_topicmodeling_text_rel.csv\n",
      "\n",
      "Grafik zum durchschnittlichen Topic-Anteil gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_topic_text_rel_means.png\n",
      "Grafik zu den dominanten Topics pro Werk gespeichert unter: Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_topic_text_rel_dominant.png\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_1312\\2301763810.py:108: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    }
   ],
   "source": [
    "# Topic Modeling für einschlägige Werke (Text)\n",
    "analysis_lines = []\n",
    "\n",
    "# Einschlägige Werke filtern\n",
    "df_relevant = df[df[\"is_relevant\"]].copy()\n",
    "texts_relevant = df_relevant[\"text_topic\"].tolist()\n",
    "\n",
    "# Stopwords\n",
    "all_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "all_stopwords.update(stopword_list)\n",
    "\n",
    "# - Einstellung: Charakterliste als Stopwords nutzen?\n",
    "USE_CHARACTER_STOPWORDS = True \n",
    "analysis_lines.append(f\"Charakter-Stopwords kombiniert: {USE_CHARACTER_STOPWORDS}\")\n",
    "if USE_CHARACTER_STOPWORDS:\n",
    "    all_stopwords.update(character_list)\n",
    "\n",
    "analysis_lines.append(f\"Stopword-Liste für Topic Modeling verwendet: {len(all_stopwords)} Begriffe\")\n",
    "\n",
    "# Bag-of-Words\n",
    "vectorizer_rel = CountVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    max_df=0.60,\n",
    "    min_df=1,\n",
    "    stop_words=list(all_stopwords)\n",
    ")\n",
    "X_rel = vectorizer_rel.fit_transform(texts_relevant)\n",
    "analysis_lines.append(f\"Vokabulargröße nach Stopword-Filterung: {len(vectorizer_rel.get_feature_names_out())}\")\n",
    "\n",
    "# LDA-Modell\n",
    "n_topics_rel = 6\n",
    "top_n_words = 10\n",
    "\n",
    "lda_rel = LatentDirichletAllocation(\n",
    "    n_components=n_topics_rel,\n",
    "    max_iter=15,\n",
    "    learning_method='batch',\n",
    "    random_state=42\n",
    ")\n",
    "lda_rel.fit(X_rel)\n",
    "\n",
    "# Topics extrahieren\n",
    "def get_topics(model, vectorizer, top_n=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topic_words = {}\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        top_words = [words[i] for i in topic.argsort()[-top_n:][::-1]]\n",
    "        topic_words[f\"Topic {idx+1}\"] = top_words\n",
    "    return topic_words\n",
    "\n",
    "topic_words_rel = get_topics(lda_rel, vectorizer_rel, top_n_words)\n",
    "analysis_lines.append(\"Topic Modeling – Top Wörter pro Topic (einschlägige Werke):\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "for t, words in topic_words_rel.items():\n",
    "    analysis_lines.append(f\"{t}: {', '.join(words)}\")\n",
    "    analysis_lines.append(\"\")\n",
    "\n",
    "# Topic-Verteilung pro Dokument\n",
    "doc_topic_rel = lda_rel.transform(X_rel)\n",
    "\n",
    "# Dominanzanalyse\n",
    "dominant_threshold_5 = 0.5\n",
    "topic_counts_5 = (doc_topic_rel > dominant_threshold_5).sum(axis=0)\n",
    "analysis_lines.append(\"Anzahl der Werke, in denen die Topics stark dominant sind (>=0.5):\")\n",
    "for i, count in enumerate(topic_counts_5):\n",
    "    analysis_lines.append(f\"Topic {i+1}: {count} Werke ({round(100*count/len(df_relevant),1)}%)\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "dominant_threshold_3 = 0.2\n",
    "topic_counts_3 = (doc_topic_rel > dominant_threshold_3).sum(axis=0)\n",
    "analysis_lines.append(f\"Anzahl der Werke, in denen die Topics präsent sind (>={dominant_threshold_3}):\")\n",
    "for i, count in enumerate(topic_counts_3):\n",
    "    analysis_lines.append(f\"Topic {i+1}: {count} Werke ({round(100*count/len(df_relevant),1)}%)\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# CSV-Ausgabe der vollständigen Topic-Verteilung\n",
    "output_path_rel = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_topicmodeling_text_rel.csv\"\n",
    ")\n",
    "df_doc_topic_rel = pd.DataFrame(\n",
    "    doc_topic_rel,\n",
    "    columns=[f\"Topic_{i+1}\" for i in range(n_topics_rel)]\n",
    ")\n",
    "df_doc_topic_rel[\"work_id\"] = df_relevant[\"work_id\"].values\n",
    "df_doc_topic_rel.to_csv(output_path_rel, index=False, encoding=\"utf-8\")\n",
    "analysis_lines.append(f\"Vollständige Topic-Verteilung für einschlägige Werke gespeichert unter: {output_path_rel}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Visualisierung: Durchschnittlicher Topic-Anteil\n",
    "topic_means = df_doc_topic_rel.drop(columns=\"work_id\").mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=topic_means.values, y=topic_means.index, palette=\"viridis\")\n",
    "plt.xlabel(\"Durchschnittlicher Topic-Anteil\")\n",
    "plt.ylabel(\"Topic\")\n",
    "plt.title(f\"Relevanz der Topics (einschlägige Werke) für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "topic_means_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_topic_text_rel_means.png\")\n",
    "plt.savefig(topic_means_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Grafik zum durchschnittlichen Topic-Anteil gespeichert unter: {topic_means_file}\")\n",
    "\n",
    "# Visualisierung: Dominantes Topic pro Werk\n",
    "dominant_topic = doc_topic_rel.argmax(axis=1)\n",
    "dominant_counts = pd.Series(dominant_topic).value_counts().sort_index()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(\n",
    "    x=[f\"Topic {i+1}\" for i in dominant_counts.index],\n",
    "    y=dominant_counts.values,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.ylabel(\"Anzahl Werke\")\n",
    "plt.xlabel(\"Dominantes Topic\")\n",
    "plt.title(f\"Dominantes Topic pro Werk (einschlägige Werke) für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "dominant_topic_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_topic_text_rel_dominant.png\")\n",
    "plt.savefig(dominant_topic_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Grafik zu den dominanten Topics pro Werk gespeichert unter: {dominant_topic_file}\")\n",
    "\n",
    "# In Analyse-Dokument schreiben\n",
    "write_analysis_block(\n",
    "    title=\"Topic Modeling – Zusammenfassung einschlägige Werke (Text)\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6affe560-21ad-4f7a-8086-4eba5beab2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_1312\\1777593124.py:151: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n",
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_1312\\1777593124.py:177: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 22:43:11] Topic Modeling – Zusammenfassung aller Werke (Tags)\n",
      "------------------------------------------------------------\n",
      "Topic Modeling für alle Werke auf Basis von Tags.\n",
      "Anzahl berücksichtigter Werke mit Tags: 315\n",
      "\n",
      "Charakter-Stopwords kombiniert: True\n",
      "Stopword-Liste für Tags verwendet: 7197 Begriffe\n",
      "(Charakter-Namen berücksichtigt: True)\n",
      "Anzahl verschiedener Tags im Vokabular nach Stopword-Filterung: 945\n",
      "\n",
      "Top Tags pro Topic (alle Werke):\n",
      "\n",
      "Topic 1: drabble, angst, drabble_day_2015, marauders'_era, shoebox_project\n",
      "Topic 2: romance, alternate_universe, canon_compliant, fluff, het\n",
      "Topic 3: angst, fluff, emotional_hurt/comfort, established_relationship, friendship\n",
      "Topic 4: post_war, fluff, marauders'_era, anal_sex, hogwarts_eighth_year\n",
      "Topic 5: fluff, romance, pre_slash, crack, humor\n",
      "\n",
      "\n",
      "Anzahl der Werke mit dominantem Topic (>= 0.5):\n",
      "Topic 1: 60 Werke (19.0%)\n",
      "Topic 2: 58 Werke (18.4%)\n",
      "Topic 3: 73 Werke (23.2%)\n",
      "Topic 4: 55 Werke (17.5%)\n",
      "Topic 5: 64 Werke (20.3%)\n",
      "\n",
      "Anzahl der Werke mit präsentem Topic (>= 0.2):\n",
      "Topic 1: 70 Werke (22.2%)\n",
      "Topic 2: 66 Werke (21.0%)\n",
      "Topic 3: 79 Werke (25.1%)\n",
      "Topic 4: 62 Werke (19.7%)\n",
      "Topic 5: 67 Werke (21.3%)\n",
      "\n",
      "Vollständige Topic-Verteilung (Tags, alle Werke) gespeichert unter:\n",
      "Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_topicmodeling_tags_all.csv\n",
      "\n",
      "Grafik: Durchschnittlicher Topic-Anteil gespeichert unter:\n",
      "Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_topic_tags_all_means.png\n",
      "\n",
      "Grafik: Dominantes Topic pro Werk gespeichert unter:\n",
      "Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_topic_tags_all_dominant.png\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Topic Modeling für alle Werke (Tags)\n",
    "analysis_lines = []\n",
    "\n",
    "\n",
    "# Alle Werke betrachten\n",
    "df_all = df.copy()\n",
    "\n",
    "# Tags vorbereiten\n",
    "def normalize_tag(tag):\n",
    "    tag = str(tag).lower().strip()\n",
    "    tag = tag.replace(\" \", \"_\")\n",
    "    tag = tag.replace(\"-\", \"_\")\n",
    "    return tag\n",
    "\n",
    "# - Freeform-Tags normalisieren\n",
    "texts_tags_all = [\n",
    "    [normalize_tag(t) for t in tags if str(t).lower() != \"nan\"]\n",
    "    for tags in df_all[\"freeform\"]\n",
    "]\n",
    "\n",
    "# - Nur Werke mit mindestens einem Tag behalten\n",
    "valid_idx = [i for i, tags in enumerate(texts_tags_all) if len(tags) > 0]\n",
    "texts_tags_all = [texts_tags_all[i] for i in valid_idx]\n",
    "work_ids_tags_all = df_all.iloc[valid_idx][\"work_id\"].values\n",
    "\n",
    "analysis_lines.append(\"Topic Modeling für alle Werke auf Basis von Tags.\")\n",
    "analysis_lines.append(f\"Anzahl berücksichtigter Werke mit Tags: {len(texts_tags_all)}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Stopwords\n",
    "all_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "all_stopwords.update(stopword_list)\n",
    "USE_CHARACTER_STOPWORDS = True  # True = Charakter-Namen werden als Stopwords hinzugefügt, False = nicht\n",
    "analysis_lines.append(f\"Charakter-Stopwords kombiniert: {USE_CHARACTER_STOPWORDS}\")\n",
    "if USE_CHARACTER_STOPWORDS:\n",
    "    all_stopwords.update(character_list)\n",
    "\n",
    "analysis_lines.append(f\"Stopword-Liste für Tags verwendet: {len(all_stopwords)} Begriffe\")\n",
    "analysis_lines.append(f\"(Charakter-Namen berücksichtigt: {USE_CHARACTER_STOPWORDS})\")\n",
    "\n",
    "# Bag-of-Words\n",
    "vectorizer_tags_all = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    min_df=1\n",
    ")\n",
    "\n",
    "# Stopwords filtern\n",
    "texts_tags_all_filtered = [\n",
    "    [t for t in tags if t not in all_stopwords] for tags in texts_tags_all\n",
    "]\n",
    "\n",
    "X_tags_all = vectorizer_tags_all.fit_transform(texts_tags_all_filtered)\n",
    "\n",
    "analysis_lines.append(\n",
    "    f\"Anzahl verschiedener Tags im Vokabular nach Stopword-Filterung: {len(vectorizer_tags_all.get_feature_names_out())}\"\n",
    ")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# LDA-Modell\n",
    "n_topics_tags_all = 5\n",
    "top_n_words = 5\n",
    "\n",
    "lda_tags_all = LatentDirichletAllocation(\n",
    "    n_components=n_topics_tags_all,\n",
    "    max_iter=30,\n",
    "    learning_method=\"batch\",\n",
    "    random_state=42\n",
    ")\n",
    "lda_tags_all.fit(X_tags_all)\n",
    "\n",
    "# Topics extrahieren\n",
    "def get_topics(model, vectorizer, top_n):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topic_words = {}\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        top_words = [words[i] for i in topic.argsort()[-top_n:][::-1]]\n",
    "        topic_words[f\"Topic {idx+1}\"] = top_words\n",
    "    return topic_words\n",
    "\n",
    "topic_words_tags_all = get_topics(\n",
    "    lda_tags_all,\n",
    "    vectorizer_tags_all,\n",
    "    top_n_words\n",
    ")\n",
    "\n",
    "analysis_lines.append(\"Top Tags pro Topic (alle Werke):\")\n",
    "analysis_lines.append(\"\")\n",
    "for topic, words in topic_words_tags_all.items():\n",
    "    analysis_lines.append(f\"{topic}: {', '.join(words)}\")\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Topic-Verteilung pro Werk\n",
    "doc_topics_tags_all = lda_tags_all.transform(X_tags_all)\n",
    "\n",
    "# Dominanzanalyse\n",
    "dominant_threshold_5 = 0.5\n",
    "topic_counts_5 = (doc_topics_tags_all >= dominant_threshold_5).sum(axis=0)\n",
    "\n",
    "analysis_lines.append(\"Anzahl der Werke mit dominantem Topic (>= 0.5):\")\n",
    "for i, count in enumerate(topic_counts_5):\n",
    "    analysis_lines.append(\n",
    "        f\"Topic {i+1}: {count} Werke \"\n",
    "        f\"({round(100 * count / len(work_ids_tags_all), 1)}%)\"\n",
    "    )\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "dominant_threshold_3 = 0.2\n",
    "topic_counts_3 = (doc_topics_tags_all >= dominant_threshold_3).sum(axis=0)\n",
    "\n",
    "analysis_lines.append(f\"Anzahl der Werke mit präsentem Topic (>= {dominant_threshold_3}):\")\n",
    "for i, count in enumerate(topic_counts_3):\n",
    "    analysis_lines.append(\n",
    "        f\"Topic {i+1}: {count} Werke \"\n",
    "        f\"({round(100 * count / len(work_ids_tags_all), 1)}%)\"\n",
    "    )\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Vollständige Topic-Verteilung speichern\n",
    "df_doc_topics_tags_all = pd.DataFrame(\n",
    "    doc_topics_tags_all,\n",
    "    columns=[f\"Topic_{i+1}\" for i in range(n_topics_tags_all)]\n",
    ")\n",
    "df_doc_topics_tags_all[\"work_id\"] = work_ids_tags_all\n",
    "\n",
    "output_path_tags_all = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_topicmodeling_tags_all.csv\"\n",
    ")\n",
    "df_doc_topics_tags_all.to_csv(\n",
    "    output_path_tags_all,\n",
    "    index=False,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "analysis_lines.append(f\"Vollständige Topic-Verteilung (Tags, alle Werke) gespeichert unter:\")\n",
    "analysis_lines.append(output_path_tags_all)\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Visualisierung: Durchschnittlicher Topic-Anteil\n",
    "topic_means = (\n",
    "    df_doc_topics_tags_all\n",
    "    .drop(columns=\"work_id\")\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x=topic_means.values,\n",
    "    y=topic_means.index,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.xlabel(\"Durchschnittlicher Topic-Anteil\")\n",
    "plt.ylabel(\"Topic\")\n",
    "plt.title(f\"Tag-Topics – durchschnittliche Relevanz für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "\n",
    "topic_means_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_topic_tags_all_means.png\"\n",
    ")\n",
    "plt.savefig(topic_means_file)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(f\"Grafik: Durchschnittlicher Topic-Anteil gespeichert unter:\")\n",
    "analysis_lines.append(topic_means_file)\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Visualisierung: Dominantes Topic pro Werk\n",
    "dominant_topic = doc_topics_tags_all.argmax(axis=1)\n",
    "dominant_counts = pd.Series(dominant_topic).value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x=[f\"Topic {i+1}\" for i in dominant_counts.index],\n",
    "    y=dominant_counts.values,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.ylabel(\"Anzahl Werke\")\n",
    "plt.xlabel(\"Dominantes Topic\")\n",
    "plt.title(f\"Dominantes Tag-Topic pro Werk für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "\n",
    "dominant_topic_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_topic_tags_all_dominant.png\"\n",
    ")\n",
    "plt.savefig(dominant_topic_file)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(f\"Grafik: Dominantes Topic pro Werk gespeichert unter:\")\n",
    "analysis_lines.append(dominant_topic_file)\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# In Analyse-Dokument schreiben\n",
    "write_analysis_block(\n",
    "    title=\"Topic Modeling – Zusammenfassung aller Werke (Tags)\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c6b3ed1a-dfdc-4dc9-aec4-0cb3e73e3a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_1312\\1685032485.py:150: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-17 22:43:11] Topic Modeling – Zusammenfassung einschlägiger Werke (Tags)\n",
      "------------------------------------------------------------\n",
      "Topic Modeling für einschlägige Werke auf Basis von Tags.\n",
      "Anzahl berücksichtigter Werke mit Tags: 3\n",
      "\n",
      "Charakter-Stopwords kombiniert: True\n",
      "Stopword-Liste für Tags verwendet: 7197 Begriffe\n",
      "(Charakter-Namen berücksichtigt: True)\n",
      "Anzahl verschiedener Tags im Vokabular nach Stopword-Filterung: 15\n",
      "\n",
      "Top Tags pro Topic (einschlägige Werke):\n",
      "\n",
      "Topic 1: genderfluid!teddy, trans_male_character, trans_sirius_black, trans, trans_character\n",
      "Topic 2: genderfluid!teddy, trans_male_character, trans_sirius_black, trans, trans_character\n",
      "Topic 3: genderfluid!teddy, trans_male_character, trans_sirius_black, trans, trans_character\n",
      "Topic 4: seriously_pointless, sorry_not_sorry, spoilers, fun_with_tags, harry_is_confused\n",
      "Topic 5: trans_sirius_black, trans_male_character, trans_character, trans, genderfluid!teddy\n",
      "\n",
      "\n",
      "Anzahl der Werke mit dominantem Topic (>= 0.5):\n",
      "Topic 1: 1 Werke (33.3%)\n",
      "Topic 2: 0 Werke (0.0%)\n",
      "Topic 3: 0 Werke (0.0%)\n",
      "Topic 4: 1 Werke (33.3%)\n",
      "Topic 5: 1 Werke (33.3%)\n",
      "\n",
      "Anzahl der Werke mit präsentem Topic (>= 0.2):\n",
      "Topic 1: 1 Werke (33.3%)\n",
      "Topic 2: 0 Werke (0.0%)\n",
      "Topic 3: 0 Werke (0.0%)\n",
      "Topic 4: 1 Werke (33.3%)\n",
      "Topic 5: 1 Werke (33.3%)\n",
      "\n",
      "Vollständige Topic-Verteilung (Tags, einschlägige Werke) gespeichert unter:\n",
      "Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_topicmodeling_tags_rel.csv\n",
      "\n",
      "Grafik: Durchschnittlicher Topic-Anteil gespeichert unter:\n",
      "Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_topic_tags_rel_means.png\n",
      "\n",
      "Grafik: Dominantes Topic pro Werk gespeichert unter:\n",
      "Analysedokumente/2015\\Analysis_HP_FanFic_10_2015_graph_topic_tags_rel_dominant.png\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_1312\\1685032485.py:176: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    }
   ],
   "source": [
    "# Topic Modeling für einschlägige Werke (Tags)\n",
    "analysis_lines = []\n",
    "\n",
    "# Einschlägige Werke filtern\n",
    "df_relevant = df[df[\"is_relevant\"]].copy()\n",
    "\n",
    "# Tags vorbereiten\n",
    "def normalize_tag(tag):\n",
    "    tag = str(tag).lower().strip()\n",
    "    tag = tag.replace(\" \", \"_\")\n",
    "    tag = tag.replace(\"-\", \"_\")\n",
    "    return tag\n",
    "\n",
    "# - Freeform-Tags normalisieren\n",
    "texts_tags_rel = [\n",
    "    [normalize_tag(t) for t in tags if str(t).lower() != \"nan\"]\n",
    "    for tags in df_relevant[\"freeform\"]\n",
    "]\n",
    "\n",
    "# - Nur Werke mit mindestens einem Tag behalten\n",
    "valid_idx = [i for i, tags in enumerate(texts_tags_rel) if len(tags) > 0]\n",
    "texts_tags_rel = [texts_tags_rel[i] for i in valid_idx]\n",
    "work_ids_tags_rel = df_relevant.iloc[valid_idx][\"work_id\"].values\n",
    "\n",
    "analysis_lines.append(\"Topic Modeling für einschlägige Werke auf Basis von Tags.\")\n",
    "analysis_lines.append(f\"Anzahl berücksichtigter Werke mit Tags: {len(texts_tags_rel)}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Stopwords vorbereiten\n",
    "all_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "all_stopwords.update(stopword_list)\n",
    "USE_CHARACTER_STOPWORDS = True  # True = Charakter-Namen werden als Stopwords hinzugefügt, False = nicht\n",
    "analysis_lines.append(f\"Charakter-Stopwords kombiniert: {USE_CHARACTER_STOPWORDS}\")\n",
    "if USE_CHARACTER_STOPWORDS:\n",
    "    all_stopwords.update(character_list)\n",
    "\n",
    "analysis_lines.append(f\"Stopword-Liste für Tags verwendet: {len(all_stopwords)} Begriffe\")\n",
    "analysis_lines.append(f\"(Charakter-Namen berücksichtigt: {USE_CHARACTER_STOPWORDS})\")\n",
    "\n",
    "# - Stopwords auf Tags anwenden\n",
    "texts_tags_rel_filtered = [\n",
    "    [t for t in tags if t not in all_stopwords] for tags in texts_tags_rel\n",
    "]\n",
    "\n",
    "# Bag-of-Words\n",
    "vectorizer_tags_rel = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    min_df=1\n",
    ")\n",
    "\n",
    "X_tags_rel = vectorizer_tags_rel.fit_transform(texts_tags_rel_filtered)\n",
    "\n",
    "analysis_lines.append(\n",
    "    f\"Anzahl verschiedener Tags im Vokabular nach Stopword-Filterung: {len(vectorizer_tags_rel.get_feature_names_out())}\"\n",
    ")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# LDA-Modell\n",
    "n_topics_tags_rel = 5\n",
    "top_n_words = 5\n",
    "\n",
    "lda_tags_rel = LatentDirichletAllocation(\n",
    "    n_components=n_topics_tags_rel,\n",
    "    max_iter=30,\n",
    "    learning_method=\"batch\",\n",
    "    random_state=42\n",
    ")\n",
    "lda_tags_rel.fit(X_tags_rel)\n",
    "\n",
    "# Topics extrahieren\n",
    "def get_topics(model, vectorizer, top_n):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topic_words = {}\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        top_words = [words[i] for i in topic.argsort()[-top_n:][::-1]]\n",
    "        topic_words[f\"Topic {idx+1}\"] = top_words\n",
    "    return topic_words\n",
    "\n",
    "topic_words_tags_rel = get_topics(\n",
    "    lda_tags_rel,\n",
    "    vectorizer_tags_rel,\n",
    "    top_n_words\n",
    ")\n",
    "\n",
    "analysis_lines.append(\"Top Tags pro Topic (einschlägige Werke):\")\n",
    "analysis_lines.append(\"\")\n",
    "for topic, words in topic_words_tags_rel.items():\n",
    "    analysis_lines.append(f\"{topic}: {', '.join(words)}\")\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Topic-Verteilung pro Werk\n",
    "doc_topics_tags_rel = lda_tags_rel.transform(X_tags_rel)\n",
    "\n",
    "# Dominanzanalyse\n",
    "dominant_threshold_5 = 0.5\n",
    "topic_counts_5 = (doc_topics_tags_rel >= dominant_threshold_5).sum(axis=0)\n",
    "\n",
    "analysis_lines.append(\"Anzahl der Werke mit dominantem Topic (>= 0.5):\")\n",
    "for i, count in enumerate(topic_counts_5):\n",
    "    analysis_lines.append(\n",
    "        f\"Topic {i+1}: {count} Werke \"\n",
    "        f\"({round(100 * count / len(work_ids_tags_rel), 1)}%)\"\n",
    "    )\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "dominant_threshold_3 = 0.2\n",
    "topic_counts_3 = (doc_topics_tags_rel >= dominant_threshold_3).sum(axis=0)\n",
    "\n",
    "analysis_lines.append(f\"Anzahl der Werke mit präsentem Topic (>= {dominant_threshold_3}):\")\n",
    "for i, count in enumerate(topic_counts_3):\n",
    "    analysis_lines.append(\n",
    "        f\"Topic {i+1}: {count} Werke \"\n",
    "        f\"({round(100 * count / len(work_ids_tags_rel), 1)}%)\"\n",
    "    )\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Vollständige Topic-Verteilung speichern\n",
    "df_doc_topics_tags_rel = pd.DataFrame(\n",
    "    doc_topics_tags_rel,\n",
    "    columns=[f\"Topic_{i+1}\" for i in range(n_topics_tags_rel)]\n",
    ")\n",
    "df_doc_topics_tags_rel[\"work_id\"] = work_ids_tags_rel\n",
    "\n",
    "output_path_tags_rel = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_topicmodeling_tags_rel.csv\"\n",
    ")\n",
    "df_doc_topics_tags_rel.to_csv(\n",
    "    output_path_tags_rel,\n",
    "    index=False,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "analysis_lines.append(f\"Vollständige Topic-Verteilung (Tags, einschlägige Werke) gespeichert unter:\")\n",
    "analysis_lines.append(output_path_tags_rel)\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Visualisierung: Durchschnittlicher Topic-Anteil\n",
    "topic_means = (\n",
    "    df_doc_topics_tags_rel\n",
    "    .drop(columns=\"work_id\")\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x=topic_means.values,\n",
    "    y=topic_means.index,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.xlabel(\"Durchschnittlicher Topic-Anteil\")\n",
    "plt.ylabel(\"Topic\")\n",
    "plt.title(f\"Tag-Topics – durchschnittliche Relevanz für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "\n",
    "topic_means_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_topic_tags_rel_means.png\"\n",
    ")\n",
    "plt.savefig(topic_means_file)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(f\"Grafik: Durchschnittlicher Topic-Anteil gespeichert unter:\")\n",
    "analysis_lines.append(topic_means_file)\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Visualisierung: Dominantes Topic pro Werk\n",
    "dominant_topic = doc_topics_tags_rel.argmax(axis=1)\n",
    "dominant_counts = pd.Series(dominant_topic).value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x=[f\"Topic {i+1}\" for i in dominant_counts.index],\n",
    "    y=dominant_counts.values,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.ylabel(\"Anzahl Werke\")\n",
    "plt.xlabel(\"Dominantes Topic\")\n",
    "plt.title(f\"Dominantes Tag-Topic pro Werk für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "\n",
    "dominant_topic_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_topic_tags_rel_dominant.png\"\n",
    ")\n",
    "plt.savefig(dominant_topic_file)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(f\"Grafik: Dominantes Topic pro Werk gespeichert unter:\")\n",
    "analysis_lines.append(dominant_topic_file)\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# In Analyse-Dokument schreiben\n",
    "write_analysis_block(\n",
    "    title=\"Topic Modeling – Zusammenfassung einschlägiger Werke (Tags)\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30efbcd-2b9d-4aeb-8d4a-36e37e632717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
