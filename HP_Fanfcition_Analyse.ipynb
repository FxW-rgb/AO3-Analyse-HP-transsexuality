{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112971dd-17b3-4942-a584-66e69c9f8234",
   "metadata": {},
   "source": [
    "# Computergestützte Analyse von Fanfiction auf Archive of Our Own\n",
    "\n",
    "## Eine korpusbasierte Frequenz-, Topic- und Sentimentanalyse themenspezifischer Diskurse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9908faf-b2c0-488e-923c-7c4f7e6c35e7",
   "metadata": {},
   "source": [
    "Dieses Notebook wertet als CSV bereitgestellte Fanfiction-Daten der Plattform AO3 aus und ermöglicht die Untersuchung themenspezifischer Diskurse. Über die Bereitstellung eines Untersuchungsgegenstandes und einer Vergleichsliste kann es grundsätzlich für unterschiedliche Forschungsfragen genutzt werden. \n",
    "\n",
    "Mit dem Notebook lassen sich sowohl die Texte als auch die Metadaten getrennt voneinander analysieren. Es fokussiert dabei auf inhaltliche Zusammenhänge und berücksichtigt keine zeitlichen Dimensionen. Um zeitliche Entwicklungen nachvollziehen zu können, muss die Analyse daher jeweils getrennt für die einzelnen Zeiträume durchgeführt werden.\n",
    "\n",
    "Konkret wird untersucht:\n",
    "- Häufigkeit: Wie häufig werden über die Vergleichsliste definierte Themen aufgegriffen?\n",
    "- Thematische Einbettung: Welche Bedeutung haben diese Themen am Gesamt-Topic-Spektrum?\n",
    "- Valenz: Werden diese Themen überwiegend positiv, neutral oder negativ dargestellt?\n",
    "\n",
    "**Zentrale Annahmen** sind:\n",
    "- Fanfiction ist kein neutraler Raum, sondern reagiert auf externe Diskurse\n",
    "- AO3 Tags sind soziale Metadaten und spiegeln bewusste Selbstverortung der Autor*innen wieder\n",
    "- Textinhalte können implizite Repräsentationen enthalten, die nicht explizit getaggt sind\n",
    "\n",
    "**Vorgehen**:\n",
    "1. Allgemeines Setup und Einstellungen\n",
    "2. Einlesen und Aufbereiten des Untersuchungsgegenstands\n",
    "3. Einlesen von Vergleichsliste, Charakter-Liste und Stopwords\n",
    "4. Berechnung eines themenspezifischen Likelihood-Scores\n",
    "5. Allgemeine statistische Auswertung\n",
    "6. Topic Modeling zur Einbettung des Themas im Gesamtkontext\n",
    "7. Sentimentanalyse zur Bewertung des Themas in einschlägigen Texten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39441a-b57c-4519-bc2e-3c740a58ec90",
   "metadata": {},
   "source": [
    "### 1. Allgemeines Setup und Einstellungen\n",
    "**Zelle 1:** Installtion/Upgrade der erforderlichen Bibliotheken & Pakete *(optional)*  \n",
    "**Zelle 2:** Importieren der erforderlichen Bibliotheken & Pakete  \n",
    "**Zelle 3:** Definieren von Input und Output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a209d582-22dc-4ab0-9a80-53121bda88a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m5 packages\u001b[0m \u001b[2min 208ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 242ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 819ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.1\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 65ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 0.50ms\u001b[0m\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m6 packages\u001b[0m \u001b[2min 83ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m15 packages\u001b[0m \u001b[2min 150ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 31ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 96ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.1.0\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m12 packages\u001b[0m \u001b[2min 134ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m12 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sofern erforderlich: Installation/Upgrade der nachfolgenden Bibliotheken\n",
    "!uv pip install --upgrade scikit-learn\n",
    "!uv pip install --upgrade lxml\n",
    "!uv pip install --upgrade nltk\n",
    "!uv pip install --upgrade seaborn\n",
    "!uv pip install --upgrade wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34491619-2829-4371-9960-f53e18c387de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "from collections import Counter, defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# --- Third-party Libraries: Data & Math ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- NLP & Text Processing ---\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# --- Machine Learning / Topic Modeling ---\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfVectorizer,\n",
    "    ENGLISH_STOP_WORDS\n",
    ")\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58396b06-7435-45ea-8be9-39d33cf8ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "# - Pfad derjenigen Datei, die die Texte der Fanfiction enthält:\n",
    "TEXT_CSV = \"Ausgabedokumente/AO3_HP_FanFic_10_2025_text.csv\"\n",
    "# - Pfad derjenigen Datei, die die Metadaten wie Title, Rating, Category, Tags enthält:\n",
    "META_CSV = \"Ausgabedokumente/AO3_HP_FanFic_10_2025_meta.csv\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_PREFIX = \"Analysis_HP_FanFic_10_2025\"\n",
    "OUTPUT_FOLDER = \"Analysedokumente\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# - Anlegen des Analyse-Dokuments\n",
    "ANALYSIS_OUTPUT_PATH = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_analyse.txt\"\n",
    ")\n",
    "\n",
    "if os.path.exists(ANALYSIS_OUTPUT_PATH):\n",
    "    raise RuntimeError(\n",
    "        f\"Analyse-Datei existiert bereits:\\n{ANALYSIS_OUTPUT_PATH}\\n\"\n",
    "        \"Bitte umbenennen oder löschen.\"\n",
    "    )\n",
    "\n",
    "def now_ts():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "with open(ANALYSIS_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(f\"{OUTPUT_PREFIX} – Gesamtanalyse\\n\")\n",
    "    f.write(f\"Erstellt am: {now_ts()}\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "def write_analysis_block(title, lines, also_print=True):\n",
    "    ts = now_ts()\n",
    "    if isinstance(lines, str):\n",
    "        lines = [lines]\n",
    "    block = []\n",
    "    block.append(f\"[{ts}] {title}\")\n",
    "    block.append(\"-\" * 60)\n",
    "    block.extend(lines)\n",
    "    block.append(\"\")\n",
    "    text = \"\\n\".join(block)\n",
    "    if also_print:\n",
    "        print(text)\n",
    "    with open(ANALYSIS_OUTPUT_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fdff5f-de9f-43f4-804e-b506a7a2c3e2",
   "metadata": {},
   "source": [
    "### 2. Einlesen der Daten, Textnormalisierung und Tag-Bereinigung\n",
    "\n",
    "**Zelle 1:** Einlesen und Aufbereiten des Untersuchungsgegenstandes  \n",
    "**Zelle 2:** Prüfen der geladenen Datenstruktur *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b32549b8-d0cd-40fe-bca0-f79d3d35221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Uni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# CSV einlesen\n",
    "df_text = pd.read_csv(TEXT_CSV, encoding='utf-8-sig', quotechar='\"')\n",
    "df_meta = pd.read_csv(META_CSV, encoding='utf-8-sig', quotechar='\"')\n",
    "\n",
    "# Zusammenführung über WorkID\n",
    "df = df_meta.merge(df_text, on=\"work_id\", how=\"left\")\n",
    "df = df.fillna(\"NaN\")\n",
    "\n",
    "# Text normalisieren und lemmatisieren\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_and_lemmatize(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Unicode-Normalisierung\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    # Kleinbuchstaben\n",
    "    text = text.lower()\n",
    "    # Sonderzeichen entfernen\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    # Mehrere Leerzeichen reduzieren\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Lemmatisierung\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"text_norm_lem\"] = df[\"text\"].apply(normalize_and_lemmatize)\n",
    "\n",
    "\n",
    "def prepare_text_for_topics(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Zahlen entfernen\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    # sehr kurze Wörter entfernen (<=2 Zeichen)\n",
    "    text = re.sub(r\"\\b\\w{1,2}\\b\", \" \", text)\n",
    "    # Mehrfach-Leerzeichen\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"text_topic\"] = df[\"text_norm_lem\"].apply(prepare_text_for_topics)\n",
    "\n",
    "\n",
    "# Tag-Aufbereitung\n",
    "TAG_COLS = [\"category\", \"relationship\", \"character\", \"freeform\"]\n",
    "\n",
    "def split_tags(cell):\n",
    "    if not cell:\n",
    "        return []\n",
    "    return [t.strip().lower() for t in str(cell).split(\",\") if t.strip()]\n",
    "\n",
    "for col in TAG_COLS:\n",
    "    df[col] = df[col].apply(split_tags)\n",
    "\n",
    "def normalize_tag(tag):\n",
    "    tag = tag.lower().strip()\n",
    "    tag = tag.replace(\" \", \"_\")\n",
    "    tag = tag.replace(\"-\", \"_\")\n",
    "    return tag\n",
    "\n",
    "# --- NaN aus Freeform und Character-Tags entfernen ---\n",
    "for col in [\"freeform\", \"character\"]:\n",
    "    df[col] = df[col].apply(lambda tags: [normalize_tag(t) for t in tags if t not in [None, np.nan, \"nan\", \"NaN\"]])\n",
    "\n",
    "df[\"rating\"] = df[\"rating\"].str.strip().str.lower()\n",
    "df[\"num_characters\"] = df[\"character\"].apply(len)\n",
    "df[\"num_freeform_tags\"] = df[\"freeform\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ccd36f5-6e93-45e2-a136-fdf71d2ff6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-10 22:39:24] Datenprüfung – Umfang, Struktur, Darstellung\n",
      "------------------------------------------------------------\n",
      "\n",
      "Der Datensatz umfasst 2111 Werke und ist strukturiert nach:\n",
      "work_id, title, rating, category, relationship, character, freeform, text, text_norm_lem, text_topic, num_characters, num_freeform_tags\n",
      "\n",
      "------------------------------  nur im Notebook  ------------------------------\n",
      "Datenzugriff prüfen\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_id</th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>category</th>\n",
       "      <th>relationship</th>\n",
       "      <th>character</th>\n",
       "      <th>freeform</th>\n",
       "      <th>text</th>\n",
       "      <th>text_norm_lem</th>\n",
       "      <th>text_topic</th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_freeform_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76158526</td>\n",
       "      <td>Will Harry be able to walk ever again?</td>\n",
       "      <td>explicit</td>\n",
       "      <td>[m/m]</td>\n",
       "      <td>[harry potter/charlie weasley]</td>\n",
       "      <td>[harry_potter, charlie_weasley]</td>\n",
       "      <td>[sexual_overstimulation, plot_what_plot/porn_w...</td>\n",
       "      <td>\"Come Harry, let's go home.\" Ms. Weasley takes...</td>\n",
       "      <td>come harry let s go home m weasley take me tog...</td>\n",
       "      <td>come harry let home weasley take together with...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    work_id                                   title    rating category  \\\n",
       "0  76158526  Will Harry be able to walk ever again?  explicit    [m/m]   \n",
       "\n",
       "                     relationship                        character  \\\n",
       "0  [harry potter/charlie weasley]  [harry_potter, charlie_weasley]   \n",
       "\n",
       "                                            freeform  \\\n",
       "0  [sexual_overstimulation, plot_what_plot/porn_w...   \n",
       "\n",
       "                                                text  \\\n",
       "0  \"Come Harry, let's go home.\" Ms. Weasley takes...   \n",
       "\n",
       "                                       text_norm_lem  \\\n",
       "0  come harry let s go home m weasley take me tog...   \n",
       "\n",
       "                                          text_topic  num_characters  \\\n",
       "0  come harry let home weasley take together with...               2   \n",
       "\n",
       "   num_freeform_tags  \n",
       "0                  5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come harry let s go home m weasley take me together with all the other weasleys \n",
      "------------------------------  nur im Notebook  ------------------------------\n"
     ]
    }
   ],
   "source": [
    "analysis_lines = []\n",
    "\n",
    "# Umfang und Struktur des Datensatzes\n",
    "analysis_lines.append(f\"\\nDer Datensatz umfasst {len(df)} Werke und ist strukturiert nach:\")\n",
    "analysis_lines.append(\", \".join(df.columns.tolist()) )\n",
    "\n",
    "# Zentral ausgeben + protokollieren\n",
    "write_analysis_block(\n",
    "    title=\"Datenprüfung – Umfang, Struktur, Darstellung\",\n",
    "    lines=analysis_lines\n",
    ")\n",
    "\n",
    "# Zugriff prüfen:\n",
    "print(\"-\"*30, \" nur im Notebook \", \"-\"*30)\n",
    "print(\"Datenzugriff prüfen\")\n",
    "display(df.head(1))\n",
    "print(df[\"text_norm_lem\"].iloc[0][:80])\n",
    "print(\"-\"*30, \" nur im Notebook \", \"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5aa2e-35e2-4888-ac86-3805654c17c6",
   "metadata": {},
   "source": [
    "### 3. Einlesen von Vergleichsliste, Characterliste und Stopwords\n",
    "**Zelle 1:** Laden der Vergleichsliste aus dem Ordner \"HilfsDokumente\"   \n",
    "**Zelle 2:** Test der Wortliste *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff1100e3-2c26-4868-bbf2-e8adbdbf35d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-10 22:39:24] Listen Einlesen – Übersicht\n",
      "------------------------------------------------------------\n",
      "Vergleichsliste erfolgreich eingelesen.\n",
      "Dateipfad: HilfsDokumente/compare_list.txt\n",
      "Umfang der Liste (Vergleichsliste): 139\n",
      "\n",
      "Auszug aus Vergleichsliste (erste 10 Terme):\n",
      "trans exclusionary radical feminist, gender recognition certificate, gender confirmation surgeries, person with a trans history, assigned female at birth, transgender individuals, transgender individual, assigned male at birth, gender non confirming, gender non conformity\n",
      "\n",
      "Character-Liste erfolgreich eingelesen.\n",
      "Dateipfad: HilfsDokumente/character_list.txt\n",
      "Umfang der Liste (Character-Liste): 195\n",
      "\n",
      "Auszug aus Character-Liste (erste 10 Terme):\n",
      "gregorowitsch, crookshanks, shacklebolt, xenophilius, grindelwald, mcgonagall, ollivander, nymphadora, longbottom, scrimgeour\n",
      "\n",
      "Stopword-Liste erfolgreich eingelesen.\n",
      "Dateipfad: HilfsDokumente/stopword_list.txt\n",
      "Umfang der Liste (Stopword-Liste): 178\n",
      "\n",
      "Auszug aus Stopword-Liste (erste 10 Terme):\n",
      "relationship, disclaimer, charackter, remembered, responded, character, streaming, whispered, remember, laughing\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pfade zu den Dateien\n",
    "comparison_file = \"HilfsDokumente/compare_list.txt\"\n",
    "character_file = \"HilfsDokumente/character_list.txt\"\n",
    "stopword_file = \"HilfsDokumente/stopword_list.txt\"\n",
    "\n",
    "analysis_lines = []\n",
    "\n",
    "# --- Funktion zum Einlesen und Prüfen einer Liste ---\n",
    "def load_list(file_path, list_name):\n",
    "    lines = []\n",
    "    title = \"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            terms = set(line.strip().lower() for line in f if line.strip())\n",
    "\n",
    "        if not terms:\n",
    "            raise ValueError(f\"{list_name} wurde gelesen, enthält aber keine gültigen Terme.\")\n",
    "\n",
    "        terms_sorted = sorted(terms, key=len, reverse=True)\n",
    "\n",
    "        lines.append(f\"{list_name} erfolgreich eingelesen.\")\n",
    "        lines.append(f\"Dateipfad: {file_path}\")\n",
    "        lines.append(f\"Umfang der Liste ({list_name}): {len(terms_sorted)}\")\n",
    "\n",
    "        # Vorschau\n",
    "        preview_n = 10\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"Auszug aus {list_name} (erste {preview_n} Terme):\")\n",
    "        lines.append(\", \".join(terms_sorted[:preview_n]))\n",
    "        lines.append(\"\")\n",
    "\n",
    "        title = f\"{list_name} – erfolgreich geladen\"\n",
    "        return terms_sorted, lines, title\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        lines.append(f\"FEHLER: {list_name} konnte nicht gefunden werden.\")\n",
    "        lines.append(f\"Erwarteter Pfad: {file_path}\")\n",
    "        lines.append(\"\")\n",
    "        title = f\"{list_name} – FEHLER (Datei nicht gefunden)\"\n",
    "        return [], lines, title\n",
    "\n",
    "    except Exception as e:\n",
    "        lines.append(f\"FEHLER beim Einlesen von {list_name}:\")\n",
    "        lines.append(str(e))\n",
    "        lines.append(\"\")\n",
    "        title = f\"{list_name} – FEHLER (Einlesen fehlgeschlagen)\"\n",
    "        return [], lines, title\n",
    "\n",
    "\n",
    "# --- Vergleichsliste einlesen ---\n",
    "compare_list, compare_lines, compare_title = load_list(comparison_file, \"Vergleichsliste\")\n",
    "analysis_lines.extend(compare_lines)\n",
    "\n",
    "# --- Character-Liste einlesen ---\n",
    "character_list, character_lines, character_title = load_list(character_file, \"Character-Liste\")\n",
    "analysis_lines.extend(character_lines)\n",
    "\n",
    "# --- Stopword-Liste einlesen ---\n",
    "stopword_list, stopword_lines, stopword_title = load_list(stopword_file, \"Stopword-Liste\")\n",
    "analysis_lines.extend(stopword_lines)\n",
    "\n",
    "# --- Zentral ausgeben + protokollieren ---\n",
    "write_analysis_block(\n",
    "    title=\"Listen Einlesen – Übersicht\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a16c8aa-963d-49a1-9bf6-0a74ebed2e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Begriffe mit Häufigkeit:\n",
      "assigned at birth: 2\n",
      "gender identity: 2\n",
      "transgender: 5\n"
     ]
    }
   ],
   "source": [
    "# Test der Vergleichsliste\n",
    "# - TestText\n",
    "text1 = \"\"\"\n",
    "Transgender is an adjective to describe people whose gender identity differs\n",
    "from the sex they were assigned at birth. \n",
    "People who are transgender may also use other terms, in addition to transgender, \n",
    "to describe their gender more specifically. \n",
    "Use the term(s) the person uses to describe their gender. \n",
    "It is important to note that being transgender is not dependent upon \n",
    "physical appearance or medical procedures. \n",
    "A person can call themself transgender the moment they realize \n",
    "that their gender identity is different than the sex they were assigned at birth.\n",
    "\"\"\"\n",
    "\n",
    "text1_lower = text1.lower()\n",
    "matches = []\n",
    "matches_counter = Counter()\n",
    "\n",
    "# - Greedy-Matching\n",
    "for term in compare_list:\n",
    "    term_escaped = re.escape(term)\n",
    "    # Alle Vorkommen zählen\n",
    "    found = re.findall(rf'\\b{term_escaped}\\b', text1_lower)\n",
    "    if found:\n",
    "        matches_counter[term] += len(found)\n",
    "        # Term aus Text entfernen, um Doppelzählungen zu vermeiden\n",
    "        text1_lower = re.sub(rf'\\b{term_escaped}\\b', ' ', text1_lower)\n",
    "\n",
    "# - Ausgabe\n",
    "print(\"Gefundene Begriffe mit Häufigkeit:\")\n",
    "for term, count in matches_counter.items():\n",
    "    print(f\"{term}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50795de5-54a3-4348-8f12-c3485accf042",
   "metadata": {},
   "source": [
    "### 4. Berechnung eines Likelihood-Score\n",
    "Um die Intensität der thematischen Auseinandersetzung differenziert abzubilden, wird ein kontinuierlicher Likelihood-Score eingeführt. Dieser berücksichtigt die Anzahl und Häufigkeit der in der Vergleichsliste enthaltenen Begriffe und modelliert den thematischen Bezug als graduelles Merkmal. \n",
    "\n",
    "#### Technische Beschreibung des Likelihood-Scores\n",
    "Der Likelihood-Score ist ein gewichteter Relevanzwert, der angibt, wie stark ein Fanfiction-Werk auf eine vordefinierte Vergleichsliste von Begriffen (z. B. trans-spezifische Terme) reagiert. Der Score kombiniert Treffer in Metadaten-Tags und im Textkörper, um eine differenzierte Einschätzung zu ermöglichen.  \n",
    "\n",
    "**Identifikation der Treffer**\n",
    "Metadaten (freeform + character) und lemmatisierte Textkörper werden mit der Vergleichsliste abgeglichen. Die Vergleichsliste ist dazu absteigend nach Ausdruckslänge sortiert, sodass zusammenhängende Ausdrücke korrekt identifiziert werden können. Nach einem Treffer wird der entsprechende Textabschnitt maskiert, um Mehrfachzählungen derselben Textstelle und damit eine künstliche Erhöhung des Scores zu vermeiden. Die Trefferzahl wird pro Term erfasst, um Aussagen darüber treffen zu können, welche Begriffe wie häufig verwendet werden.  \n",
    "\n",
    "**Berechnung der Term-Scores**\n",
    "Bei der Berechnung des Scores soll berücksichtigt werden, dass viele unterschiedliche relevante Begriffen eine breitere, eine wiederholte Verwendung relevanter Begriffe eine tiefere Auseinandersetzung mit einer Thematik indizieren. Daher wird für jeden Bereich (freeform, character, text) ein Score gebildet, der zwei Komponenten kombiniert: Unique Hits gibt an, wie viele unterschiedliche Terme aus der Vergleichsliste im Text vorkommen, Frequency Sum enthält die standardmäßig logarithmisch skaliert Trefferanzahl pro Term.  \n",
    "\n",
    "**Gewichtung der Bereiche**\n",
    "Da Tags regelmäßig prägnanter und oft genauer in der thematischen Einordnung sind, während der Text länger, aber potenziell verrauschter ist, werden die Scores der Tags und des Textkörpers mit 80:20 gewichtet kombiniert.  \n",
    "\n",
    "#### Aufbau\n",
    "**Zelle 1:** Gewichtung *(einstellbar)*  \n",
    "**Zelle 2:** Berechnung des Likeliehood-Scores  \n",
    "**Zelle 3:** Ausgabe zentraler Kennzahlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "156854a9-1a06-4825-8d59-31c54878475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gewichtung EINSTELLBAR!\n",
    "WEIGHTS = {\"tags\": 0.8, \"text\": 0.2} \n",
    "USE_LOG_SCALING = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76c4f873-0abf-4a58-b38d-4bb17a1b1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Matches identifizieren und zählen mit Greedy-Matching gegen compare_list ---\n",
    "def count_term_hits(text, terms):\n",
    "    counter = Counter()\n",
    "\n",
    "    if not text:\n",
    "        return counter\n",
    "\n",
    "    # Alles klein, Unterstriche und Bindestriche normalisieren\n",
    "    text_lower = text.lower()\n",
    "    text_normalized = text_lower.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "\n",
    "    # Maske zur Vermeidung doppelter Treffer (Greedy)\n",
    "    mask = [\" \"] * len(text_normalized)\n",
    "\n",
    "    for term in terms:\n",
    "        term_lower = term.lower()\n",
    "        term_normalized = term_lower.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "        pattern = re.escape(term_normalized)\n",
    "\n",
    "        # Regex für ganze Wörter / Phrasen\n",
    "        regex = re.compile(rf'\\b{pattern}\\b')\n",
    "\n",
    "        for m in regex.finditer(text_normalized):\n",
    "            start, end = m.span()\n",
    "\n",
    "            # Prüfen, ob Bereich bereits maskiert wurde\n",
    "            if all(c == \" \" for c in mask[start:end]):\n",
    "                counter[term] += 1\n",
    "                # Bereich maskieren\n",
    "                mask[start:end] = [\"*\"] * (end - start)\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "# --- Score berechnen ---\n",
    "def score(row):\n",
    "    # --- Freeform Tags ---\n",
    "    freeform_text = \" \".join(row.get(\"freeform\", []))\n",
    "    freeform_hits = count_term_hits(freeform_text, compare_list)\n",
    "    freeform_unique = len(freeform_hits)\n",
    "    freeform_freq_sum = (\n",
    "        sum(math.log(1 + v) for v in freeform_hits.values()) if USE_LOG_SCALING else sum(freeform_hits.values())\n",
    "    )\n",
    "    freeform_score = freeform_unique + freeform_freq_sum\n",
    "\n",
    "    # --- Character Tags ---\n",
    "    char_text = \" \".join(row.get(\"character\", []))\n",
    "    char_hits = count_term_hits(char_text, compare_list)\n",
    "    char_unique = len(char_hits)\n",
    "    char_freq_sum = (\n",
    "        sum(math.log(1 + v) for v in char_hits.values()) if USE_LOG_SCALING else sum(char_hits.values())\n",
    "    )\n",
    "    char_score = char_unique + char_freq_sum\n",
    "\n",
    "    # --- Text ---\n",
    "    text_text = row.get(\"text_norm_lem\", \"\")\n",
    "    text_hits = count_term_hits(text_text, compare_list)\n",
    "    text_unique = len(text_hits)\n",
    "    text_freq_sum = (\n",
    "        sum(math.log(1 + v) for v in text_hits.values()) if USE_LOG_SCALING else sum(text_hits.values())\n",
    "    )\n",
    "    text_score = text_unique + text_freq_sum\n",
    "\n",
    "    # --- Gesamt-Score ---\n",
    "    total_score = round(\n",
    "        (WEIGHTS[\"tags\"] * (freeform_score + char_score) + WEIGHTS[\"text\"] * text_score),\n",
    "        2\n",
    "    )\n",
    "\n",
    "    return pd.Series({\n",
    "        \"likelihood\": total_score,\n",
    "        \"freeform_hits_counter\": freeform_hits,\n",
    "        \"character_hits_counter\": char_hits,\n",
    "        \"text_hits_counter\": text_hits,\n",
    "        \"freeform_score\": freeform_score,\n",
    "        \"character_score\": char_score,\n",
    "        \"text_score\": text_score\n",
    "    })\n",
    "\n",
    "\n",
    "# --- Score-Funktion auf DataFrame anwenden ---\n",
    "df[[\n",
    "    \"likelihood\",\n",
    "    \"freeform_hits_counter\",\n",
    "    \"character_hits_counter\",\n",
    "    \"text_hits_counter\",\n",
    "    \"freeform_score\",\n",
    "    \"character_score\",\n",
    "    \"text_score\"\n",
    "]] = df.apply(score, axis=1)\n",
    "\n",
    "\n",
    "# --- Trefferzahlen berechnen ---\n",
    "df[\"freeform_hits_count\"] = df[\"freeform_hits_counter\"].apply(lambda c: sum(c.values()))\n",
    "df[\"character_hits_count\"] = df[\"character_hits_counter\"].apply(lambda c: sum(c.values()))\n",
    "df[\"text_hits_count\"] = df[\"text_hits_counter\"].apply(lambda c: sum(c.values()))\n",
    "df[\"tag_hits_count\"] = df[\"freeform_hits_count\"] + df[\"character_hits_count\"]\n",
    "df[\"total_hits_count\"] = df[\"freeform_hits_count\"] + df[\"character_hits_count\"] + df[\"text_hits_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c18fea2a-7b4a-4918-b894-5ae977c9de19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-10 22:41:34] Likelihood-Score - Gewichtung und Berechnung\n",
      "------------------------------------------------------------\n",
      "Score- und Treffer-Berechnung abgeschlossen für\n",
      "Gewichtung: tags=0.8, text=0.2\n",
      "Log-Skalierung: True\n",
      "\n",
      "Anzahl der Werke mit mindestens einem Treffer: 98\n",
      "Summe aller Treffer: 200\n",
      "Durchschnittliche Treffer pro Werk: 2.04\n",
      "\n",
      "Likelihood-Score Verteilung:\n",
      "Höchster Score: 6.28\n",
      "Niedrigster Score: 0.0\n",
      "Durchschnitt (Mean): 0.07\n",
      "Median: 0.0\n",
      "Standardabweichung: 0.39\n",
      "\n",
      "Histogramm des Likelihood-Scores gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_likelihood_distribution.png\n",
      "\n",
      "Pfad zur CSV-Datei für die manuelle Prüfung: Analysedokumente\\Analysis_HP_FanFic_10_2025_likelihood_positives.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Grundverteilung beschreiben ---\n",
    "likelihood_desc = df[\"likelihood\"].describe()\n",
    "min_score = likelihood_desc[\"min\"]\n",
    "max_score = likelihood_desc[\"max\"]\n",
    "mean_score = round(likelihood_desc[\"mean\"], 2)\n",
    "median_score = likelihood_desc[\"50%\"]\n",
    "std_score = round(likelihood_desc[\"std\"], 2)\n",
    "\n",
    "# --- Grafische Darstellung der Grundverteilung ---\n",
    "hist_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_likelihood_distribution.png\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(df[\"likelihood\"], bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.xlabel(\"Likelihood-Score\")\n",
    "plt.ylabel(\"Anzahl Texte\")\n",
    "plt.title(f\"Verteilung des Likelihood-Scores für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(hist_file)\n",
    "plt.close()  \n",
    "\n",
    "# --- Ausgabe in CSV-Datei zur manuellen Prüfung ---\n",
    "df_export = df.copy()\n",
    "\n",
    "df_export[\"freeform_hits_terms\"] = df_export[\"freeform_hits_counter\"].apply(\n",
    "    lambda c: \", \".join(c.keys()) if c else \"\"\n",
    ")\n",
    "df_export[\"character_hits_terms\"] = df_export[\"character_hits_counter\"].apply(\n",
    "    lambda c: \", \".join(c.keys()) if c else \"\"\n",
    ")\n",
    "df_export[\"text_hits_terms\"] = df_export[\"text_hits_counter\"].apply(\n",
    "    lambda c: \", \".join(c.keys()) if c else \"\"\n",
    ")\n",
    "\n",
    "# Nur Zeilen mit mindestens einem Treffer\n",
    "df_export = df_export[df_export[\"total_hits_count\"] > 0]\n",
    "\n",
    "output_columns = [\n",
    "    \"work_id\",\n",
    "    \"likelihood\",\n",
    "    \"freeform_hits_count\",\n",
    "    \"freeform_hits_terms\",\n",
    "    \"character_hits_count\",\n",
    "    \"character_hits_terms\",\n",
    "    \"text_hits_count\",\n",
    "    \"text_hits_terms\",\n",
    "    \"total_hits_count\"\n",
    "]\n",
    "\n",
    "output_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_likelihood_positives.csv\"\n",
    ")\n",
    "\n",
    "df_export[output_columns].to_csv(\n",
    "    output_file,\n",
    "    index=False,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "num_ids = len(df_export)\n",
    "\n",
    "# --- Zusammenfassung für Analyse-Dokument ---\n",
    "analysis_lines = []\n",
    "analysis_lines.append(\"Score- und Treffer-Berechnung abgeschlossen für\")\n",
    "analysis_lines.append(f\"Gewichtung: tags={WEIGHTS['tags']}, text={WEIGHTS['text']}\")\n",
    "analysis_lines.append(f\"Log-Skalierung: {USE_LOG_SCALING}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "analysis_lines.append(f\"Anzahl der Werke mit mindestens einem Treffer: {num_ids}\")\n",
    "\n",
    "total_hits_sum = df_export[\"total_hits_count\"].sum()\n",
    "avg_hits_per_work = round(df_export[\"total_hits_count\"].mean(), 2) if num_ids > 0 else 0\n",
    "analysis_lines.append(f\"Summe aller Treffer: {total_hits_sum}\")\n",
    "analysis_lines.append(f\"Durchschnittliche Treffer pro Werk: {avg_hits_per_work}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "analysis_lines.append(\"Likelihood-Score Verteilung:\")\n",
    "analysis_lines.append(f\"Höchster Score: {max_score}\")\n",
    "analysis_lines.append(f\"Niedrigster Score: {min_score}\")\n",
    "analysis_lines.append(f\"Durchschnitt (Mean): {mean_score}\")\n",
    "analysis_lines.append(f\"Median: {median_score}\")\n",
    "analysis_lines.append(f\"Standardabweichung: {std_score}\")\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(f\"Histogramm des Likelihood-Scores gespeichert unter: {hist_file}\")\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(f\"Pfad zur CSV-Datei für die manuelle Prüfung: {output_file}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Zentral ausgeben + protokollieren ---\n",
    "write_analysis_block(\n",
    "    title=\"Likelihood-Score - Gewichtung und Berechnung\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d06c6-a2ae-4727-96fb-4541e15fd0cc",
   "metadata": {},
   "source": [
    "### 5. Allgemeine Statistische Auswertung\n",
    "Für die weitere Auswertung wird zunächst auf Basis der Grundverteilung des Likeliehood-Scores ein Schwellenwert gesetzt, der mit zusätzlichen Vorgaben hinsichtlich der Matches in den Tags und Textkörpern ergänzt werden kann.\n",
    "\n",
    "Die vor dem Hintergrund dieses Schwellenwertes als relevant klassifizierten Werke werden in diesem Abschnitt näher untersucht hinsichtlich\n",
    "- **Breite & Tiefe**: Wie viele Begriffe der Vergleichsliste werden verwendet (Breite) und wie häufig kommen einzelne Begriffe vor (Tiefe).\n",
    "- **Rating**: Von Autor:innen vergeben (Explicit, General Audiences, Mature, Not Rated, Teen and Up). Ziel war zu prüfen, ob transbezogene Inhalte mit sexualisierten Inhalten oder Altersbeschränkungen assoziiert sind, bzw. ob transbezogene Inhalte auch in Werken ohne Altersbeschränkung vorkommen.  \n",
    "- **Charakter**: Häufigkeit der Charaktere in relevanten Werken und Gesamt-Korpus, um die Relevanzrelation pro Figur zu bestimmen.  \n",
    "Die Auswertung liefert so quantitative Hinweise darauf, welche Begriffe, Tags, Charaktere und Alterskategorien im Zusammenhang mit einschlägigen Werken besonders häufig auftreten.\n",
    "\n",
    "#### Aufbau\n",
    "**Zelle 1:** Definition eines Schwellenwertes (Likeliehood-Score)  \n",
    "**Zelle 2:** Grundlegende Kennzahlen  \n",
    "**Zelle 3:** Breite und Tiefe der Matches  \n",
    "**Zelle 4:** Matches vs. Rating  \n",
    "**Zelle 5:** Matches vs. Charactere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9604e8fc-cf17-423a-b377-76ea4eb5049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition eines Schwellenwerts / Einstellbar!\n",
    "LIKELIHOOD_THRESHOLD = 0.15\n",
    "MIN_TAG_HITS = 0\n",
    "MIN_TEXT_HITS = 0\n",
    "\n",
    "df[\"is_relevant\"] = (\n",
    "    (df[\"likelihood\"] >= LIKELIHOOD_THRESHOLD) &\n",
    "    (df[\"tag_hits_count\"] >= MIN_TAG_HITS) &\n",
    "    (df[\"text_hits_count\"] >= MIN_TEXT_HITS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba86e0c0-6ab8-4584-88a6-faa8b6d9577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-10 22:41:34] Grundlegende Kennzahlen zu den einschlägigen Works\n",
      "------------------------------------------------------------\n",
      "Definierter Likelihood-Schwellenwert: 0.15\n",
      "Mindestanzahl Matches in Tags: 0\n",
      "Mindestanzahl Matches in Text: 0\n",
      "\n",
      "Anzahl aller Werke: 2111\n",
      "Anzahl einschlägiger Werke: 98\n",
      "Anteil einschlägiger Werke: 4.64 %\n",
      "\n",
      "Freeform-Tags:\n",
      "- Werke mit mindestens einem Freeform-Tag: 98 von 98 (100.0 %)\n",
      "- Gesamtanzahl Freeform-Tags in relevanten Werken: 1536\n",
      "- Anteil dieser Freeform-Tags am gesamten Korpus: 6.3 %\n",
      "\n",
      "Character-Tags:\n",
      "- Werke mit mindestens einem Character-Tag: 97 von 98 (99.0 %)\n",
      "- Gesamtanzahl Character-Tags in relevanten Werken: 385\n",
      "- Anteil dieser Character-Tags am gesamten Korpus: 5.4 %\n",
      "\n",
      "Textumfang der einschlägigen Werke:\n",
      "- Gesamtlänge der Texte einschlägiger Werke: 647,662 Tokens\n",
      "- Anteil am gesamten Textkorpus: 10.4 %\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grundlegende Kennzahlen\n",
    "analysis_lines = []\n",
    "\n",
    "analysis_lines.append(f\"Definierter Likelihood-Schwellenwert: {LIKELIHOOD_THRESHOLD}\")\n",
    "analysis_lines.append(f\"Mindestanzahl Matches in Tags: {MIN_TAG_HITS}\")\n",
    "analysis_lines.append(f\"Mindestanzahl Matches in Text: {MIN_TEXT_HITS}\")\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(f\"Anzahl aller Werke: {len(df)}\")\n",
    "analysis_lines.append(f\"Anzahl einschlägiger Werke: {df['is_relevant'].sum()}\")\n",
    "analysis_lines.append(f\"Anteil einschlägiger Werke: {round(df['is_relevant'].mean() * 100, 2)} %\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "df_relevant = df[df[\"is_relevant\"]].copy()\n",
    "\n",
    "# --- Freeform-Tags ---\n",
    "# Anzahl Werke mit mindestens einem Freeform-Tag\n",
    "relevant_freeform_works = (df_relevant[\"freeform\"].apply(len) > 0).sum()\n",
    "all_freeform_works = (df[\"freeform\"].apply(len) > 0).sum()\n",
    "\n",
    "# Gesamtanzahl Freeform-Tags\n",
    "total_freeform_relevant = df_relevant[\"freeform\"].apply(len).sum()\n",
    "total_freeform_all = df[\"freeform\"].apply(len).sum()\n",
    "\n",
    "analysis_lines.append(\"Freeform-Tags:\")\n",
    "analysis_lines.append(\n",
    "    f\"- Werke mit mindestens einem Freeform-Tag: \"\n",
    "    f\"{relevant_freeform_works} von {len(df_relevant)} \"\n",
    "    f\"({round(100*relevant_freeform_works/len(df_relevant),1)} %)\"\n",
    ")\n",
    "analysis_lines.append(\n",
    "    f\"- Gesamtanzahl Freeform-Tags in relevanten Werken: {total_freeform_relevant}\"\n",
    ")\n",
    "analysis_lines.append(\n",
    "    f\"- Anteil dieser Freeform-Tags am gesamten Korpus: \"\n",
    "    f\"{round(100*total_freeform_relevant/total_freeform_all,1)} %\"\n",
    ")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Character-Tags ---\n",
    "relevant_character_works = (df_relevant[\"character\"].apply(len) > 0).sum()\n",
    "all_character_works = (df[\"character\"].apply(len) > 0).sum()\n",
    "\n",
    "total_character_relevant = df_relevant[\"character\"].apply(len).sum()\n",
    "total_character_all = df[\"character\"].apply(len).sum()\n",
    "\n",
    "analysis_lines.append(\"Character-Tags:\")\n",
    "analysis_lines.append(\n",
    "    f\"- Werke mit mindestens einem Character-Tag: \"\n",
    "    f\"{relevant_character_works} von {len(df_relevant)} \"\n",
    "    f\"({round(100*relevant_character_works/len(df_relevant),1)} %)\"\n",
    ")\n",
    "analysis_lines.append(\n",
    "    f\"- Gesamtanzahl Character-Tags in relevanten Werken: {total_character_relevant}\"\n",
    ")\n",
    "analysis_lines.append(\n",
    "    f\"- Anteil dieser Character-Tags am gesamten Korpus: \"\n",
    "    f\"{round(100*total_character_relevant/total_character_all,1)} %\"\n",
    ")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Text-Statistiken\n",
    "df[\"text_length\"] = df[\"text_norm_lem\"].apply(lambda x: len(x.split()))\n",
    "df_relevant[\"text_length\"] = df_relevant[\"text_norm_lem\"].apply(\n",
    "    lambda x: len(x.split())\n",
    ")\n",
    "\n",
    "total_text_length_relevant = df_relevant[\"text_length\"].sum()\n",
    "total_text_length_all = df[\"text_length\"].sum()\n",
    "\n",
    "analysis_lines.append(\"Textumfang der einschlägigen Werke:\")\n",
    "analysis_lines.append(\n",
    "    f\"- Gesamtlänge der Texte einschlägiger Werke: \"\n",
    "    f\"{total_text_length_relevant:,} Tokens\"\n",
    ")\n",
    "analysis_lines.append(\n",
    "    f\"- Anteil am gesamten Textkorpus: \"\n",
    "    f\"{round(100 * total_text_length_relevant / total_text_length_all, 1)} %\"\n",
    ")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- In Analyse-Dokument schreiben ---\n",
    "write_analysis_block(\n",
    "    title=f\"Grundlegende Kennzahlen zu den einschlägigen Works\",\n",
    "    lines=analysis_lines\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1aa06d3a-3af3-4b56-a77d-002f46d750f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-10 23:07:50] Breite & Tiefe der Matches – Statistik (compare_list)\n",
      "------------------------------------------------------------\n",
      "Für Tags (Freeform + Character, compare_list):\n",
      "Gesamtanzahl Treffer: 101 in 59 Werken\n",
      "Anzahl verschiedener Begriffe: 11\n",
      "Top 10 Begriffe: [('trans', 72), ('transphobia', 5), ('pronouns', 5), ('ftm', 4), ('genderfluid', 4), ('non binary', 4), ('gender dysphoria', 3), ('gender euphoria', 1), ('terf', 1), ('genderqueer', 1)]\n",
      "\n",
      "Für Textkörper (compare_list):\n",
      "Gesamtanzahl Treffer: 99 in 55 Werken\n",
      "Anzahl verschiedener Begriffe: 24\n",
      "Top 10 Begriffe: [('transition', 32), ('binder', 13), ('transgender', 12), ('trans', 11), ('sexual orientation', 3), ('transitioning', 3), ('non binary', 3), ('deadname', 2), ('intersex', 2), ('gender expression', 2)]\n",
      "\n",
      "Co-Occurrence Top Begriffe (compare_list):\n",
      "                     trans  transition  binder  transgender  non binary  transphobia  pronouns  ftm  genderfluid  gender dysphoria  sexual orientation  transitioning  genderqueer  deadname  intersex  gender expression  gender identity  transsexual  gender reassignment  passing woman\n",
      "trans                   83           2       8           10           3            3         1    4            1                 2                   0              0            1         2         0                  1                2            2                    0              0\n",
      "transition               2          32       0            0           0            0         0    0            0                 0                   0              1            0         1         0                  0                0            0                    0              0\n",
      "binder                   8           0      13            6           1            2         0    1            0                 0                   0              0            0         0         0                  0                0            0                    0              0\n",
      "transgender             10           0       6           12           2            1         0    2            0                 0                   0              0            0         0         0                  1                1            0                    0              0\n",
      "non binary               3           0       1            2           7            1         1    0            0                 0                   0              0            0         0         0                  1                1            0                    0              0\n",
      "transphobia              3           0       2            1           1            5         0    0            1                 1                   0              0            0         0         0                  0                0            0                    0              0\n",
      "pronouns                 1           0       0            0           1            0         5    0            1                 0                   0              0            0         0         0                  0                0            0                    0              0\n",
      "ftm                      4           0       1            2           0            0         0    4            0                 0                   0              0            0         0         0                  0                0            0                    0              0\n",
      "genderfluid              1           0       0            0           0            1         1    0            4                 2                   0              0            0         0         0                  0                1            0                    0              0\n",
      "gender dysphoria         2           0       0            0           0            1         0    0            2                 3                   0              0            0         0         0                  0                1            0                    0              0\n",
      "sexual orientation       0           0       0            0           0            0         0    0            0                 0                   3              0            0         0         0                  0                0            0                    0              0\n",
      "transitioning            0           1       0            0           0            0         0    0            0                 0                   0              3            0         0         0                  0                0            0                    0              0\n",
      "genderqueer              1           0       0            0           0            0         0    0            0                 0                   0              0            2         0         0                  0                0            0                    0              0\n",
      "deadname                 2           1       0            0           0            0         0    0            0                 0                   0              0            0         2         0                  0                0            0                    0              0\n",
      "intersex                 0           0       0            0           0            0         0    0            0                 0                   0              0            0         0         2                  0                0            0                    0              0\n",
      "gender expression        1           0       0            1           1            0         0    0            0                 0                   0              0            0         0         0                  2                1            0                    0              0\n",
      "gender identity          2           0       0            1           1            0         0    0            1                 1                   0              0            0         0         0                  1                2            0                    0              0\n",
      "transsexual              2           0       0            0           0            0         0    0            0                 0                   0              0            0         0         0                  0                0            2                    0              0\n",
      "gender reassignment      0           0       0            0           0            0         0    0            0                 0                   0              0            0         0         0                  0                0            0                    2              0\n",
      "passing woman            0           0       0            0           0            0         0    0            0                 0                   0              0            0         0         0                  0                0            0                    0              1\n",
      "\n",
      "Grafik zur Co-Occurrence gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_cooccurrence_compare.png\n",
      "Grafik Wordcloud gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_wordcloud_compare.png\n",
      "Grafik zu der Art der Matches pro Werk gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_hits_tags_vs_text_compare.png\n",
      "Grafik zur Verteilung der Matches nach Werken gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_hits_per_work_compare.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Breite & Tiefe der Matches – Statistik (compare_list)\n",
    "analysis_lines = []\n",
    "\n",
    "# --- Einschlägige Werke filtern ---\n",
    "df_relevant = df[df[\"is_relevant\"]].copy()\n",
    "\n",
    "# --- Hilfsfunktionen ---\n",
    "from collections import Counter\n",
    "\n",
    "def aggregate_counters(series):\n",
    "    \"\"\"Alle Counter in einer Serie zusammenführen\"\"\"\n",
    "    total = Counter()\n",
    "    for c in series:\n",
    "        total.update(c)\n",
    "    return total\n",
    "\n",
    "def works_with_hits(series_of_counters):\n",
    "    \"\"\"Zählt, in wie vielen Dokumenten mindestens ein Treffer vorhanden ist\"\"\"\n",
    "    return sum(1 for c in series_of_counters if sum(c.values()) > 0)\n",
    "\n",
    "# --- Counter pro Werk auf Basis compare_list ---\n",
    "def get_compare_hits(row):\n",
    "    \"\"\"\n",
    "    Zählt Treffer in freeform, character und text nur für compare_list.\n",
    "    Verwendet dieselbe Greedy-Matching-Logik wie der Likelihood-Score\n",
    "    (count_term_hits), um konsistente Ergebnisse sicherzustellen.\n",
    "    \"\"\"\n",
    "    combined_counter = Counter()\n",
    "\n",
    "    # --- Freeform ---\n",
    "    freeform_tags = row.get(\"freeform\", [])\n",
    "    freeform_text = \" \".join(freeform_tags)\n",
    "    freeform_hits = count_term_hits(freeform_text, compare_list)\n",
    "    combined_counter.update(freeform_hits)\n",
    "\n",
    "    # --- Character ---\n",
    "    char_tags = row.get(\"character\", [])\n",
    "    char_text = \" \".join(char_tags)\n",
    "    char_hits = count_term_hits(char_text, compare_list)\n",
    "    combined_counter.update(char_hits)\n",
    "\n",
    "    # --- Text ---\n",
    "    text_content = row.get(\"text_norm_lem\", \"\")\n",
    "    text_hits = count_term_hits(text_content, compare_list)\n",
    "    combined_counter.update(text_hits)\n",
    "\n",
    "    return freeform_hits, char_hits, text_hits, combined_counter\n",
    "\n",
    "# --- Counters erzeugen ---\n",
    "df_relevant[[\n",
    "    \"freeform_hits_counter\",\n",
    "    \"character_hits_counter\",\n",
    "    \"text_hits_counter\",\n",
    "    \"combined_counter\"\n",
    "]] = df_relevant.apply(\n",
    "    lambda row: pd.Series(get_compare_hits(row)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- Aggregierte Counter ---\n",
    "counter_tags = aggregate_counters(\n",
    "    df_relevant[\"freeform_hits_counter\"] +\n",
    "    df_relevant[\"character_hits_counter\"]\n",
    ")\n",
    "counter_text = aggregate_counters(df_relevant[\"text_hits_counter\"])\n",
    "counter_combined = aggregate_counters(df_relevant[\"combined_counter\"])\n",
    "\n",
    "# --- Statistikfunktion ---\n",
    "def term_stats(counter, series_of_counters, name):\n",
    "    total_terms = sum(counter.values())\n",
    "    unique_terms = len(counter)\n",
    "    works_count = works_with_hits(series_of_counters)\n",
    "    top_terms = counter.most_common(10)\n",
    "\n",
    "    analysis_lines.append(f\"Für {name}:\")\n",
    "    analysis_lines.append(f\"Gesamtanzahl Treffer: {total_terms} in {works_count} Werken\")\n",
    "    analysis_lines.append(f\"Anzahl verschiedener Begriffe: {unique_terms}\")\n",
    "    analysis_lines.append(f\"Top 10 Begriffe: {top_terms}\")\n",
    "    analysis_lines.append(\"\")\n",
    "\n",
    "# --- Auswertungen ---\n",
    "term_stats(\n",
    "    counter_tags,\n",
    "    df_relevant[\"freeform_hits_counter\"] +\n",
    "    df_relevant[\"character_hits_counter\"],\n",
    "    \"Tags (Freeform + Character, compare_list)\"\n",
    ")\n",
    "term_stats(\n",
    "    counter_text,\n",
    "    df_relevant[\"text_hits_counter\"],\n",
    "    \"Textkörper (compare_list)\"\n",
    ")\n",
    "\n",
    "# --- Co-Occurrence Matrix (Top 20 kombinierte Begriffe) ---\n",
    "top_n = 20\n",
    "top_terms_list = [t for t, _ in counter_combined.most_common(top_n)]\n",
    "\n",
    "co_matrix = pd.DataFrame(\n",
    "    0,\n",
    "    index=top_terms_list,\n",
    "    columns=top_terms_list\n",
    ")\n",
    "\n",
    "for counter in df_relevant[\"combined_counter\"]:\n",
    "    terms_in_doc = {t: counter[t] for t in counter if t in top_terms_list}\n",
    "    for t1, count1 in terms_in_doc.items():\n",
    "        co_matrix.loc[t1, t1] += count1\n",
    "        for t2, count2 in terms_in_doc.items():\n",
    "            if t1 != t2:\n",
    "                co_matrix.loc[t1, t2] += min(count1, count2)\n",
    "\n",
    "analysis_lines.append(\"Co-Occurrence Top Begriffe (compare_list):\")\n",
    "analysis_lines.append(co_matrix.to_string())\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Heatmap ---\n",
    "co_matrix_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_cooccurrence_compare.png\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(co_matrix, cmap=\"viridis\", annot=True, fmt=\"d\")\n",
    "plt.title(f\"Co-Occurrence Top 20 Begriffe (compare_list) für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(co_matrix_file)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(f\"Grafik zur Co-Occurrence gespeichert unter: {co_matrix_file}\")\n",
    "\n",
    "# --- Wordcloud ---\n",
    "wordcloud_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_wordcloud_compare.png\"\n",
    ")\n",
    "\n",
    "wc = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color=\"white\"\n",
    ").generate_from_frequencies(counter_combined)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Wordcloud für compare_list – {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(wordcloud_file)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(f\"Grafik Wordcloud gespeichert unter: {wordcloud_file}\")\n",
    "\n",
    "# --- Histogramm: Tags vs. Text pro Werk ---\n",
    "hist_file_tags_text = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_hits_tags_vs_text_compare.png\"\n",
    ")\n",
    "\n",
    "# Berechnung der Treffer pro Werk\n",
    "tags_counts = (\n",
    "    df_relevant[\"freeform_hits_counter\"].apply(lambda c: sum(c.values())) +\n",
    "    df_relevant[\"character_hits_counter\"].apply(lambda c: sum(c.values()))\n",
    ")\n",
    "text_counts = df_relevant[\"text_hits_counter\"].apply(lambda c: sum(c.values()))\n",
    "work_labels = df_relevant[\"work_id\"].astype(str)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(work_labels, tags_counts, label=\"Tags (Freeform + Character)\")\n",
    "plt.bar(work_labels, text_counts, bottom=tags_counts, label=\"Text\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Werk-ID\")\n",
    "plt.ylabel(\"Anzahl Treffer\")\n",
    "plt.title(f\"Art der Matches pro Werk (compare_list) für {OUTPUT_PREFIX}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(hist_file_tags_text)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(\n",
    "    f\"Grafik zu der Art der Matches pro Werk gespeichert unter: {hist_file_tags_text}\"\n",
    ")\n",
    "\n",
    "# --- Histogramm: Gesamt-Treffer pro Werk ---\n",
    "hist_file_total = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_hits_per_work_compare.png\"\n",
    ")\n",
    "\n",
    "total_hits_per_work = df_relevant[\"combined_counter\"].apply(lambda c: sum(c.values()))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(total_hits_per_work, bins=30, edgecolor=\"black\")\n",
    "plt.xlabel(\"Gesamtanzahl Treffer pro Werk\")\n",
    "plt.ylabel(\"Anzahl Werke\")\n",
    "plt.title(\"Verteilung der Treffer pro Werk (compare_list)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(hist_file_total)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(\n",
    "    f\"Grafik zur Verteilung der Matches nach Werken gespeichert unter: {hist_file_total}\"\n",
    ")\n",
    "\n",
    "# --- Analyseblock schreiben ---\n",
    "write_analysis_block(\n",
    "    title=\"Breite & Tiefe der Matches – Statistik (compare_list)\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9590fdc6-27e5-4e75-9fff-575ad16800bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_5488\\2268886027.py:37: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=rating_counts.index, y=rating_counts.values, palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-10 22:41:52] Matches nach Rating\n",
      "------------------------------------------------------------\n",
      "Matches nach Rating\n",
      "- Analyse, in welchem Rating die Werke zum Themenschwerpunkt enthalten sind\n",
      "\n",
      "Anzahl einschlägiger Werke pro Rating:\n",
      "explicit: 50 Werke (51.02%)\n",
      "teen and up audiences: 21 Werke (21.43%)\n",
      "mature: 13 Werke (13.27%)\n",
      "general audiences: 12 Werke (12.24%)\n",
      "not rated: 2 Werke (2.04%)\n",
      "\n",
      "Relativer Anteil im Vergleich zur Gesamtstichprobe (normiert):\n",
      "explicit: Gesamt 38.7% | Einschlägig 51.02%\n",
      "general audiences: Gesamt 24.63% | Einschlägig 12.24%\n",
      "mature: Gesamt 10.75% | Einschlägig 13.27%\n",
      "not rated: Gesamt 3.84% | Einschlägig 2.04%\n",
      "teen and up audiences: Gesamt 22.07% | Einschlägig 21.43%\n",
      "\n",
      "Einschlägige Werke pro Rating gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_rating_counts.png\n",
      "Rating-Vergleich Gesamt vs. Einschlägig gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_rating_comparison.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matches vs Rating\n",
    "analysis_lines = []\n",
    "analysis_lines.append(\"Matches nach Rating\")\n",
    "analysis_lines.append(\"- Analyse, in welchem Rating die Werke zum Themenschwerpunkt enthalten sind\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# - Anzahl und Anteile einschlägiger Werke\n",
    "rating_counts = df_relevant[\"rating\"].value_counts()\n",
    "rating_percent = (rating_counts / len(df_relevant) * 100).round(2)\n",
    "\n",
    "analysis_lines.append(\"Anzahl einschlägiger Werke pro Rating:\")\n",
    "for r, count in rating_counts.items():\n",
    "    analysis_lines.append(f\"{r}: {count} Werke ({rating_percent[r]}%)\")\n",
    "\n",
    "# - Vergleich: Gesamt vs. einschlägig\n",
    "rating_compare = pd.DataFrame({\n",
    "    \"gesamt\": df[\"rating\"].value_counts(normalize=True),\n",
    "    \"einschlägig\": df_relevant[\"rating\"].value_counts(normalize=True)\n",
    "}).fillna(0)\n",
    "\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(\"Relativer Anteil im Vergleich zur Gesamtstichprobe (normiert):\")\n",
    "for r in rating_compare.index:\n",
    "    gesamt_pct = round(rating_compare.loc[r, \"gesamt\"]*100, 2)\n",
    "    relevant_pct = round(rating_compare.loc[r, \"einschlägig\"]*100, 2)\n",
    "    analysis_lines.append(f\"{r}: Gesamt {gesamt_pct}% | Einschlägig {relevant_pct}%\")\n",
    "\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Grafiken erstellen\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# - Absolute Anzahl einschlägiger Werke pro Rating\n",
    "fig1_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_rating_counts.png\")\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=rating_counts.index, y=rating_counts.values, palette=\"viridis\")\n",
    "plt.ylabel(\"Anzahl Werke\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.title(f\"Einschlägige Werke pro Rating für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig1_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Einschlägige Werke pro Rating gespeichert unter: {fig1_file}\")\n",
    "\n",
    "# - Relativer Vergleich: Gesamt vs. einschlägig\n",
    "fig2_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_rating_comparison.png\")\n",
    "rating_compare_plot = rating_compare.sort_index()  # optional nach Rating sortieren\n",
    "rating_compare_plot.plot(kind=\"bar\", figsize=(8,5))\n",
    "plt.ylabel(\"Anteil (normiert)\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.title(f\"Rating-Vergleich Gesamt vs. Einschlägig für {OUTPUT_PREFIX}\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig2_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Rating-Vergleich Gesamt vs. Einschlägig gespeichert unter: {fig2_file}\")\n",
    "\n",
    "# Ausgabeblock schreiben\n",
    "write_analysis_block(\n",
    "    title=\"Matches nach Rating\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "153377f3-04b4-4606-99a4-225587272d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_5488\\1609719855.py:97: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"count\", y=\"character\", data=top_characters, palette=\"viridis\")\n",
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_5488\\1609719855.py:108: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"fraction_relevant\", y=\"character\", data=comparison_df.head(15), palette=\"coolwarm\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-10 22:41:53] Matches vs Character – kombiniert compare_list + character_list\n",
      "------------------------------------------------------------\n",
      "Figuren im Themenzusammenhang- Analyse der Figuren in einschlägigen Werken\n",
      "\n",
      "\n",
      "Figuren mit explizitem Bezug\n",
      "Top 15 Figuren mit explizitem Bezug (inkl. Work IDs):\n",
      "1. trans_regulus_black (30 Werke)\n",
      "Work IDs: 60181408, 71115726, 71390066, 71427291, 71558041, 71638646, 71654156, 71677146, 71681946, 71749946, 71813036, 71837436, 71837786, 71945241, 71961046, 72023176, 72078311, 72092711, 72108571, 72287316, 72417596, 72500056, 72591531, 72629041, 72636416, 73011126, 73128221, 73189381, 73447521, 73454846\n",
      "2. trans_sirius_black (4 Werke)\n",
      "Work IDs: 71754236, 71908236, 72624701, 73051026\n",
      "3. trans_percy_weasley (3 Werke)\n",
      "Work IDs: 72023136, 73035356, 73404586\n",
      "4. ftm_percy_weasley (3 Werke)\n",
      "Work IDs: 72023136, 73035356, 73404586\n",
      "5. genderfluid_sirius_black (3 Werke)\n",
      "Work IDs: 72455821, 72528926, 73168956\n",
      "6. trans_remus_lupin (3 Werke)\n",
      "Work IDs: 71837436, 71908236, 72664191\n",
      "7. sirius_black_uses_he/him_and_she/her_and_they/them_pronouns (2 Werke)\n",
      "Work IDs: 72455821, 73051026\n",
      "8. trans_male_remus_lupin (2 Werke)\n",
      "Work IDs: 71908236, 72664191\n",
      "9. narcissa_black_malfoy (2 Werke)\n",
      "Work IDs: 72071991, 72625641\n",
      "10. trans_male_sirius_black (2 Werke)\n",
      "Work IDs: 71908236, 72624701\n",
      "11. trans_male_harry_potter (2 Werke)\n",
      "Work IDs: 72535051, 72591676\n",
      "12. trans_harry_potter (2 Werke)\n",
      "Work IDs: 72535051, 72591676\n",
      "13. trans_ron_weasley (1 Werke)\n",
      "Work IDs: 73398356\n",
      "14. molly_is_transphobic (1 Werke)\n",
      "Work IDs: 73398356\n",
      "15. changing_pronouns_for_harry_potter (1 Werke)\n",
      "Work IDs: 73032696\n",
      "\n",
      "Figuren mit implizitem Bezug (Vorkommen einschlägigen Texten)\n",
      "Top 15 Figuren in einschlägigen Werken:\n",
      "1. james_potter: 39 Werke\n",
      "2. regulus_black: 37 Werke\n",
      "3. harry_potter: 30 Werke\n",
      "4. sirius_black: 29 Werke\n",
      "5. remus_lupin: 26 Werke\n",
      "6. hermione_granger: 22 Werke\n",
      "7. draco_malfoy: 20 Werke\n",
      "8. ron_weasley: 15 Werke\n",
      "9. severus_snape: 12 Werke\n",
      "10. lily_evans_potter: 9 Werke\n",
      "11. ginny_weasley: 8 Werke\n",
      "12. peter_pettigrew: 6 Werke\n",
      "13. tom_riddle_|_voldemort: 5 Werke\n",
      "14. theodore_nott: 5 Werke\n",
      "15. percy_weasley: 4 Werke\n",
      "\n",
      "Relativer Anteil einschlägige vs. alle Werke (mind. 5 Werke insgesamt):\n",
      "74. parvati_patil: 2 / 5 Werke (28.599999999999998 % in relevanten Werken)\n",
      "34. kingsley_shacklebolt: 3 / 9 Werke (25.0 % in relevanten Werken)\n",
      "17. justin_finch_fletchley: 1 / 4 Werke (20.0 % in relevanten Werken)\n",
      "63. original_character: 1 / 4 Werke (20.0 % in relevanten Werken)\n",
      "65. hannah_abbott: 2 / 8 Werke (20.0 % in relevanten Werken)\n",
      "10. oliver_wood: 3 / 12 Werke (20.0 % in relevanten Werken)\n",
      "61. regulus_black: 37 / 166 Werke (18.2 % in relevanten Werken)\n",
      "47. queenie_goldstein: 1 / 5 Werke (16.7 % in relevanten Werken)\n",
      "35. jacob_kowalski: 1 / 5 Werke (16.7 % in relevanten Werken)\n",
      "33. tina_goldstein: 1 / 5 Werke (16.7 % in relevanten Werken)\n",
      "50. antonin_dolohov: 1 / 5 Werke (16.7 % in relevanten Werken)\n",
      "15. orion_black: 1 / 5 Werke (16.7 % in relevanten Werken)\n",
      "90. walburga_black: 2 / 10 Werke (16.7 % in relevanten Werken)\n",
      "40. minor_characters: 2 / 11 Werke (15.4 % in relevanten Werken)\n",
      "24. percy_weasley: 4 / 22 Werke (15.4 % in relevanten Werken)\n",
      "\n",
      "Grafik der Top Figuren gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_topcharacters.png\n",
      "Grafik zum relativen Vorkommen (relevant vs. nicht relevant) gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_relativecharacterrelevance.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matches vs Character (verschärft: Vergleichsliste + Character-Liste)\n",
    "\n",
    "analysis_lines = []\n",
    "analysis_lines.append(\"Figuren im Themenzusammenhang- Analyse der Figuren in einschlägigen Werken\")\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# df_relevant und df_nonrelevant\n",
    "df_relevant = df[df[\"is_relevant\"]].copy()\n",
    "df_nonrelevant = df[~df[\"is_relevant\"]].copy()\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Figuren mit explizitem Bezug: nur Tags, die sowohl compare_list als auch character_list matchen\n",
    "analysis_lines.append(\"Figuren mit explizitem Bezug\")\n",
    "\n",
    "combined_tags_counter = Counter()\n",
    "combined_tags_work_ids = defaultdict(set) \n",
    "\n",
    "for idx, row in df_relevant.iterrows():\n",
    "    work_id = row[\"work_id\"]\n",
    "    all_tags = row.get(\"freeform\", []) + row.get(\"character\", [])\n",
    "\n",
    "    for tag in all_tags:\n",
    "        # Normalisierung: Unterstriche / Leerzeichen\n",
    "        tag_norm = tag.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "        \n",
    "        # Prüfen: mindestens ein Begriff aus compare_list UND ein Begriff aus character_list enthalten\n",
    "        if any(term in tag_norm for term in compare_list) and any(char in tag_norm for char in character_list):\n",
    "            combined_tags_counter[tag] += 1\n",
    "            combined_tags_work_ids[tag].add(work_id)\n",
    "\n",
    "# Top 15 kombinierte Tags\n",
    "top_combined_tags = pd.DataFrame(\n",
    "    [\n",
    "        (tag, combined_tags_counter[tag], sorted(combined_tags_work_ids[tag]))\n",
    "        for tag, _ in combined_tags_counter.most_common(15)\n",
    "    ],\n",
    "    columns=[\"tag\", \"count\", \"work_ids\"]\n",
    ")\n",
    "\n",
    "# Ausgabe\n",
    "analysis_lines.append(\"Top 15 Figuren mit explizitem Bezug (inkl. Work IDs):\")\n",
    "for idx, row in top_combined_tags.iterrows():\n",
    "    work_ids_str = \", \".join(str(wid) for wid in row[\"work_ids\"])\n",
    "    analysis_lines.append(f\"{idx+1}. {row['tag']} ({row['count']} Werke)\")\n",
    "    analysis_lines.append(f\"Work IDs: {work_ids_str}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Implizites Vorkommen\n",
    "analysis_lines.append(\"Figuren mit implizitem Bezug (Vorkommen einschlägigen Texten)\")\n",
    "char_counter = Counter()\n",
    "for chars in df_relevant[\"character\"]:\n",
    "    char_counter.update(chars)\n",
    "\n",
    "top_characters = pd.DataFrame(\n",
    "    char_counter.most_common(15),\n",
    "    columns=[\"character\", \"count\"]\n",
    ")\n",
    "\n",
    "analysis_lines.append(\"Top 15 Figuren in einschlägigen Werken:\")\n",
    "for idx, row in top_characters.iterrows():\n",
    "    analysis_lines.append(f\"{idx+1}. {row['character']}: {row['count']} Werke\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# Relatives Vorkommen einschlägig vs. alle Werke\n",
    "char_relevant = Counter()\n",
    "char_nonrelevant = Counter()\n",
    "\n",
    "for chars in df_relevant[\"character\"]:\n",
    "    char_relevant.update(chars)\n",
    "for chars in df_nonrelevant[\"character\"]:\n",
    "    char_nonrelevant.update(chars)\n",
    "\n",
    "comparison = []\n",
    "for char in set(list(char_relevant.keys()) + list(char_nonrelevant.keys())):\n",
    "    count_rel = char_relevant.get(char, 0)\n",
    "    count_nonrel = char_nonrelevant.get(char, 0)\n",
    "    if count_rel + count_nonrel >= 5:\n",
    "        rel_fraction = count_rel / (count_rel + count_nonrel)\n",
    "        comparison.append((char, count_rel, count_nonrel, round(rel_fraction, 3)))\n",
    "\n",
    "comparison_df = pd.DataFrame(\n",
    "    comparison, columns=[\"character\", \"relevant_count\", \"nonrelevant_count\", \"fraction_relevant\"]\n",
    ").sort_values(\"fraction_relevant\", ascending=False)\n",
    "\n",
    "analysis_lines.append(\"Relativer Anteil einschlägige vs. alle Werke (mind. 5 Werke insgesamt):\")\n",
    "for idx, row in comparison_df.head(15).iterrows():\n",
    "    analysis_lines.append(\n",
    "        f\"{idx+1}. {row['character']}: {row['relevant_count']} / {row['nonrelevant_count']} Werke \"\n",
    "        f\"({row['fraction_relevant']*100} % in relevanten Werken)\"\n",
    "    )\n",
    "\n",
    "# Grafiken\n",
    "fig_chars_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_topcharacters.png\")\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"count\", y=\"character\", data=top_characters, palette=\"viridis\")\n",
    "plt.xlabel(\"Anzahl einschlägiger Werke\")\n",
    "plt.ylabel(\"Figur\")\n",
    "plt.title(f\"Top 15 Figuren in einschlägigen Werken für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_chars_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"\\nGrafik der Top Figuren gespeichert unter: {fig_chars_file}\")\n",
    "\n",
    "fig_chars_rel_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_relativecharacterrelevance.png\")\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"fraction_relevant\", y=\"character\", data=comparison_df.head(15), palette=\"coolwarm\")\n",
    "plt.xlabel(\"Anteil in relevanten Werken\")\n",
    "plt.ylabel(\"Figur\")\n",
    "plt.title(f\"Figuren-Anteil in einschlägigen vs. allen Texten für {OUTPUT_PREFIX}\")\n",
    "plt.xlim(0,1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_chars_rel_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Grafik zum relativen Vorkommen (relevant vs. nicht relevant) gespeichert unter: {fig_chars_rel_file}\")\n",
    "\n",
    "# Ausgabeblock schreiben\n",
    "write_analysis_block(\n",
    "    title=\"Matches vs Character – kombiniert compare_list + character_list\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f00e6-beab-407c-abde-e19b646faeaa",
   "metadata": {},
   "source": [
    "### 6. Topic Modeling zur Einbettung des Themas im Gesamtkontext\n",
    "\n",
    "Im Rahmen dieses Projekts werden die Topic-Modeling-Analysen getrennt für **narrative Volltexte** und **paratextuelle Tags** durchgeführt, um die Haupt-Topics zu identifizieren. Zudem wird unterschieden zwischen: **allen Werken** und **einschlägigen Werken** (positiver Trans-Bezug),  \n",
    "um zu prüfen, ob Trans-Themen tatsächlich dominieren und welche Topics damit verknüpft sind. Die vollständigen Topic-Zuordnungen werden im Ordner *Ausgabedokumente* gespeichert.\n",
    "\n",
    "**Zelle 1:** Topic Modeling für alle Werke (Text)  \n",
    "**Zelle 2:** Topic Modeling für einschlägige Werke (Text)  \n",
    "**Zelle 3:** Topic Modeling für alle Werke (Tags)  \n",
    "**Zelle 4:** Topic Modeling für einschlägige Werke (Tags)  \n",
    "\n",
    "#### Vorverarbeitung\n",
    "**Texte:**\n",
    "- Normalisierung und linguistische Vorverarbeitung (Tokenisierung, Lemmatisierung)  \n",
    "- Entfernung englischer Stoppwörter und korpusspezifischer Stoppwörter (z. B. Figuren, Sprechverben)  \n",
    "- Bag-of-Words-Vektorisierung (`CountVectorizer`)  \n",
    "- Ausschluss sehr seltener (`min_df=2`) und extrem häufiger Wörter (`max_df=0.5`)  \n",
    "\n",
    "**Tags:**\n",
    "- Normalisierung (Kleinschreibung, Leerzeichen und Bindestriche → Unterstriche)  \n",
    "- Keine Lemmatisierung (Tags sind semantisch stabil)  \n",
    "- Bag-of-Words-Vektorisierung mit tokenisierten Tags  \n",
    "- Alle Tags berücksichtigt (`min_df=1`)  \n",
    "\n",
    "#### Topic Modeling\n",
    "- Methode: **Latent Dirichlet Allocation (LDA)**  \n",
    "- Jedes Dokument wird als Wahrscheinlichkeitsverteilung über Topics modelliert, jedes Topic als Verteilung über Wörter/Tags  \n",
    "- Anzahl Topics: `n=3` für Text und Tags (vergleichbar)  \n",
    "- Batch-Lernverfahren, `random_state=42` (Reproduzierbarkeit)  \n",
    "- Zusätzlich: **Topic-Dominanz**  \n",
    "  - Ein Topic gilt als dominant, wenn sein Anteil > 0.5  \n",
    "  - Ermöglicht die Unterscheidung zwischen klar fokussierten und heterogenen Werken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e9b39ae-8308-483b-ac8a-d6f7573f91d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_5488\\1590962917.py:96: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=topic_means.values, y=topic_means.index, palette=\"viridis\")\n",
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_5488\\1590962917.py:110: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-10 22:42:34] Topic Modeling über alle Werke (Texte, inkl. optionaler Charakter-Stopwords)\n",
      "------------------------------------------------------------\n",
      "Stopword-Liste für Topic Modeling verwendet: 685 Begriffe\n",
      "(Charkter-Namen berücksichtigt: True\n",
      "Vokabulargröße nach Stopword-Filterung: 50185\n",
      "Topic Modeling – Top Wörter pro Topic:\n",
      "\n",
      "Topic 1: love, friend, man, got, life, need, smile, wanted, people, took, turned, wand, mean, bit, help\n",
      "\n",
      "Topic 2: barty, evan, marlene, pandora, dorcas, reg, barty evan, reggie, cigarette, evan barty, rosier, pete, princess, sybil, party\n",
      "\n",
      "Topic 3: dan, cormac, torus, duo, men, fan, ran, fang, chang, ban, ding, rang, man, que, hen\n",
      "\n",
      "Topic 4: oliver, pumpkin, candy, ginevra, cake, penelope, halloween, broom, puddlemere, perce, patch, pumpkin patch, effie, gourd, walburga\n",
      "\n",
      "Topic 5: lyall, raised single, did live, started running, cock, body, breath, need, wand, hard, hip, sound, fucking, light, fuck\n",
      "\n",
      "Topic 6: version, unmasked, love, sound, wanted, breath, light, body, wand, evan, shoulder, took, turned, life, barty\n",
      "\n",
      "Topic 7: cock, body, fuck, hip, kiss, inside, breath, tongue, hard, thigh, fucking, neck, need, shoulder, throat\n",
      "\n",
      "Topic 8: air, low, breath, sharp, quiet, word, light, sound, soft, silence, gaze, dark, leaned, beneath, cold\n",
      "\n",
      "Topic 9: terence, adrian, mile, graham, taneen, benjy, cassius, haggard, nott, lucy, higgs, egg, jason, masquerade, blind date\n",
      "\n",
      "Topic 10: tentacle, sucker, octopus, pirate, slime, elizabeth, couple costume, exploited, sweet tender, quite smile, deviant, happen happened, experienced life, took, need\n",
      "\n",
      "Anzahl der Werke, in denen die Topics stark dominierend sind (>=0.5):\n",
      "Topic 1: 1019 Werke (48.3%)\n",
      "Topic 2: 0 Werke (0.0%)\n",
      "Topic 3: 1 Werke (0.0%)\n",
      "Topic 4: 0 Werke (0.0%)\n",
      "Topic 5: 0 Werke (0.0%)\n",
      "Topic 6: 1 Werke (0.0%)\n",
      "Topic 7: 740 Werke (35.1%)\n",
      "Topic 8: 156 Werke (7.4%)\n",
      "Topic 9: 0 Werke (0.0%)\n",
      "Topic 10: 0 Werke (0.0%)\n",
      "\n",
      "Anzahl der Werke, in denen die Topics präsent sind (>=0.3):\n",
      "Topic 1: 1360 Werke (64.4%)\n",
      "Topic 2: 5 Werke (0.2%)\n",
      "Topic 3: 1 Werke (0.0%)\n",
      "Topic 4: 0 Werke (0.0%)\n",
      "Topic 5: 0 Werke (0.0%)\n",
      "Topic 6: 1 Werke (0.0%)\n",
      "Topic 7: 950 Werke (45.0%)\n",
      "Topic 8: 285 Werke (13.5%)\n",
      "Topic 9: 0 Werke (0.0%)\n",
      "Topic 10: 0 Werke (0.0%)\n",
      "\n",
      "Vollständige Topic-Verteilung für alle Werke gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_topicmodeling_text_all.csv\n",
      "\n",
      "Grafik zum durchschnittlichen Topic-Anteil gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_topic_text_all_means.png\n",
      "Grafik zu den dominanten Topics pro Werk gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_topic_text_all_dominant.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Topic Modeling für alle Werke (Text) mit optionalen Charakter-Stopwords ---\n",
    "\n",
    "#---Einstellung: Charakterliste als Stopwords nutzen ----------------------------------\n",
    "USE_CHARACTER_STOPWORDS = True  # True = Charakter-Namen werden als Stopwords hinzugefügt, False = nicht\n",
    "analysis_lines.append(f\"Charakter-Stopwords kombiniert: {USE_CHARACTER_STOPWORDS}\")\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "analysis_lines = []\n",
    "\n",
    "# Texte aller Werke\n",
    "texts_all = df[\"text_topic\"].tolist()\n",
    "\n",
    "# --- Stopwords vorbereiten ---\n",
    "all_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "all_stopwords.update(stopword_list)\n",
    "\n",
    "if USE_CHARACTER_STOPWORDS:\n",
    "    all_stopwords.update(character_list)\n",
    "\n",
    "analysis_lines.append(f\"Stopword-Liste für Topic Modeling verwendet: {len(all_stopwords)} Begriffe\")\n",
    "analysis_lines.append(f\"(Charkter-Namen berücksichtigt: {USE_CHARACTER_STOPWORDS}\")\n",
    "\n",
    "# --- Bag-of-Words ---\n",
    "vectorizer_all = CountVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    max_df=0.65,\n",
    "    min_df=5,\n",
    "    stop_words=list(all_stopwords)\n",
    ")\n",
    "X_all = vectorizer_all.fit_transform(texts_all)\n",
    "analysis_lines.append(f\"Vokabulargröße nach Stopword-Filterung: {len(vectorizer_all.get_feature_names_out())}\")\n",
    "\n",
    "# --- LDA-Modell ---\n",
    "n_topics_all = 10\n",
    "top_n = 15\n",
    "\n",
    "lda_all = LatentDirichletAllocation(\n",
    "    n_components=n_topics_all,\n",
    "    max_iter=10,\n",
    "    learning_method='online',\n",
    "    random_state=42\n",
    ")\n",
    "lda_all.fit(X_all)\n",
    "\n",
    "# --- Topics extrahieren ---\n",
    "def get_topics(model, vectorizer, top_n=15):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topic_words = {}\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        top_words = [words[i] for i in topic.argsort()[-top_n:][::-1]]\n",
    "        topic_words[f\"Topic {idx+1}\"] = top_words\n",
    "    return topic_words\n",
    "\n",
    "topic_words = get_topics(lda_all, vectorizer_all, top_n)\n",
    "analysis_lines.append(\"Topic Modeling – Top Wörter pro Topic:\")\n",
    "analysis_lines.append(\"\")\n",
    "for t, words in topic_words.items():\n",
    "    analysis_lines.append(f\"{t}: {', '.join(words)}\")\n",
    "    analysis_lines.append(\"\")\n",
    "\n",
    "# --- Topic-Verteilung pro Dokument ---\n",
    "doc_topic_all = lda_all.transform(X_all)\n",
    "\n",
    "# --- Dominanzanalyse ---\n",
    "dominant_threshold_5 = 0.5\n",
    "topic_counts_5 = (doc_topic_all > dominant_threshold_5).sum(axis=0)\n",
    "analysis_lines.append(\"Anzahl der Werke, in denen die Topics stark dominierend sind (>=0.5):\")\n",
    "for i, count in enumerate(topic_counts_5):\n",
    "    analysis_lines.append(f\"Topic {i+1}: {count} Werke ({round(100*count/len(df),1)}%)\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "dominant_threshold_3 = 0.3\n",
    "topic_counts_3 = (doc_topic_all > dominant_threshold_3).sum(axis=0)\n",
    "analysis_lines.append(\"Anzahl der Werke, in denen die Topics präsent sind (>=0.3):\")\n",
    "for i, count in enumerate(topic_counts_3):\n",
    "    analysis_lines.append(f\"Topic {i+1}: {count} Werke ({round(100*count/len(df),1)}%)\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- CSV-Ausgabe der vollständigen Topic-Verteilung ---\n",
    "output_path = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_topicmodeling_text_all.csv\"\n",
    ")\n",
    "df_doc_topic_all = pd.DataFrame(\n",
    "    doc_topic_all,\n",
    "    columns=[f\"Topic_{i+1}\" for i in range(n_topics_all)]\n",
    ")\n",
    "df_doc_topic_all[\"work_id\"] = df[\"work_id\"].values\n",
    "df_doc_topic_all.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "analysis_lines.append(f\"Vollständige Topic-Verteilung für alle Werke gespeichert unter: {output_path}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Visualisierung: Durchschnittlicher Topic-Anteil ---\n",
    "topic_means = df_doc_topic_all.drop(columns=\"work_id\").mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=topic_means.values, y=topic_means.index, palette=\"viridis\")\n",
    "plt.xlabel(\"Durchschnittlicher Topic-Anteil\")\n",
    "plt.ylabel(\"Topic\")\n",
    "plt.title(f\"Relevanz der Topics im Korpus für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "topic_means_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_topic_text_all_means.png\")\n",
    "plt.savefig(topic_means_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Grafik zum durchschnittlichen Topic-Anteil gespeichert unter: {topic_means_file}\")\n",
    "\n",
    "# --- Visualisierung: Dominantes Topic pro Werk ---\n",
    "dominant_topic = doc_topic_all.argmax(axis=1)\n",
    "dominant_counts = pd.Series(dominant_topic).value_counts().sort_index()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(\n",
    "    x=[f\"Topic {i+1}\" for i in dominant_counts.index],\n",
    "    y=dominant_counts.values,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.ylabel(\"Anzahl Werke\")\n",
    "plt.xlabel(\"Dominantes Topic\")\n",
    "plt.title(f\"Dominantes Topic pro Werk für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "dominant_topic_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_topic_text_all_dominant.png\")\n",
    "plt.savefig(dominant_topic_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Grafik zu den dominanten Topics pro Werk gespeichert unter: {dominant_topic_file}\")\n",
    "\n",
    "# --- In Analyse-Dokument schreiben ---\n",
    "write_analysis_block(\n",
    "    title=\"Topic Modeling über alle Werke (Texte, inkl. optionaler Charakter-Stopwords)\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bba8001f-7582-471c-bb46-6126515f4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_5488\\207008260.py:97: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=topic_means.values, y=topic_means.index, palette=\"viridis\")\n",
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_5488\\207008260.py:111: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-10 22:42:36] Topic Modeling – Zusammenfassung einschlägige Werke (Text)\n",
      "------------------------------------------------------------\n",
      "Stopword-Liste für Topic Modeling verwendet: 685 Begriffe\n",
      "(Charakter-Namen berücksichtigt: True)\n",
      "Vokabulargröße nach Stopword-Filterung: 20214\n",
      "Topic Modeling – Top Wörter pro Topic (einschlägige Werke):\n",
      "\n",
      "Topic 1: cock, floor, saw, second, pulled, doe, new, wand, magic, light\n",
      "\n",
      "Topic 2: oliver, lavender, penelope, friend, past, sat, madam, perce, sorry, broom\n",
      "\n",
      "Topic 3: cock, dark, light, work, second, magic, new, pulled, world, air\n",
      "\n",
      "Topic 4: oliver, loved, love oliver, sorry, book, work, kissed, perce, friend, world\n",
      "\n",
      "Topic 5: van, baby, toy, cock, hip, seat, seatbelt, low, fucking, moan\n",
      "\n",
      "Anzahl der Werke, in denen die Topics stark dominant sind (>=0.5):\n",
      "Topic 1: 0 Werke (0.0%)\n",
      "Topic 2: 3 Werke (3.1%)\n",
      "Topic 3: 94 Werke (95.9%)\n",
      "Topic 4: 1 Werke (1.0%)\n",
      "Topic 5: 0 Werke (0.0%)\n",
      "\n",
      "Anzahl der Werke, in denen die Topics präsent sind (>=0.3):\n",
      "Topic 1: 0 Werke (0.0%)\n",
      "Topic 2: 3 Werke (3.1%)\n",
      "Topic 3: 97 Werke (99.0%)\n",
      "Topic 4: 1 Werke (1.0%)\n",
      "Topic 5: 1 Werke (1.0%)\n",
      "\n",
      "Vollständige Topic-Verteilung für einschlägige Werke gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_topicmodeling_text_rel.csv\n",
      "\n",
      "Grafik zum durchschnittlichen Topic-Anteil gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_topic_text_rel_means.png\n",
      "Grafik zu den dominanten Topics pro Werk gespeichert unter: Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_topic_text_rel_dominant.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Topic Modeling für einschlägige Werke (Text) mit optionalen Charakter-Stopwords ---\n",
    "#---Einstellung: Charakterliste als Stopwords nutzen ----------------------------------\n",
    "USE_CHARACTER_STOPWORDS = True  # True = Charakter-Namen werden als Stopwords hinzugefügt, False = nicht\n",
    "analysis_lines.append(f\"Charakter-Stopwords kombiniert: {USE_CHARACTER_STOPWORDS}\")\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "analysis_lines = []\n",
    "\n",
    "# --- Einschlägige Werke filtern ---\n",
    "df_relevant = df[df[\"is_relevant\"]].copy()\n",
    "texts_relevant = df_relevant[\"text_topic\"].tolist()\n",
    "\n",
    "# --- Stopwords vorbereiten ---\n",
    "all_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "all_stopwords.update(stopword_list)\n",
    "\n",
    "if USE_CHARACTER_STOPWORDS:\n",
    "    all_stopwords.update(character_list)\n",
    "\n",
    "analysis_lines.append(f\"Stopword-Liste für Topic Modeling verwendet: {len(all_stopwords)} Begriffe\")\n",
    "analysis_lines.append(f\"(Charakter-Namen berücksichtigt: {USE_CHARACTER_STOPWORDS})\")\n",
    "\n",
    "# --- Bag-of-Words ---\n",
    "vectorizer_rel = CountVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    max_df=0.65,\n",
    "    min_df=2,\n",
    "    stop_words=list(all_stopwords)\n",
    ")\n",
    "X_rel = vectorizer_rel.fit_transform(texts_relevant)\n",
    "analysis_lines.append(f\"Vokabulargröße nach Stopword-Filterung: {len(vectorizer_rel.get_feature_names_out())}\")\n",
    "\n",
    "# --- LDA-Modell ---\n",
    "n_topics_rel = 5\n",
    "top_n_words = 10\n",
    "\n",
    "lda_rel = LatentDirichletAllocation(\n",
    "    n_components=n_topics_rel,\n",
    "    max_iter=10,\n",
    "    learning_method='online',\n",
    "    random_state=42\n",
    ")\n",
    "lda_rel.fit(X_rel)\n",
    "\n",
    "# --- Topics extrahieren ---\n",
    "def get_topics(model, vectorizer, top_n=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topic_words = {}\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        top_words = [words[i] for i in topic.argsort()[-top_n:][::-1]]\n",
    "        topic_words[f\"Topic {idx+1}\"] = top_words\n",
    "    return topic_words\n",
    "\n",
    "topic_words_rel = get_topics(lda_rel, vectorizer_rel, top_n_words)\n",
    "analysis_lines.append(\"Topic Modeling – Top Wörter pro Topic (einschlägige Werke):\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "for t, words in topic_words_rel.items():\n",
    "    analysis_lines.append(f\"{t}: {', '.join(words)}\")\n",
    "    analysis_lines.append(\"\")\n",
    "\n",
    "# --- Topic-Verteilung pro Dokument ---\n",
    "doc_topic_rel = lda_rel.transform(X_rel)\n",
    "\n",
    "# --- Dominanzanalyse ---\n",
    "dominant_threshold_5 = 0.5\n",
    "topic_counts_5 = (doc_topic_rel > dominant_threshold_5).sum(axis=0)\n",
    "analysis_lines.append(\"Anzahl der Werke, in denen die Topics stark dominant sind (>=0.5):\")\n",
    "for i, count in enumerate(topic_counts_5):\n",
    "    analysis_lines.append(f\"Topic {i+1}: {count} Werke ({round(100*count/len(df_relevant),1)}%)\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "dominant_threshold_3 = 0.3\n",
    "topic_counts_3 = (doc_topic_rel > dominant_threshold_3).sum(axis=0)\n",
    "analysis_lines.append(\"Anzahl der Werke, in denen die Topics präsent sind (>=0.3):\")\n",
    "for i, count in enumerate(topic_counts_3):\n",
    "    analysis_lines.append(f\"Topic {i+1}: {count} Werke ({round(100*count/len(df_relevant),1)}%)\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- CSV-Ausgabe der vollständigen Topic-Verteilung ---\n",
    "output_path_rel = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_topicmodeling_text_rel.csv\"\n",
    ")\n",
    "df_doc_topic_rel = pd.DataFrame(\n",
    "    doc_topic_rel,\n",
    "    columns=[f\"Topic_{i+1}\" for i in range(n_topics_rel)]\n",
    ")\n",
    "df_doc_topic_rel[\"work_id\"] = df_relevant[\"work_id\"].values\n",
    "df_doc_topic_rel.to_csv(output_path_rel, index=False, encoding=\"utf-8\")\n",
    "analysis_lines.append(f\"Vollständige Topic-Verteilung für einschlägige Werke gespeichert unter: {output_path_rel}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Visualisierung: Durchschnittlicher Topic-Anteil ---\n",
    "topic_means = df_doc_topic_rel.drop(columns=\"work_id\").mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=topic_means.values, y=topic_means.index, palette=\"viridis\")\n",
    "plt.xlabel(\"Durchschnittlicher Topic-Anteil\")\n",
    "plt.ylabel(\"Topic\")\n",
    "plt.title(f\"Relevanz der Topics (einschlägige Werke) für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "topic_means_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_topic_text_rel_means.png\")\n",
    "plt.savefig(topic_means_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Grafik zum durchschnittlichen Topic-Anteil gespeichert unter: {topic_means_file}\")\n",
    "\n",
    "# --- Visualisierung: Dominantes Topic pro Werk ---\n",
    "dominant_topic = doc_topic_rel.argmax(axis=1)\n",
    "dominant_counts = pd.Series(dominant_topic).value_counts().sort_index()\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(\n",
    "    x=[f\"Topic {i+1}\" for i in dominant_counts.index],\n",
    "    y=dominant_counts.values,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.ylabel(\"Anzahl Werke\")\n",
    "plt.xlabel(\"Dominantes Topic\")\n",
    "plt.title(f\"Dominantes Topic pro Werk (einschlägige Werke) für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "dominant_topic_file = os.path.join(OUTPUT_FOLDER, f\"{OUTPUT_PREFIX}_graph_topic_text_rel_dominant.png\")\n",
    "plt.savefig(dominant_topic_file)\n",
    "plt.close()\n",
    "analysis_lines.append(f\"Grafik zu den dominanten Topics pro Werk gespeichert unter: {dominant_topic_file}\")\n",
    "\n",
    "# --- In Analyse-Dokument schreiben ---\n",
    "write_analysis_block(\n",
    "    title=\"Topic Modeling – Zusammenfassung einschlägige Werke (Text)\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6affe560-21ad-4f7a-8086-4eba5beab2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_5488\\3850752505.py:154: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n",
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_5488\\3850752505.py:180: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-10 22:42:46] Topic Modeling – Tags aller Werke (Tags)\n",
      "------------------------------------------------------------\n",
      "Topic Modeling für alle Werke auf Basis von Tags.\n",
      "Anzahl berücksichtigter Werke mit Tags: 2073\n",
      "\n",
      "Stopword-Liste für Tags verwendet: 685 Begriffe\n",
      "(Charakter-Namen berücksichtigt: True)\n",
      "Anzahl verschiedener Tags im Vokabular nach Stopword-Filterung: 8887\n",
      "\n",
      "Top Tags pro Topic (alle Werke):\n",
      "\n",
      "Topic 1: kinktober_2025, smut, anal_sex, plot_what_plot/porn_without_plot, oral_sex\n",
      "Topic 2: fluff, halloween, harry_potter_epilogue_what_epilogue_|_ewe, kinktober_2025, alternate_universe___muggle\n",
      "Topic 3: kinktober_2025, hp_wlw_reverse_small_bang_2025, digital_art, art, fluff\n",
      "Topic 4: fluff, angst, hurt/comfort, one_shot, flufftober_2025\n",
      "Topic 5: explicit_sexual_content, alternate_universe___canon_divergence, established_relationship, harry_potter_epilogue_what_epilogue_|_ewe, smut\n",
      "\n",
      "\n",
      "Anzahl der Werke mit dominantem Topic (>= 0.5):\n",
      "Topic 1: 631 Werke (30.4%)\n",
      "Topic 2: 308 Werke (14.9%)\n",
      "Topic 3: 302 Werke (14.6%)\n",
      "Topic 4: 409 Werke (19.7%)\n",
      "Topic 5: 319 Werke (15.4%)\n",
      "\n",
      "Anzahl der Werke mit präsentem Topic (>= 0.3):\n",
      "Topic 1: 783 Werke (37.8%)\n",
      "Topic 2: 384 Werke (18.5%)\n",
      "Topic 3: 382 Werke (18.4%)\n",
      "Topic 4: 530 Werke (25.6%)\n",
      "Topic 5: 387 Werke (18.7%)\n",
      "\n",
      "Vollständige Topic-Verteilung (Tags, alle Werke) gespeichert unter:\n",
      "Analysedokumente\\Analysis_HP_FanFic_10_2025_topicmodeling_tags_all.csv\n",
      "\n",
      "Grafik: Durchschnittlicher Topic-Anteil gespeichert unter:\n",
      "Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_topic_tags_all_means.png\n",
      "\n",
      "Grafik: Dominantes Topic pro Werk gespeichert unter:\n",
      "Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_topic_tags_all_dominant.png\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Topic Modeling für alle Werke (Tags) mit optionalen Charakter-Stopwords ---\n",
    "#---Einstellung: Charakterliste als Stopwords nutzen ----------------------------------\n",
    "USE_CHARACTER_STOPWORDS = True  # True = Charakter-Namen werden als Stopwords hinzugefügt, False = nicht\n",
    "analysis_lines.append(f\"Charakter-Stopwords kombiniert: {USE_CHARACTER_STOPWORDS}\")\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "analysis_lines = []\n",
    "\n",
    "# --- Alle Werke betrachten ---\n",
    "df_all = df.copy()\n",
    "\n",
    "# --- Tags vorbereiten ---\n",
    "def normalize_tag(tag):\n",
    "    tag = str(tag).lower().strip()\n",
    "    tag = tag.replace(\" \", \"_\")\n",
    "    tag = tag.replace(\"-\", \"_\")\n",
    "    return tag\n",
    "\n",
    "# - Freeform-Tags normalisieren\n",
    "texts_tags_all = [\n",
    "    [normalize_tag(t) for t in tags if str(t).lower() != \"nan\"]\n",
    "    for tags in df_all[\"freeform\"]\n",
    "]\n",
    "\n",
    "# - Nur Werke mit mindestens einem Tag behalten\n",
    "valid_idx = [i for i, tags in enumerate(texts_tags_all) if len(tags) > 0]\n",
    "texts_tags_all = [texts_tags_all[i] for i in valid_idx]\n",
    "work_ids_tags_all = df_all.iloc[valid_idx][\"work_id\"].values\n",
    "\n",
    "analysis_lines.append(\"Topic Modeling für alle Werke auf Basis von Tags.\")\n",
    "analysis_lines.append(f\"Anzahl berücksichtigter Werke mit Tags: {len(texts_tags_all)}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Stopwords vorbereiten ---\n",
    "all_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "all_stopwords.update(stopword_list)\n",
    "\n",
    "if USE_CHARACTER_STOPWORDS:\n",
    "    all_stopwords.update(character_list)\n",
    "\n",
    "analysis_lines.append(f\"Stopword-Liste für Tags verwendet: {len(all_stopwords)} Begriffe\")\n",
    "analysis_lines.append(f\"(Charakter-Namen berücksichtigt: {USE_CHARACTER_STOPWORDS})\")\n",
    "\n",
    "# --- Bag-of-Words ---\n",
    "vectorizer_tags_all = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    min_df=1\n",
    ")\n",
    "\n",
    "# - Stopwords filtern\n",
    "texts_tags_all_filtered = [\n",
    "    [t for t in tags if t not in all_stopwords] for tags in texts_tags_all\n",
    "]\n",
    "\n",
    "X_tags_all = vectorizer_tags_all.fit_transform(texts_tags_all_filtered)\n",
    "\n",
    "analysis_lines.append(\n",
    "    f\"Anzahl verschiedener Tags im Vokabular nach Stopword-Filterung: {len(vectorizer_tags_all.get_feature_names_out())}\"\n",
    ")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- LDA-Modell ---\n",
    "n_topics_tags_all = 5\n",
    "top_n_words = 5\n",
    "\n",
    "lda_tags_all = LatentDirichletAllocation(\n",
    "    n_components=n_topics_tags_all,\n",
    "    max_iter=30,\n",
    "    learning_method=\"batch\",\n",
    "    random_state=42\n",
    ")\n",
    "lda_tags_all.fit(X_tags_all)\n",
    "\n",
    "# --- Topics extrahieren ---\n",
    "def get_topics(model, vectorizer, top_n):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topic_words = {}\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        top_words = [words[i] for i in topic.argsort()[-top_n:][::-1]]\n",
    "        topic_words[f\"Topic {idx+1}\"] = top_words\n",
    "    return topic_words\n",
    "\n",
    "topic_words_tags_all = get_topics(\n",
    "    lda_tags_all,\n",
    "    vectorizer_tags_all,\n",
    "    top_n_words\n",
    ")\n",
    "\n",
    "analysis_lines.append(\"Top Tags pro Topic (alle Werke):\")\n",
    "analysis_lines.append(\"\")\n",
    "for topic, words in topic_words_tags_all.items():\n",
    "    analysis_lines.append(f\"{topic}: {', '.join(words)}\")\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Topic-Verteilung pro Werk ---\n",
    "doc_topics_tags_all = lda_tags_all.transform(X_tags_all)\n",
    "\n",
    "# --- Dominanzanalyse ---\n",
    "dominant_threshold_5 = 0.5\n",
    "topic_counts_5 = (doc_topics_tags_all >= dominant_threshold_5).sum(axis=0)\n",
    "\n",
    "analysis_lines.append(\"Anzahl der Werke mit dominantem Topic (>= 0.5):\")\n",
    "for i, count in enumerate(topic_counts_5):\n",
    "    analysis_lines.append(\n",
    "        f\"Topic {i+1}: {count} Werke \"\n",
    "        f\"({round(100 * count / len(work_ids_tags_all), 1)}%)\"\n",
    "    )\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "dominant_threshold_3 = 0.3\n",
    "topic_counts_3 = (doc_topics_tags_all >= dominant_threshold_3).sum(axis=0)\n",
    "\n",
    "analysis_lines.append(\"Anzahl der Werke mit präsentem Topic (>= 0.3):\")\n",
    "for i, count in enumerate(topic_counts_3):\n",
    "    analysis_lines.append(\n",
    "        f\"Topic {i+1}: {count} Werke \"\n",
    "        f\"({round(100 * count / len(work_ids_tags_all), 1)}%)\"\n",
    "    )\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Vollständige Topic-Verteilung speichern ---\n",
    "df_doc_topics_tags_all = pd.DataFrame(\n",
    "    doc_topics_tags_all,\n",
    "    columns=[f\"Topic_{i+1}\" for i in range(n_topics_tags_all)]\n",
    ")\n",
    "df_doc_topics_tags_all[\"work_id\"] = work_ids_tags_all\n",
    "\n",
    "output_path_tags_all = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_topicmodeling_tags_all.csv\"\n",
    ")\n",
    "df_doc_topics_tags_all.to_csv(\n",
    "    output_path_tags_all,\n",
    "    index=False,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "analysis_lines.append(f\"Vollständige Topic-Verteilung (Tags, alle Werke) gespeichert unter:\")\n",
    "analysis_lines.append(output_path_tags_all)\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Visualisierung: Durchschnittlicher Topic-Anteil ---\n",
    "topic_means = (\n",
    "    df_doc_topics_tags_all\n",
    "    .drop(columns=\"work_id\")\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x=topic_means.values,\n",
    "    y=topic_means.index,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.xlabel(\"Durchschnittlicher Topic-Anteil\")\n",
    "plt.ylabel(\"Topic\")\n",
    "plt.title(f\"Tag-Topics – durchschnittliche Relevanz für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "\n",
    "topic_means_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_topic_tags_all_means.png\"\n",
    ")\n",
    "plt.savefig(topic_means_file)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(f\"Grafik: Durchschnittlicher Topic-Anteil gespeichert unter:\")\n",
    "analysis_lines.append(topic_means_file)\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Visualisierung: Dominantes Topic pro Werk ---\n",
    "dominant_topic = doc_topics_tags_all.argmax(axis=1)\n",
    "dominant_counts = pd.Series(dominant_topic).value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x=[f\"Topic {i+1}\" for i in dominant_counts.index],\n",
    "    y=dominant_counts.values,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.ylabel(\"Anzahl Werke\")\n",
    "plt.xlabel(\"Dominantes Topic\")\n",
    "plt.title(f\"Dominantes Tag-Topic pro Werk für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "\n",
    "dominant_topic_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_topic_tags_all_dominant.png\"\n",
    ")\n",
    "plt.savefig(dominant_topic_file)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(f\"Grafik: Dominantes Topic pro Werk gespeichert unter:\")\n",
    "analysis_lines.append(dominant_topic_file)\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- In Analyse-Dokument schreiben ---\n",
    "write_analysis_block(\n",
    "    title=\"Topic Modeling – Tags aller Werke (Tags)\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6b3ed1a-dfdc-4dc9-aec4-0cb3e73e3a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_5488\\1248677206.py:154: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n",
      "C:\\Users\\Uni\\AppData\\Local\\Temp\\ipykernel_5488\\1248677206.py:180: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-10 22:42:47] Topic Modeling – Tags einschlägiger Werke (inkl. optionaler Charakter-Stopwords)\n",
      "------------------------------------------------------------\n",
      "Topic Modeling für einschlägige Werke auf Basis von Tags.\n",
      "Anzahl berücksichtigter Werke mit Tags: 98\n",
      "\n",
      "Stopword-Liste für Tags verwendet: 685 Begriffe\n",
      "(Charakter-Namen berücksichtigt: True)\n",
      "Anzahl verschiedener Tags im Vokabular nach Stopword-Filterung: 1027\n",
      "\n",
      "Top Tags pro Topic (einschlägige Werke):\n",
      "\n",
      "Topic 1: trans_regulus_black, oral_sex, kinktober_2025, vaginal_sex, anal_sex\n",
      "Topic 2: kinktober_2025, anal_sex, praise_kink, smut, shameless_smut\n",
      "Topic 3: kinktober_2025, trans_regulus_black, kinktober, alternate_universe___muggle, one_shot\n",
      "Topic 4: trans_regulus_black, alternate_universe___modern_setting, plot_what_plot/porn_without_plot, fluff, smut\n",
      "Topic 5: trans_regulus_black, trans_male_character, vaginal_sex, plot_what_plot/porn_without_plot, oral_sex\n",
      "\n",
      "\n",
      "Anzahl der Werke mit dominantem Topic (>= 0.5):\n",
      "Topic 1: 25 Werke (25.5%)\n",
      "Topic 2: 17 Werke (17.3%)\n",
      "Topic 3: 15 Werke (15.3%)\n",
      "Topic 4: 24 Werke (24.5%)\n",
      "Topic 5: 17 Werke (17.3%)\n",
      "\n",
      "Anzahl der Werke mit präsentem Topic (>= 0.3):\n",
      "Topic 1: 25 Werke (25.5%)\n",
      "Topic 2: 17 Werke (17.3%)\n",
      "Topic 3: 15 Werke (15.3%)\n",
      "Topic 4: 24 Werke (24.5%)\n",
      "Topic 5: 17 Werke (17.3%)\n",
      "\n",
      "Vollständige Topic-Verteilung (Tags, einschlägige Werke) gespeichert unter:\n",
      "Analysedokumente\\Analysis_HP_FanFic_10_2025_topicmodeling_tags_rel.csv\n",
      "\n",
      "Grafik: Durchschnittlicher Topic-Anteil gespeichert unter:\n",
      "Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_topic_tags_rel_means.png\n",
      "\n",
      "Grafik: Dominantes Topic pro Werk gespeichert unter:\n",
      "Analysedokumente\\Analysis_HP_FanFic_10_2025_graph_topic_tags_rel_dominant.png\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Topic Modeling für einschlägige Werke (Tags) mit optionalen Charakter-Stopwords ---\n",
    "#---Einstellung: Charakterliste als Stopwords nutzen ----------------------------------\n",
    "USE_CHARACTER_STOPWORDS = True  # True = Charakter-Namen werden als Stopwords hinzugefügt, False = nicht\n",
    "analysis_lines.append(f\"Charakter-Stopwords kombiniert: {USE_CHARACTER_STOPWORDS}\")\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "analysis_lines = []\n",
    "\n",
    "# --- Einschlägige Werke filtern ---\n",
    "df_relevant = df[df[\"is_relevant\"]].copy()\n",
    "\n",
    "# --- Tags vorbereiten ---\n",
    "def normalize_tag(tag):\n",
    "    tag = str(tag).lower().strip()\n",
    "    tag = tag.replace(\" \", \"_\")\n",
    "    tag = tag.replace(\"-\", \"_\")\n",
    "    return tag\n",
    "\n",
    "# - Freeform-Tags normalisieren\n",
    "texts_tags_rel = [\n",
    "    [normalize_tag(t) for t in tags if str(t).lower() != \"nan\"]\n",
    "    for tags in df_relevant[\"freeform\"]\n",
    "]\n",
    "\n",
    "# - Nur Werke mit mindestens einem Tag behalten\n",
    "valid_idx = [i for i, tags in enumerate(texts_tags_rel) if len(tags) > 0]\n",
    "texts_tags_rel = [texts_tags_rel[i] for i in valid_idx]\n",
    "work_ids_tags_rel = df_relevant.iloc[valid_idx][\"work_id\"].values\n",
    "\n",
    "analysis_lines.append(\"Topic Modeling für einschlägige Werke auf Basis von Tags.\")\n",
    "analysis_lines.append(f\"Anzahl berücksichtigter Werke mit Tags: {len(texts_tags_rel)}\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Stopwords vorbereiten ---\n",
    "all_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "all_stopwords.update(stopword_list)\n",
    "\n",
    "if USE_CHARACTER_STOPWORDS:\n",
    "    all_stopwords.update(character_list)\n",
    "\n",
    "analysis_lines.append(f\"Stopword-Liste für Tags verwendet: {len(all_stopwords)} Begriffe\")\n",
    "analysis_lines.append(f\"(Charakter-Namen berücksichtigt: {USE_CHARACTER_STOPWORDS})\")\n",
    "\n",
    "# - Stopwords auf Tags anwenden\n",
    "texts_tags_rel_filtered = [\n",
    "    [t for t in tags if t not in all_stopwords] for tags in texts_tags_rel\n",
    "]\n",
    "\n",
    "# --- Bag-of-Words ---\n",
    "vectorizer_tags_rel = CountVectorizer(\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    min_df=1\n",
    ")\n",
    "\n",
    "X_tags_rel = vectorizer_tags_rel.fit_transform(texts_tags_rel_filtered)\n",
    "\n",
    "analysis_lines.append(\n",
    "    f\"Anzahl verschiedener Tags im Vokabular nach Stopword-Filterung: {len(vectorizer_tags_rel.get_feature_names_out())}\"\n",
    ")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- LDA-Modell ---\n",
    "n_topics_tags_rel = 5\n",
    "top_n_words = 5\n",
    "\n",
    "lda_tags_rel = LatentDirichletAllocation(\n",
    "    n_components=n_topics_tags_rel,\n",
    "    max_iter=30,\n",
    "    learning_method=\"batch\",\n",
    "    random_state=42\n",
    ")\n",
    "lda_tags_rel.fit(X_tags_rel)\n",
    "\n",
    "# --- Topics extrahieren ---\n",
    "def get_topics(model, vectorizer, top_n):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    topic_words = {}\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        top_words = [words[i] for i in topic.argsort()[-top_n:][::-1]]\n",
    "        topic_words[f\"Topic {idx+1}\"] = top_words\n",
    "    return topic_words\n",
    "\n",
    "topic_words_tags_rel = get_topics(\n",
    "    lda_tags_rel,\n",
    "    vectorizer_tags_rel,\n",
    "    top_n_words\n",
    ")\n",
    "\n",
    "analysis_lines.append(\"Top Tags pro Topic (einschlägige Werke):\")\n",
    "analysis_lines.append(\"\")\n",
    "for topic, words in topic_words_tags_rel.items():\n",
    "    analysis_lines.append(f\"{topic}: {', '.join(words)}\")\n",
    "analysis_lines.append(\"\")\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Topic-Verteilung pro Werk ---\n",
    "doc_topics_tags_rel = lda_tags_rel.transform(X_tags_rel)\n",
    "\n",
    "# --- Dominanzanalyse ---\n",
    "dominant_threshold_5 = 0.5\n",
    "topic_counts_5 = (doc_topics_tags_rel >= dominant_threshold_5).sum(axis=0)\n",
    "\n",
    "analysis_lines.append(\"Anzahl der Werke mit dominantem Topic (>= 0.5):\")\n",
    "for i, count in enumerate(topic_counts_5):\n",
    "    analysis_lines.append(\n",
    "        f\"Topic {i+1}: {count} Werke \"\n",
    "        f\"({round(100 * count / len(work_ids_tags_rel), 1)}%)\"\n",
    "    )\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "dominant_threshold_3 = 0.3\n",
    "topic_counts_3 = (doc_topics_tags_rel >= dominant_threshold_3).sum(axis=0)\n",
    "\n",
    "analysis_lines.append(\"Anzahl der Werke mit präsentem Topic (>= 0.3):\")\n",
    "for i, count in enumerate(topic_counts_3):\n",
    "    analysis_lines.append(\n",
    "        f\"Topic {i+1}: {count} Werke \"\n",
    "        f\"({round(100 * count / len(work_ids_tags_rel), 1)}%)\"\n",
    "    )\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Vollständige Topic-Verteilung speichern ---\n",
    "df_doc_topics_tags_rel = pd.DataFrame(\n",
    "    doc_topics_tags_rel,\n",
    "    columns=[f\"Topic_{i+1}\" for i in range(n_topics_tags_rel)]\n",
    ")\n",
    "df_doc_topics_tags_rel[\"work_id\"] = work_ids_tags_rel\n",
    "\n",
    "output_path_tags_rel = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_topicmodeling_tags_rel.csv\"\n",
    ")\n",
    "df_doc_topics_tags_rel.to_csv(\n",
    "    output_path_tags_rel,\n",
    "    index=False,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "analysis_lines.append(f\"Vollständige Topic-Verteilung (Tags, einschlägige Werke) gespeichert unter:\")\n",
    "analysis_lines.append(output_path_tags_rel)\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Visualisierung: Durchschnittlicher Topic-Anteil ---\n",
    "topic_means = (\n",
    "    df_doc_topics_tags_rel\n",
    "    .drop(columns=\"work_id\")\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x=topic_means.values,\n",
    "    y=topic_means.index,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.xlabel(\"Durchschnittlicher Topic-Anteil\")\n",
    "plt.ylabel(\"Topic\")\n",
    "plt.title(f\"Tag-Topics – durchschnittliche Relevanz für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "\n",
    "topic_means_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_topic_tags_rel_means.png\"\n",
    ")\n",
    "plt.savefig(topic_means_file)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(f\"Grafik: Durchschnittlicher Topic-Anteil gespeichert unter:\")\n",
    "analysis_lines.append(topic_means_file)\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- Visualisierung: Dominantes Topic pro Werk ---\n",
    "dominant_topic = doc_topics_tags_rel.argmax(axis=1)\n",
    "dominant_counts = pd.Series(dominant_topic).value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    x=[f\"Topic {i+1}\" for i in dominant_counts.index],\n",
    "    y=dominant_counts.values,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.ylabel(\"Anzahl Werke\")\n",
    "plt.xlabel(\"Dominantes Topic\")\n",
    "plt.title(f\"Dominantes Tag-Topic pro Werk für {OUTPUT_PREFIX}\")\n",
    "plt.tight_layout()\n",
    "\n",
    "dominant_topic_file = os.path.join(\n",
    "    OUTPUT_FOLDER,\n",
    "    f\"{OUTPUT_PREFIX}_graph_topic_tags_rel_dominant.png\"\n",
    ")\n",
    "plt.savefig(dominant_topic_file)\n",
    "plt.close()\n",
    "\n",
    "analysis_lines.append(f\"Grafik: Dominantes Topic pro Werk gespeichert unter:\")\n",
    "analysis_lines.append(dominant_topic_file)\n",
    "analysis_lines.append(\"\")\n",
    "\n",
    "# --- In Analyse-Dokument schreiben ---\n",
    "write_analysis_block(\n",
    "    title=\"Topic Modeling – Tags einschlägiger Werke (inkl. optionaler Charakter-Stopwords)\",\n",
    "    lines=analysis_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa3a2f-b2c9-4486-9286-cf0f5c65aa0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
